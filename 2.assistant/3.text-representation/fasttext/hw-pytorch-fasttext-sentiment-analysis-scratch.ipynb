{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPUzqg-23daF"
   },
   "source": [
    "# 使用 FastText 做情感分析\n",
    "\n",
    "在上一个 notebook 中，我们使用所有用于情绪分析的常用技术，成功地实现了约84%的测试准确率。在本笔记本中，我们将实现一个模型，该模型可以获得可比较的结果，同时训练速度显著加快，并使用大约一半的参数。更具体地说，我们将实现论文中的“FastText”模型[高效文本分类的技巧包](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36mlD0aq3daJ"
   },
   "source": [
    "## 准备数据\n",
    "\n",
    "FastText论文中的一个关键概念是，它们计算输入句子的n元，并将其附加到句子的末尾。在这里，我们将使用双克。简单地说，双格是在一个句子中连续出现的一对单词/标记。\n",
    "\n",
    "比如, 在句子中 \"how are you ?\",  bi-grams: \"how are\", \"are you\" and \"you ?\".\n",
    "\n",
    "`generate_bigrams` 函数获取一个已标记的句子，计算bigram并将其附加到标记化列表的末尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAYj5ruf3daK"
   },
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(\" \".join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLO0ZSji3daM"
   },
   "source": [
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4DXE8Gv3daM",
    "outputId": "5bfac5fd-671d-4f67-bc19-88047bea7c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'film', 'is', 'terrible', 'film is', 'This film', 'is terrible']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigrams([\"This\", \"film\", \"is\", \"terrible\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdCWKKyC3daO"
   },
   "source": [
    "TorchText `Field` 有一个 `preprocessing` 参数。此处传递的函数将在句子被标记化（从字符串转换为标记列表）之后，但在其被数字化（从标记列表转换为索引列表）之前应用于句子。这就是我们传递 `generate_bigrams` 函数的地方。\n",
    "\n",
    "由于我们不使用RNN，我们不能使用压缩填充序列，因此我们不需要设置 `include_length=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwsa2rih3daP",
    "outputId": "706f3719-ad02-4a74-d3e3-e576a65c4e7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en_core_web_sm\",\n",
    "    preprocessing=generate_bigrams,\n",
    ")\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM-E7QM-3daR"
   },
   "source": [
    "As before, we load the IMDb dataset and create the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9T2tqJP3daS",
    "outputId": "5bcd730d-4f90-4f93-ba70-d4c2bac48882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48rj6s_h3daS"
   },
   "source": [
    "Build the vocab and load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5MULkjt3daT"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    train_data,\n",
    "    max_size=MAX_VOCAB_SIZE,\n",
    "    vectors=\"glove.6B.100d\",\n",
    "    unk_init=torch.Tensor.normal_,\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M5H275h3daU"
   },
   "source": [
    "并创建迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmgmFtMN3daU",
    "outputId": "3e72e671-cd4c-48d6-e3c4-21f426befa5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eBOo4KB3daV"
   },
   "source": [
    "## 构建模型\n",
    "\n",
    "\n",
    "该模型的参数远小于前一模型，因为它只有两个具有任何参数的层，嵌入层和线性层。看不到RNN组件！\n",
    "\n",
    "相反，它首先使用 `Embedding`  层（蓝色）计算每个单词的单词嵌入，然后计算所有单词嵌入的平均值（粉色），并通过 `Linear` 层（银色）将其输入，就这样！\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment8.png?raw=1)\n",
    "\n",
    "我们使用 `avg_pool2d`（平均池二维）函数实现平均。最初，您可能会认为使用二维池似乎很奇怪，我们的句子肯定是一维的，而不是二维的？但是，您可以将单词嵌入视为二维网格，其中单词沿着一个轴，单词嵌入的维度沿着另一个轴。下图是一个转换为5维单词嵌入后的示例句子，单词沿垂直轴，嵌入沿水平轴。[4x5]张量中的每个元素都由一个绿色块表示。\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment9.png?raw=1)\n",
    "\n",
    "`avg_pool2d` 使用大小为“嵌入式”的过滤器。形状[1]（即句子的长度）乘以1。这在下图中以粉红色显示。\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment10.png?raw=1)\n",
    "\n",
    "我们计算过滤器覆盖的所有元素的平均值，然后过滤器向右滑动，计算句子中每个单词的下一列嵌入值的平均值。\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment11.png?raw=1)\n",
    "\n",
    "每个过滤器位置为我们提供一个值，即所有覆盖元素的平均值。在滤波器覆盖所有嵌入维数后，我们得到[1x5]张量。这个张量然后通过线性层来产生我们的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gySPSW423daW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # text = [sent len, batch size]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "\n",
    "        # pooled = [batch size, embedding_dim]\n",
    "\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTIKx5ow3daX"
   },
   "source": [
    "如前所述，我们将创建 `FastText` 类的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLxxWVz13daX"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIbbK1eM3daX"
   },
   "source": [
    "查看我们模型中的参数数量，我们可以看到，我们的参数与第一个笔记本中的标准RNN大致相同，只有前一个模型的一半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU3Fxkn33daX",
    "outputId": "3074b341-be42-460d-99e3-571047a54e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u14FvOxj3daY"
   },
   "source": [
    "并将预训练的向量复制到我们的嵌入层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iulR6mcV3daY",
    "outputId": "6b899616-969e-462d-b26a-9630cbd3c569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1606, -0.7357,  0.5809,  ...,  0.8704, -1.5637, -1.5724],\n",
       "        [-1.3126, -1.6717,  0.4203,  ...,  0.2348, -0.9110,  1.0914],\n",
       "        [-1.5268,  1.5639, -1.0541,  ...,  1.0045, -0.6813, -0.8846]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RykChBoW3daY"
   },
   "source": [
    "并将预训练的向量复制到我们的嵌入层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bnro9tR3daY"
   },
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSqHrMPT3daZ"
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sxmnY343daZ"
   },
   "source": [
    "训练模型与上次完全相同。\n",
    "\n",
    "我们初始化优化器。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jknxwtw3daa"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S_UiAD83daa"
   },
   "source": [
    "我们定义标准，并将模型和标准放置在GPU上（如果可用）。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cswr2Z5S3daa"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH9nLErH3dac"
   },
   "source": [
    "我们实现了计算精度的函数。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WitnHuns3dac"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RECNvqH13dad"
   },
   "source": [
    "我们定义了一个用于训练模型的函数...\n",
    "\n",
    "\n",
    "**Note**: 我们不再使用 dropout，因此不需要使用 `model.train（）`，但正如第一本笔记本中提到的，使用它是一个很好的实践。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8ZaR7Ll3dad"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC-5IlR33dad"
   },
   "source": [
    "我们定义了一个用于测试模型的函数...\n",
    "\n",
    "**Note**: 再次，我们离开 `model.eval（）` 即使我们不使用dropout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS1ts1wM3dae"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W5b_X_L3dae"
   },
   "source": [
    "如前所述，我们将实现一个有用的函数来告诉我们一个epoch需要多长时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSKpZi_r3dae"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJRKLObS3daf"
   },
   "source": [
    "最后，我们训练我们的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv3O4q9Z3daf",
    "outputId": "7783c38d-8379-4cea-fbbb-13ea58d74621"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.688 | Train Acc: 61.31%\n",
      "\t Val. Loss: 0.637 |  Val. Acc: 72.46%\n",
      "Epoch: 02 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.651 | Train Acc: 75.04%\n",
      "\t Val. Loss: 0.507 |  Val. Acc: 76.92%\n",
      "Epoch: 03 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.578 | Train Acc: 79.91%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 80.97%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.501 | Train Acc: 83.97%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 84.34%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.435 | Train Acc: 86.96%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 86.18%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut3-model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-wgoLdj3daf"
   },
   "source": [
    "…并获得测试精度！\n",
    "\n",
    "结果与上一个笔记本中的结果相当，但培训所需时间要少得多！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrAnMMOI3daf",
    "outputId": "1a8aaabf-ae52-4ade-a6be-0fede708df0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.381 | Test Acc: 85.42%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"tut3-model.pt\"))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkVm-ruR3dag"
   },
   "source": [
    "## 用户输入\n",
    "\n",
    "和以前一样，我们可以测试用户提供的任何输入，确保从标记化的句子中生成双字图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrLQDde33dag"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SnsI5WP3dag"
   },
   "source": [
    "一个负面评论的例子..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AP8lIUk3dag",
    "outputId": "0579827b-7db8-4e6f-efd0-70bc836faec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1313092350011553e-12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4b-g7Dz3dah"
   },
   "source": [
    "一个正面评论的例子..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZCffxjP3dah",
    "outputId": "dec8069b-3e63-4580-c653-86f04e8c7d50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
