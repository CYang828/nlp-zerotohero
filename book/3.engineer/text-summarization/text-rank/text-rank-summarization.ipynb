{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank 的处理流程（single-domain-multiple-documents）\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/block_3.png)\n",
    "\n",
    "\n",
    "这是一个 single-domain-multiple-documents 的摘要任务。我们会使用多个文章作为输入，并生成一个要点式的摘要。\n",
    "\n",
    "- 1.把所有的文章中的句子，连接起来\n",
    "- 2.把文本分割成一个个的句子\n",
    "- 3.对每个句子做向量化\n",
    "- 4.计算句子之间的相似度，并存储到矩阵中\n",
    "- 5.把相似度矩阵转化为图，其中句子是节点，相似度是节点间的权重\n",
    "- 6.取到 top-k 权重的句子，作为最终的摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取并探索数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /home/kkb/.local/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/kkb/.local/lib/python3.8/site-packages (from pandas) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5FvQ9LHGtja"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nba-articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "hm6pa5Af4qbE",
    "outputId": "7bf41859-f291-44ae-a785-12cd486a53a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>浓眉哥”安东尼-戴维斯自从出道以来，就倍受全美球迷追捧。美国球评称他为美国内线最后的希望。浓...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>北京时间11月18日，洛杉矶湖人以102-109不敌密尔沃基雄鹿。赛后，湖人主帅沃格尔谈到了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人，字母哥在今天的比赛中23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人。此役，字母哥23投18中...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>北京时间11月18日，湖人败给雄鹿，拉塞尔-威斯布鲁克表现不是特别抢眼，但失误赛季第二少，助...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                       article_text\n",
       "0           1  浓眉哥”安东尼-戴维斯自从出道以来，就倍受全美球迷追捧。美国球评称他为美国内线最后的希望。浓...\n",
       "1           2  北京时间11月18日，洛杉矶湖人以102-109不敌密尔沃基雄鹿。赛后，湖人主帅沃格尔谈到了...\n",
       "2           3  北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人，字母哥在今天的比赛中23...\n",
       "3           4  北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人。此役，字母哥23投18中...\n",
       "4           5  北京时间11月18日，湖人败给雄鹿，拉塞尔-威斯布鲁克表现不是特别抢眼，但失误赛季第二少，助..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本转化为句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 标准化文本\n",
    "def as_text(v):  ## 生成unicode字符串\n",
    "    if v is None:\n",
    "        return None\n",
    "    elif isinstance(v, bytes):\n",
    "        return v.decode('utf-8', errors='ignore')\n",
    "    elif isinstance(v, str):\n",
    "        return v\n",
    "    else:\n",
    "        raise ValueError('Unknown type %r' % type(v))\n",
    "\n",
    "        \n",
    "text_type    = str\n",
    "string_types = (str,)\n",
    "xrange       = range\n",
    "\n",
    "\n",
    "def is_text(v):\n",
    "    return isinstance(v, text_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSegmentation(object):\n",
    "    \"\"\" 分句 \"\"\"\n",
    "    \n",
    "    def __init__(self, delimiters=['?', '!', ';', '？', '！', '。', '；', '……', '…', '\\n']\n",
    "):\n",
    "        \"\"\"\n",
    "        Keyword arguments:\n",
    "        delimiters -- 可迭代对象，用来拆分句子\n",
    "        \"\"\"\n",
    "        self.delimiters = set([as_text(item) for item in delimiters])\n",
    "    \n",
    "    def segment(self, text):\n",
    "        res = [as_text(text)]\n",
    "\n",
    "        for sep in self.delimiters:\n",
    "            text, res = res, []\n",
    "            for seq in text:\n",
    "                res += seq.split(sep)\n",
    "        res = [s.strip() for s in res if len(s.strip()) > 0]\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京时间11月18日，洛杉矶湖人以102-109不敌密尔沃基雄鹿',\n",
       " '赛后，湖人主帅沃格尔谈到了霍顿-塔克',\n",
       " '沃格尔说：“塔克在过去两场比赛中都表现得很出色，即使勒布朗回来，他也有充分的理由首发出战”',\n",
       " '“在替补席上，塔克也有得分和组织的任务，但是现在的塔克足够优秀，他也可以获得首发的位置”',\n",
       " '此役，塔克出战38分58秒，18投9中得到了25分12篮板3助攻2抢断',\n",
       " '在上一场比赛中，塔克得到了职业生涯最高的28分',\n",
       " '值得一提的是，在下一场比赛中，詹姆斯还未确定是否出战']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SentenceSegmentation()\n",
    "ss.segment(df['article_text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZVoc3R6G9a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['浓眉哥”安东尼-戴维斯自从出道以来，就倍受全美球迷追捧',\n",
       "  '美国球评称他为美国内线最后的希望',\n",
       "  '浓眉也确实争气，第二个赛季就打出了场均20.8分10篮板2.8封盖的华丽数据',\n",
       "  '球迷们把浓眉选进了全明星阵容',\n",
       "  '美国媒体也毫不客气，看浓眉就像看自家孩子一样',\n",
       "  '2014-2015赛季，在浓眉入行的第三年，ESPN就把浓眉排在了自己百大球员的第三名',\n",
       "  '在他身后的有杜兰特/哈登/库里/威少，一干成名已久且当时或日后都拿到了MVP的超级巨星',\n",
       "  'ESPN当时的解释是，在浓眉当时的年纪，只有勒布朗-詹姆斯和魔术师的胜利贡献值比浓眉更高',\n",
       "  '从那以后，浓眉哥在ESPN的排名就没低于过前六（期间两次被排在第二，2015年和2020年）',\n",
       "  '但是就在本赛季开始前，ESPN把浓眉排在了第九，这几乎是浓眉成名后，被媒体最看衰的一次',\n",
       "  '新科MVP约基奇，MVP排行榜第二的恩比德，率队夺冠的字母哥，都没什么争议的排在了字母哥之前',\n",
       "  '浓眉哥还是全美内线最后的希望吗',\n",
       "  '考虑道其他几大内线的国籍，这么说或许没问题，美国也真没拿得出手的内线了，不然梦之队也不会召唤麦基这水平的球员了',\n",
       "  '但是如果还说浓眉是全NBA最好的内线',\n",
       "  '看完了今天雄鹿和湖人的比赛，即使是湖人球迷也沉默不语了',\n",
       "  '现在的浓眉哥，距离最强内线的称号真的是越来越远了',\n",
       "  '这个赛季，湖人队交易来了威少，将19-20赛季的夺冠阵容几乎全部拆散',\n",
       "  '在引援方面湖人引进的几乎全是后卫，身高体重合格的锋线只有阿里扎一个人，1米93的贝兹莫尔都得顶到前锋位置了',\n",
       "  '年轻时候防守都不及格的安东尼都被迫在最近逼成了球队的首发大前锋',\n",
       "  '这一套头重脚轻的阵容，素以防守调教起家的沃格尔也玩不转了',\n",
       "  '湖人队如今每场比赛让对方在篮下的出手是全联盟最多的，原因很简单：球队的外线防守几乎被人一步就过，四处漏风，对方的持球型锋线谁都可以轻松突破第一道锋线杀入禁区',\n",
       "  '湖人队本赛季每场比赛丢112.3分已经排到了联盟第三，球队的防守效率也只是联盟第15',\n",
       "  '19-20赛季，湖人队的防守效率联盟第三',\n",
       "  '上个赛季，湖人防守效率联盟第一',\n",
       "  '本赛季，哪怕有戴维斯这么个最佳防守球员级别的内线大闸在，也防不住湖人到处漏人的外线了',\n",
       "  '在防守端浓眉已经算拼尽全力了，可是效果却惨不忍睹：本赛季浓眉在场的时候，湖人每百回合丢109.5分',\n",
       "  '他不在场，湖人每百回合丢98分',\n",
       "  '增重以后的浓眉速率变慢，护框也没以前那么厉害了',\n",
       "  '今天字母哥上半场几乎球球都冲着浓眉去，浓眉却完全做不出有效干扰，让字母哥半场13投12中，彻底打爆了湖人禁区',\n",
       "  '防守的问题不该浓眉去背锅，他已经是湖人防守最努力的球员了',\n",
       "  '可在进攻端浓眉的问题还是很大的',\n",
       "  '曾经的浓眉号称全联盟第一吃饼侠，既能挡拆顺下攻框，又能拉开空间杵在外线百步穿杨',\n",
       "  '能力能外，能持球能无球，在球场攻防两端影响巨大',\n",
       "  '可本赛季的浓眉呢',\n",
       "  '以前的浓眉跳投虽说不算十拿九稳，那也是绝对不能放的点',\n",
       "  '可本赛季，他前27次三分出手只命中了4球，创下了NBA历史赛季前25次出手三分以上的命中率新低',\n",
       "  '今天对阵雄鹿赛前，媒体晒出数据：字母哥最近三场命中的三分数量（6记）等于浓眉整个赛季命中的三分数量（6记）',\n",
       "  '在比赛中，湖人还习惯地在外线放空字母哥，字母哥毫不客气全场三分4投3中',\n",
       "  '不止在三分线外毫无杀伤，命中率堪比麦基',\n",
       "  '浓眉在中距离效率也不能算高',\n",
       "  '在中距离他79投32中，命中率只是刚过40%',\n",
       "  '从进攻方式上看，詹姆斯不在，他有太多由自己发起的进攻了',\n",
       "  '每场比赛浓眉有4.2次低位背打和2.1次面框单打',\n",
       "  '在鹈鹕时代，浓眉的得分受助攻率几乎都在70%以上，本赛季他得分的受助攻率只有57.2%，是他的生涯新低',\n",
       "  '这说明了什么',\n",
       "  '沃格尔设计的战术也很有问题，后卫没法和浓眉发动有效的配合，让浓眉能轻松吃点饼',\n",
       "  '球球让浓眉那么费劲儿的干凿',\n",
       "  '真不是浓眉的正确使用方法',\n",
       "  '上个赛季哈雷尔在沃格尔手下就创下了自己场均挡拆顺下攻框数的赛季新低',\n",
       "  '手里有着哈雷尔+施罗德都不知道多打挡拆，沃格尔或许是真不知道怎么让内线打的舒服',\n",
       "  '浓眉已经证明了自己是总冠军球队的二当家，可是比起约基奇，字母哥，他的带队能力确实差的太远',\n",
       "  '可这真的能怪浓眉吗',\n",
       "  '28岁的他恐怕是学不来字母哥的冲击型打法和约基奇的策动全队了',\n",
       "  '湖人唯一能做到的，只能快点等詹姆斯回来，别再让浓眉打这么辛苦疲惫了'],\n",
       " ['北京时间11月18日，洛杉矶湖人以102-109不敌密尔沃基雄鹿',\n",
       "  '赛后，湖人主帅沃格尔谈到了霍顿-塔克',\n",
       "  '沃格尔说：“塔克在过去两场比赛中都表现得很出色，即使勒布朗回来，他也有充分的理由首发出战”',\n",
       "  '“在替补席上，塔克也有得分和组织的任务，但是现在的塔克足够优秀，他也可以获得首发的位置”',\n",
       "  '此役，塔克出战38分58秒，18投9中得到了25分12篮板3助攻2抢断',\n",
       "  '在上一场比赛中，塔克得到了职业生涯最高的28分',\n",
       "  '值得一提的是，在下一场比赛中，詹姆斯还未确定是否出战'],\n",
       " ['北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人，字母哥在今天的比赛中23投18中，三分球 4投3中，砍下47分9篮板',\n",
       "  '因此，美媒统计了关于字母哥的一项数据，他也成为了马布里之后能够砍下这项数据的第一人',\n",
       "  '根据数据统计，在今天的比赛中，字母哥轰下了45分且命中率达到75%以上',\n",
       "  '因此，字母哥成为了马布里在2005年对阵湖人的比赛之后，首位能够对阵湖人的时候砍下45+分且命中率75+%的球员',\n",
       "  '最终，在字母哥的带领下，雄鹿有惊无险在主场战胜了湖人'],\n",
       " ['北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人',\n",
       "  '此役，字母哥23投18中，三分球 4投3中，砍下47分9篮板',\n",
       "  '同时，美媒统计了一项字母哥与浓眉哥的数据对比',\n",
       "  '根据数据统计，字母哥在最近3场比赛中投进了6记三分球，这和浓眉哥本赛季以来一共投进6记三分球一样多',\n",
       "  '因此，相比起字母哥的出色状态，浓眉哥在本赛季以来的状态一般，而本场比赛浓眉哥则是15投9中，贡献了18分9板4助2帽的数据',\n",
       "  '其中，浓眉哥的三分球1投0中'],\n",
       " ['北京时间11月18日，湖人败给雄鹿，拉塞尔-威斯布鲁克表现不是特别抢眼，但失误赛季第二少，助攻赛季最高',\n",
       "  '威斯布鲁克全场16投7中，得了19分、15次助攻和4个篮板',\n",
       "  '从数据看，并不是赛季最出色的，但全场只有3次失误，这是巨大的进步',\n",
       "  '此前的比赛，威少一直以失误多出名，最多一场甚至上双',\n",
       "  '此外还有9次、8次和7次',\n",
       "  '只有一场，他失误2次，为赛季最低',\n",
       "  '还有两次失误3次，今天是第三次',\n",
       "  '在詹姆斯缺阵的情况下，威斯布鲁克成为场上的主要控球者',\n",
       "  '今天他表现其实不错，三分球5投2中，也算正常发挥',\n",
       "  '湖人与阵容整齐的卫冕冠军雄鹿能打得难解难分',\n",
       "  '等詹姆斯等人复出后，他们还是能给人希望',\n",
       "  '在今天的比赛中，威斯布鲁克15次助攻是本赛季最高',\n",
       "  '此前他最多助攻14次',\n",
       "  '威少似乎正在适应新的角色，这对湖人来说是个好消息'],\n",
       " ['北京时间11月18日，篮网主场以109-99击败了骑士',\n",
       "  '篮网胜得很艰难',\n",
       "  '全队4人得了20分以上，哈登27分、10个篮板和7次助攻，杜兰特23分，帕蒂-米尔斯首发，得了21分',\n",
       "  '替补出场的阿尔德里奇得了24分7个篮板',\n",
       "  '骑士命中率只有39.3%，三分球44投仅11中',\n",
       "  '里奇-卢比奥得了25分5次助攻，达柳斯-加兰24分、6次助攻和5个篮板',\n",
       "  '凯文-勒夫复出，得了11分9个篮板，奥斯曼11分',\n",
       "  '探花秀莫布利和贾里特-阿伦等人缺阵'],\n",
       " ['北京时间11月18日，美媒StatMuse连续晒出金州勇士的数据并且证明球队已经是今年的夺冠大热门，具体情况如下：',\n",
       "  '本赛季赛季至今，勇士的防守排名联盟第一，进攻位居联盟第二',\n",
       "  '自从三分时代（1980年）以来，只有三支球队在攻防效率上都排在前两位，他们最终都拿到了总冠军',\n",
       "  '这三支球队分别是2017年的勇士、2015年的勇士和1996年的公牛',\n",
       "  '值得一提的是，在2017年，勇士的进攻第一防守第二，他们在常规赛取得67胜15负的战绩，最终4-1击败骑士夺冠',\n",
       "  '而2015年的勇士，进攻第二防守第一，他们在常规赛同样取得67胜15负的战绩，最终4-2击败骑士夺冠',\n",
       "  '1996年的公牛，进攻第一防守也是第一，球队在常规赛取得72胜10负的战绩，最终4-2击败超音速（雷霆前身）夺冠'],\n",
       " ['北京时间11月18日，亚特兰大老鹰队在主场以110比99取得胜利，本场比赛老鹰队球星特雷-杨拿到了18分外加11次助攻，成为球队取胜的关键球员',\n",
       "  '特雷-杨虽然今天手感不是特别顺，但是他的持球威胁带给对手的冲击力，牵制了很多防守精力',\n",
       "  '全场比赛特雷-杨送出了11次助攻，这是他本赛季第7次得分和助攻同时上双的比赛，是本赛季达成此成就最多的球员',\n",
       "  '上个赛季入选全明星的特雷-杨，还带领球队杀入到了分区决赛',\n",
       "  '本赛季已经确立球队核心位置的吹杨，场均能够贡献25.3分9.1次助攻3.7个篮板，投篮命中率保持在45.1%，投篮效率值达到了生涯新高'],\n",
       " ['北京时间11月18日，亚特兰大老鹰队主场以110比99赢下比赛，取得了三连胜',\n",
       "  '本场比赛赢下之后，球队的主场战绩来到了6胜1负',\n",
       "  '数据统计显示，从上个赛季到今天的比赛结束，老鹰队已经连续13次在主场战胜东部球队，这是球队自1996-97赛季之后，主场连胜东部球队的最长纪录',\n",
       "  '在1996-97赛季时老鹰队当时连续在主场赢东部球队场次达到了15场，当个赛季老鹰队取得了常规赛第二的战绩，季后赛中球队杀入到了分区半决赛'],\n",
       " ['北京时间11月18日，独行侠大将卢卡-东契奇缺阵，他们在客场败给了太阳',\n",
       "  '东契奇上一场比赛受伤，虽然不是很严重，坚持打完了比赛，但为了谨慎起见，独行侠今天让他休息',\n",
       "  '“东契奇正在接受治疗，我们明天再看看情况吧，”主帅基德说，据他透露，东契奇左踝和膝盖轻微受伤',\n",
       "  '少了东契奇，波尔津吉斯得了21分，而哈达威得了22分，他们以98-105败给太阳',\n",
       "  '独行侠已经很努力，前三节还领先5分，但最后一节丢了37分，遭到翻盘',\n",
       "  '独行侠最后一节打得本来就不好，再加上少了东契奇，更是雪上加霜']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有文章的句子聚合起来\n",
    "sentences = []\n",
    "\n",
    "\n",
    "for s in df['article_text']:\n",
    "    sentences.append(ss.segment(s))  \n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(items):\n",
    "    for x in items:\n",
    "        # 终止条件，检验是否为可迭代对象\n",
    "        if hasattr(x,'__iter__') and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_lwimHsHB9l"
   },
   "outputs": [],
   "source": [
    "# 拉平列表\n",
    "sentences = list(flatten(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['浓眉哥”安东尼-戴维斯自从出道以来，就倍受全美球迷追捧',\n",
       " '美国球评称他为美国内线最后的希望',\n",
       " '浓眉也确实争气，第二个赛季就打出了场均20.8分10篮板2.8封盖的华丽数据',\n",
       " '球迷们把浓眉选进了全明星阵容',\n",
       " '美国媒体也毫不客气，看浓眉就像看自家孩子一样']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对所有句子进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: jieba in /home/kkb/.local/lib/python3.8/site-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "\n",
    "def get_default_stop_words_file():\n",
    "    d = os.path.dirname(os.path.realpath('text-rank'))\n",
    "    return os.path.join(d, 'stopwords.txt')\n",
    "\n",
    "class WordSegmentation(object):\n",
    "    \"\"\" 分词 \"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words_file = None, allow_speech_tags = ['an', 'i', 'j', 'l', 'n', 'nr', 'nrfg', 'ns', 'nt', 'nz', 't', 'v', 'vd', 'vn', 'eng']):\n",
    "        \"\"\"\n",
    "        Keyword arguments:\n",
    "        stop_words_file    -- 保存停止词的文件路径，utf8编码，每行一个停止词。若不是str类型，则使用默认的停止词\n",
    "        allow_speech_tags  -- 词性列表，用于过滤\n",
    "        \"\"\"     \n",
    "        \n",
    "        allow_speech_tags = [as_text(item) for item in allow_speech_tags]\n",
    "\n",
    "        self.default_speech_tag_filter = allow_speech_tags\n",
    "        self.stop_words = set()\n",
    "        self.stop_words_file = get_default_stop_words_file()\n",
    "        if type(stop_words_file) is str:\n",
    "            self.stop_words_file = stop_words_file\n",
    "        for word in codecs.open(self.stop_words_file, 'r', 'utf-8', 'ignore'):\n",
    "            self.stop_words.add(word.strip())\n",
    "    \n",
    "    def segment(self, text, lower = True, use_stop_words = True, use_speech_tags_filter = False):\n",
    "        \"\"\"对一段文本进行分词，返回list类型的分词结果\n",
    "\n",
    "        Keyword arguments:\n",
    "        lower                  -- 是否将单词小写（针对英文）\n",
    "        use_stop_words         -- 若为True，则利用停止词集合来过滤（去掉停止词）\n",
    "        use_speech_tags_filter -- 是否基于词性进行过滤。若为True，则使用self.default_speech_tag_filter过滤。否则，不过滤。    \n",
    "        \"\"\"\n",
    "        text = as_text(text)\n",
    "        jieba_result = pseg.cut(text)\n",
    "        \n",
    "        if use_speech_tags_filter == True:\n",
    "            jieba_result = [w for w in jieba_result if w.flag in self.default_speech_tag_filter]\n",
    "        else:\n",
    "            jieba_result = [w for w in jieba_result]\n",
    "\n",
    "        # 去除特殊符号\n",
    "        word_list = [w.word.strip() for w in jieba_result if w.flag!='x']\n",
    "        word_list = [word for word in word_list if len(word)>0]\n",
    "        \n",
    "        if lower:\n",
    "            word_list = [word.lower() for word in word_list]\n",
    "\n",
    "        if use_stop_words:\n",
    "            word_list = [word.strip() for word in word_list if word.strip() not in self.stop_words]\n",
    "\n",
    "        return word_list\n",
    "        \n",
    "    def segment_sentences(self, sentences, lower=True, use_stop_words=True, use_speech_tags_filter=False):\n",
    "        \"\"\"将列表sequences中的每个元素/句子转换为由单词构成的列表。\n",
    "        \n",
    "        sequences -- 列表，每个元素是一个句子（字符串类型）\n",
    "        \"\"\"\n",
    "        \n",
    "        res = []\n",
    "        for sentence in sentences:\n",
    "            res.append(self.segment(text=sentence, \n",
    "                                    lower=lower, \n",
    "                                    use_stop_words=use_stop_words, \n",
    "                                    use_speech_tags_filter=use_speech_tags_filter))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.384 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['浓眉', '哥', '安东尼', '戴维斯', '出', '道', '以来', '倍受', '全美', '球迷', '追捧'], ['美国', '球', '评称', '美国', '内线', '最后', '希望'], ['浓眉', '确实', '争气', '第二个', '赛季', '打出', '了场', '均', '20.8', '分', '10', '篮板', '2.8', '封盖', '华丽', '数据'], ['球迷', '浓眉', '选进', '全', '明星阵容'], ['美国', '媒体', '毫不客气', '看', '浓眉', '看', '孩子'], ['2014', '2015', '赛季', '浓眉', '入行', '第三年', 'espn', '浓眉', '排在', '百大', '球员', '第三名'], ['身后', '杜兰特', '哈登', '库里', '威', '少', '干', '成名', '已', '久', '当时', '日后', '都', '拿到', 'mvp', '超级', '巨星'], ['espn', '当时', '解释', '浓眉', '当时', '年纪', '勒布朗', '詹姆斯', '魔术师', '胜利', '贡献', '值比', '浓眉', '更', '高'], ['从那以后', '浓眉', '哥', 'espn', '排名', '没', '低于', '过前', '六', '期间', '两次', '排在', '第二', '2015', '年', '2020', '年'], ['本赛季', '前', 'espn', '浓眉', '排在', '第九', '几乎', '浓眉', '成名', '后', '媒体', '最', '看衰', '一次'], ['新科', 'mvp', '约基', '奇', 'mvp', '排行榜', '第二', '恩', '比德', '率队', '夺冠', '字母', '哥', '都', '没什么', '争议', '排在', '字母', '哥', '之前'], ['浓眉', '哥', '全美', '内线', '最后', '希望'], ['考虑', '道', '大', '内线', '国籍', '说', '或许', '没', '问题', '美国', '真', '没', '出手', '内线', '梦之队', '不会', '召唤', '麦基', '水平', '球员'], ['还', '说', '浓眉', '全', 'nba', '最好', '内线'], ['看完', '今天', '雄鹿', '湖人', '比赛', '湖', '人', '球迷', '沉默不语'], ['现在', '浓眉', '哥', '距离', '最强', '内线', '称号', '真的', '越来越', '远'], ['赛季', '湖人队', '交易', '威', '少', '19', '20', '赛季', '夺冠', '阵容', '几乎', '全部', '拆散'], ['引援', '方面', '湖人', '引进', '几乎', '全', '后卫', '身高体重', '合格', '锋线', '阿里', '扎', '一个', '人', '1', '米', '93', '贝兹莫尔', '都', '顶', '前锋', '位置'], ['年轻', '防守', '都', '不及格', '安东尼', '都', '被迫', '最近', '逼成', '球队', '首发', '大', '前锋'], ['一套', '头重脚轻', '阵容', '素以', '防守', '调教', '起家', '沃格尔', '玩不转'], ['湖人队', '如今', '每场', '比赛', '对方', '篮下', '出手', '全', '联盟', '最多', '原因', '很', '简单', '球队', '外线', '防守', '几乎', '人', '一步', '四处', '漏风', '对方', '持球', '型', '锋线', '都', '轻松', '突破', '第一道', '锋线', '杀入', '禁区'], ['湖人队', '本赛季', '每场', '比赛', '丢', '112.3', '分', '已经', '排到', '联盟', '第三', '球队', '防守', '效率', '联盟', '15'], ['19', '20', '赛季', '湖人队', '防守', '效率', '联盟', '第三'], ['上', '赛季', '湖人', '防守', '效率', '联盟', '第一'], ['本赛季', '戴维斯', '最佳', '防守', '球员', '级别', '内线', '大闸', '防', '不住', '湖人', '到处', '漏人', '外线'], ['防守', '端', '浓眉', '已经', '算拼', '尽全力', '效果', '却', '惨不忍睹', '本赛季', '浓眉', '在场', '湖人', '每百', '回合', '丢', '109.5', '分'], ['不', '在场', '湖人', '每百', '回合', '丢', '98', '分'], ['增重', '以后', '浓眉', '速率', '变慢', '护框', '没', '以前', '厉害'], ['今天', '字母', '哥', '上半场', '几乎', '球球', '都', '冲着', '浓眉', '去', '浓眉', '却', '完全', '做', '不出', '有效', '干扰', '字母', '哥', '半场', '13', '投', '12', '中', '彻底', '打爆', '湖人', '禁区'], ['防守', '问题', '不该', '浓眉', '去', '背', '锅', '已经', '湖', '人', '防守', '最', '努力', '球员'], ['进攻', '端', '浓眉', '问题', '很大'], ['曾经', '浓眉', '号称', '全', '联盟', '第一', '吃', '饼', '侠', '能挡', '拆顺', '下攻', '框', '拉开', '空间', '杵', '外线', '百步穿杨'], ['能力', '外', '持球', '无球', '球场', '攻防', '两端', '影响', '巨大'], ['本赛季', '浓眉'], ['以前', '浓眉', '跳投', '不算', '十拿九稳', '绝对', '不能', '放', '点'], ['本赛季', '前', '27', '次', '三分', '出手', '只', '命中', '4', '球', '创下', 'nba', '历史', '赛季', '前', '25', '次', '出手', '三分', '以上', '命中率', '新低'], ['今天', '对阵', '雄鹿', '赛前', '媒体', '晒出', '数据', '字母', '哥', '最近', '三场', '命中', '三分', '数量', '6', '记', '等于', '浓眉', '整个', '赛季', '命中', '三分', '数量', '6', '记'], ['比赛', '中', '湖人', '还', '习惯', '外线', '放空', '字母', '哥', '字母', '哥', '毫不客气', '全场', '三分', '4', '投', '3', '中'], ['不止', '三分', '线', '外', '毫无', '杀伤', '命中率', '堪比麦', '基'], ['浓眉', '中距离', '效率', '不能', '算', '高'], ['中距离', '79', '投', '32', '中', '命中率', '刚', '40'], ['进攻', '方式', '上看', '詹姆斯', '不', '太', '发起', '进攻'], ['每场', '比赛', '浓眉', '4.2', '次', '低位', '背打', '2.1', '次面', '框', '单打'], ['鹈鹕', '时代', '浓眉', '得分', '受', '助攻', '率', '几乎', '都', '70', '以上', '本赛季', '得分', '受', '助攻', '率', '57.2', '生涯', '新低'], ['说明'], ['沃格尔', '设计', '战术', '很', '问题', '后卫', '没法', '浓眉', '发动', '有效', '配合', '浓眉', '轻松', '吃', '点', '饼'], ['球球', '浓眉', '费劲儿', '干凿'], ['真', '不是', '浓眉', '正确', '使用', '方法'], ['上', '赛季', '哈雷尔', '沃格尔', '手下', '创下', '场', '均', '挡', '拆顺', '下攻', '框数', '赛季', '新低'], ['手里', '有着', '哈雷尔', '施罗德', '都', '不', '知道', '打挡', '拆', '沃格尔', '或许', '真不知道', '内线', '舒服'], ['浓眉', '已经', '证明', '总冠军', '球队', '二当家', '比起', '约基', '奇', '字母', '哥', '带队', '能力', '确实', '差', '太远'], ['真的', '能怪', '浓眉'], ['28', '岁', '恐怕', '是学不来', '字母', '哥', '冲击', '型', '打法', '和约', '基奇', '策动', '全队'], ['湖人', '唯一', '做到', '只能', '快点', '詹姆斯', '回来', '再', '浓眉', '辛苦', '疲惫'], ['北京', '时间', '11', '月', '18', '日', '洛杉矶', '湖人', '102', '109', '不敌', '密尔沃基', '雄鹿'], ['赛后', '湖人', '主帅', '沃格尔', '谈到', '霍顿', '塔克'], ['沃格尔', '说', '塔克', '过去', '两场', '比赛', '中', '都', '表现', '很', '出色', '勒布朗', '回来', '充分', '理由', '首发', '出战'], ['替补席', '上', '塔克', '得分', '组织', '任务', '现在', '塔克', '足够', '优秀', '获得', '首发', '位置'], ['此役', '塔克', '出战', '38', '分', '58', '秒', '18', '投', '9', '中', '得到', '25', '分', '12', '篮板', '助攻', '抢断'], ['上', '一场', '比赛', '中', '塔克', '得到', '职业生涯', '最高', '28', '分'], ['值得一提的是', '下一场', '比赛', '中', '詹姆斯', '还', '未确定', '是否', '出战'], ['北京', '时间', '11', '月', '18', '日', '密尔沃基', '雄鹿', '109', '102', '战胜', '洛杉矶', '湖人', '字母', '哥', '今天', '比赛', '中', '23', '投', '18', '中', '三分球', '4', '投', '3', '中', '砍', '下', '47', '分', '9', '篮板'], ['美', '媒', '统计', '字母', '哥', '一项', '数据', '成为', '马布里', '之后', '能够', '砍', '下', '这项', '数据', '第一', '人'], ['数据', '统计', '今天', '比赛', '中', '字母', '哥', '轰下', '45', '分且', '命中率', '达到', '75', '以上'], ['字母', '哥', '成为', '马布里', '2005', '年', '对阵', '湖人', '比赛', '之后', '首位', '能够', '对阵', '湖人', '砍', '下', '45', '分且', '命中率', '75', '球员'], ['最终', '字母', '哥', '带领', '下', '雄鹿', '有惊无险', '主场', '战胜', '湖人'], ['北京', '时间', '11', '月', '18', '日', '密尔沃基', '雄鹿', '109', '102', '战胜', '洛杉矶', '湖人'], ['此役', '字母', '哥', '23', '投', '18', '中', '三分球', '4', '投', '3', '中', '砍', '下', '47', '分', '9', '篮板'], ['美', '媒', '统计', '一项', '字母', '哥', '浓眉', '哥', '数据', '对比'], ['数据', '统计', '字母', '哥', '最近', '3', '场', '比赛', '中投', '进', '6', '记', '三分球', '浓眉', '哥', '本赛季', '以来', '一共', '投进', '6', '记', '三分球'], ['相比', '字母', '哥', '出色', '状态', '浓眉', '哥', '本赛季', '以来', '状态', '本场', '比赛', '浓眉', '哥', '15', '投', '9', '中', '贡献', '18', '分', '9', '板', '4', '助', '2', '帽', '数据'], ['浓眉', '哥', '三分球', '1', '投', '0', '中'], ['北京', '时间', '11', '月', '18', '日', '湖人', '败给', '雄鹿', '拉塞尔', '威', '斯布鲁克', '表现', '不是', '特别', '抢眼', '失误', '赛季', '第二', '少', '助攻', '赛季', '最高'], ['威', '斯布鲁克', '全场', '16', '投', '7', '中', '19', '分', '15', '次', '助攻', '4', '篮板'], ['数据', '看', '不是', '赛季', '最出色', '全场', '3', '次', '失误', '巨大', '进步'], ['此前', '比赛', '威', '少', '一直', '失误', '出名', '最', '一场', '上双'], ['9', '次', '8', '次', '7', '次'], ['一场', '失误', '2', '次', '赛季', '最低'], ['两次', '失误', '3', '次', '今天', '第三次'], ['詹姆斯', '缺阵', '情况', '下', '威', '斯布鲁克', '成为', '场上', '主要', '控球'], ['今天', '表现', '其实', '不错', '三分球', '5', '投', '2', '中', '算', '正常', '发挥'], ['湖人', '阵容', '整齐', '卫冕冠军', '雄鹿', '打得', '难解难分'], ['詹姆斯', '人', '复出', '后', '人', '希望'], ['今天', '比赛', '中', '威', '斯布鲁克', '15', '次', '助攻', '本赛季', '最高'], ['此前', '最', '助攻', '14', '次'], ['威', '少', '似乎', '正在', '适应', '新', '角色', '湖人', '来说', '好消息'], ['北京', '时间', '11', '月', '18', '日', '篮网', '主场', '109', '99', '击败', '骑士'], ['篮网', '胜', '很', '艰难'], ['全队', '4', '人', '20', '分', '以上', '哈登', '27', '分', '10', '篮板', '7', '次', '助攻', '杜兰特', '23', '分', '帕蒂', '米尔斯', '首发', '21', '分'], ['替补', '出场', '阿尔德', '里奇', '24', '分', '7', '篮板'], ['骑士', '命中率', '39.3', '三分球', '44', '投', '仅', '11', '中'], ['里奇', '卢比', '奥得', '25', '分', '5', '次', '助攻', '达柳斯', '加兰', '24', '分', '6', '次', '助攻', '5', '篮板'], ['凯文', '勒夫', '复出', '11', '分', '9', '篮板', '奥斯曼', '11', '分'], ['探花', '秀', '莫布利', '贾', '里特', '阿伦', '人', '缺阵'], ['北京', '时间', '11', '月', '18', '日', '美', '媒', 'statmuse', '连续', '晒出', '金州', '勇士', '数据', '证明', '球队', '已经', '今年', '夺冠', '大', '热门', '具体情况', '如下'], ['本赛季', '赛季', '至今', '勇士', '防守', '排名', '联盟', '第一', '进攻', '位居', '联盟', '第二'], ['三分', '时代', '1980', '年', '以来', '三支', '球队', '攻防', '效率', '上', '都', '排', '前', '两位', '最终', '都', '拿到', '总冠军'], ['三支', '球队', '分别', '2017', '年', '勇士', '2015', '年', '勇士', '1996', '年', '公牛'], ['值得一提的是', '2017', '年', '勇士', '进攻', '第一', '防守', '第二', '常规赛', '取得', '67', '胜', '15', '负', '战绩', '最终', '击败', '骑士', '夺冠'], ['2015', '年', '勇士', '进攻', '第二', '防守', '第一', '常规赛', '同样', '取得', '67', '胜', '15', '负', '战绩', '最终', '击败', '骑士', '夺冠'], ['1996', '年', '公牛', '进攻', '第一', '防守', '第一', '球队', '常规赛', '取得', '72', '胜', '10', '负', '战绩', '最终', '击败', '超音速', '雷霆', '前身', '夺冠'], ['北京', '时间', '11', '月', '18', '日', '亚特兰大', '老鹰队', '主场', '110', '99', '取得胜利', '本场', '比赛', '老鹰队', '球星', '特雷', '杨', '拿到', '18', '分', '外加', '11', '次', '助攻', '成为', '球队', '取胜', '关键', '球员'], ['特雷', '杨', '今天', '手感', '不是', '特别', '持球', '威胁', '带给', '对手', '冲击力', '牵制', '很多', '防守', '精力'], ['全场', '比赛', '特雷', '杨', '送出', '11', '次', '助攻', '本赛季', '7', '次', '得分', '助攻', '上', '双', '比赛', '本赛季', '达成', '成就', '最多', '球员'], ['上', '赛季', '入选', '全明星', '特雷', '杨', '还', '带领', '球队', '杀入', '分区', '决赛'], ['本赛季', '已经', '确立', '球队', '核心', '位置', '吹', '杨', '场', '均', '能够', '贡献', '25.3', '分', '9.1', '次', '助攻', '3.7', '篮板', '投篮', '命中率', '保持', '45.1', '投篮', '效率', '值', '达到', '生涯', '新高'], ['北京', '时间', '11', '月', '18', '日', '亚特兰大', '老鹰队', '主场', '110', '99', '赢下', '比赛', '取得', '三连胜'], ['本场', '比赛', '赢下', '之后', '球队', '主场', '战绩', '来到', '6', '胜', '1', '负'], ['数据', '统计', '显示', '上个', '赛季', '今天', '比赛', '结束', '老鹰队', '已经', '连续', '13', '次在', '主场', '战胜', '东部', '球队', '球队', '1996', '97', '赛季', '之后', '主场', '连胜', '东部', '球队', '最长', '纪录'], ['1996', '97', '赛季', '时', '老鹰队', '当时', '连续', '主场', '赢', '东部', '球队', '场次', '达到', '15', '场', '赛季', '老鹰队', '取得', '常规赛', '第二', '战绩', '季后赛', '中', '球队', '杀入', '分区', '半决赛'], ['北京', '时间', '11', '月', '18', '日', '独行侠', '大将', '卢卡', '东契奇', '缺阵', '客场', '败给', '太阳'], ['东契奇', '上', '一场', '比赛', '受伤', '不是', '很', '严重', '坚持', '打完', '比赛', '谨慎', '见', '独行侠', '今天', '休息'], ['东契', '奇正', '接受', '治疗', '明天', '再', '看看', '情况', '主帅', '基德', '说', '透露', '东契奇', '左踝', '膝盖', '轻微', '受伤'], ['少', '东契奇', '波尔', '津', '吉斯', '21', '分', '哈达威', '22', '分', '98', '105', '败给', '太阳'], ['独行侠', '已经', '很', '努力', '前', '三节', '还', '领先', '5', '分', '最后', '一节', '丢', '37', '分', '遭到', '翻盘'], ['独行侠', '最后', '一节', '本来', '不好', '再', '加上', '少', '东契奇', '更是', '雪上加霜']]\n"
     ]
    }
   ],
   "source": [
    "ws = WordSegmentation()\n",
    "words = ws.segment_sentences(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下载 Glove 中文词向量\n",
    "\n",
    "[中文词向量下载地址](https://github.com/Embedding/Chinese-Word-Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "a5Skjq6DJUtQ",
    "outputId": "eeba2eed-d0ed-42f8-f45f-dea8a99bcb5b"
   },
   "outputs": [],
   "source": [
    "# !wget http://aimaksen.bslience.cn/sgns.wiki.word.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "K_GXHzwDJq-2",
    "outputId": "9387687a-04f5-41d9-f525-b6bca3b8f97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bunzip2: Can't open input file sgns.wiki.word.bz2: No such file or directory.\n"
     ]
    }
   ],
   "source": [
    "! bunzip2 sgns.wiki.word.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsXIa7CBKsWQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('sgns.wiki.word', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352163"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子的向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "\n",
    "for i in words:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i])/(len(i)+0.001)\n",
    "    else:\n",
    "        v = np.zeros((300,))\n",
    "\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.13509555,  0.16762212, -0.13855885,  0.04146333,  0.10073085,\n",
       "         0.00951659, -0.2506782 , -0.10022862, -0.02341496, -0.13381875,\n",
       "        -0.07906353,  0.00769676,  0.00259358,  0.28280473, -0.09426296,\n",
       "         0.07290537, -0.10634705, -0.23625089,  0.12549394,  0.15635297,\n",
       "         0.03257395,  0.17713143,  0.05828978,  0.10605808,  0.01918762,\n",
       "        -0.1489031 ,  0.04032315, -0.19743878,  0.05144577, -0.1445034 ,\n",
       "         0.11960395, -0.12295799, -0.07967439,  0.08989102,  0.17582846,\n",
       "         0.11723988,  0.22522315,  0.0086774 , -0.28286803,  0.06486292,\n",
       "        -0.08689301, -0.15750304, -0.06989282,  0.00045659,  0.19347657,\n",
       "         0.07916716,  0.0708072 ,  0.15490519, -0.01492464, -0.27415547,\n",
       "        -0.13504653,  0.10740814, -0.0360749 ,  0.07881547, -0.00441814,\n",
       "         0.09027706, -0.07603191,  0.003977  , -0.2035883 ,  0.360943  ,\n",
       "        -0.05830797, -0.14227706, -0.09754686, -0.20729113, -0.1192568 ,\n",
       "        -0.09305036,  0.05117926, -0.06525925,  0.07354712, -0.17181075,\n",
       "         0.35894692, -0.23346402,  0.13197125,  0.21379028,  0.08723853,\n",
       "        -0.06365421, -0.05336942, -0.19552012,  0.07046486,  0.05464394,\n",
       "         0.14449024, -0.17251876, -0.0761617 ,  0.00503572, -0.05814625,\n",
       "        -0.13151386, -0.27047905, -0.06072748, -0.00576784, -0.1177382 ,\n",
       "         0.25635797,  0.1171319 ,  0.0999191 , -0.12295528, -0.21153821,\n",
       "         0.22525641, -0.02725425, -0.09373347, -0.07069684, -0.05033251,\n",
       "         0.1856472 , -0.02435542, -0.0587532 ,  0.26753792,  0.02025198,\n",
       "         0.04743278,  0.04090365, -0.04665103,  0.09714407, -0.01659876,\n",
       "         0.0266752 , -0.17991374, -0.01071448, -0.15454312,  0.25043866,\n",
       "         0.43460345,  0.01474375,  0.0461098 ,  0.15841578, -0.15565194,\n",
       "        -0.06289783, -0.02628243,  0.00809572, -0.04769365,  0.02460229,\n",
       "        -0.12543467, -0.00281883, -0.01394618,  0.09696819,  0.1084325 ,\n",
       "         0.01906599, -0.2867447 , -0.10135323, -0.24344766,  0.04033015,\n",
       "         0.04128315, -0.05837388,  0.08297327,  0.2794705 , -0.13941559,\n",
       "        -0.15328632, -0.13555793,  0.14016499,  0.07924489,  0.10171811,\n",
       "         0.06953777, -0.1268401 ,  0.07658204,  0.03076729,  0.10021307,\n",
       "         0.01770448, -0.33222094,  0.04863576,  0.15223905, -0.15276183,\n",
       "        -0.11217944, -0.03215662,  0.05045168, -0.12894827, -0.343859  ,\n",
       "         0.02066021, -0.3282905 , -0.04642204, -0.06971157,  0.12696354,\n",
       "         0.01576257,  0.08918887, -0.08602082,  0.06400055,  0.15750486,\n",
       "         0.30169475, -0.0039171 , -0.18575159,  0.01238169, -0.40254894,\n",
       "        -0.141409  ,  0.07401473,  0.03987074, -0.11419588, -0.15887754,\n",
       "         0.05148822, -0.01875102,  0.02717644, -0.05305863, -0.13275129,\n",
       "        -0.05569985, -0.26817498,  0.06542377, -0.14898562, -0.11249477,\n",
       "         0.11944488, -0.11897663, -0.09295828,  0.01611635, -0.03319071,\n",
       "         0.00288837, -0.05361967, -0.03106517,  0.00706036,  0.2700578 ,\n",
       "         0.11711643, -0.23538065,  0.12197718,  0.18905644,  0.07407799,\n",
       "         0.00636697, -0.07461995,  0.03594837,  0.07226925, -0.05008555,\n",
       "        -0.10755958, -0.19291681,  0.1718133 , -0.13647804, -0.00956304,\n",
       "        -0.0515165 ,  0.06519835, -0.03368402,  0.13403152, -0.13362785,\n",
       "        -0.13572821, -0.12631714, -0.07334377,  0.09584437, -0.32155013,\n",
       "         0.3766374 ,  0.04800137, -0.18526761,  0.07143886,  0.12722915,\n",
       "         0.11881046,  0.21425214,  0.13264212,  0.05998182,  0.05143896,\n",
       "        -0.12100418,  0.1092437 ,  0.09605382, -0.11022189,  0.00650368,\n",
       "        -0.02458804,  0.09031843,  0.22532241, -0.02278901, -0.06439141,\n",
       "        -0.00650595, -0.04495055, -0.12741522, -0.02247287, -0.11328688,\n",
       "         0.05219198,  0.06857604,  0.18835996,  0.050006  ,  0.03604663,\n",
       "         0.09479383,  0.043989  ,  0.12738115,  0.01084102,  0.11855559,\n",
       "         0.17303482, -0.00389547, -0.09805381,  0.3570298 , -0.02047077,\n",
       "        -0.10453486,  0.13102563,  0.01835233,  0.03521962,  0.0343967 ,\n",
       "        -0.07573482,  0.02742405,  0.23260103,  0.17254278,  0.03394945,\n",
       "        -0.01697736,  0.3309227 , -0.11147204, -0.15203898,  0.33821425,\n",
       "         0.13693403, -0.13509789, -0.11738878,  0.05725752,  0.01035006,\n",
       "        -0.04849378, -0.13401954,  0.34861633,  0.13491692, -0.2958328 ,\n",
       "        -0.10799026, -0.15621352, -0.09527698,  0.11761575, -0.11634487,\n",
       "         0.10622606,  0.18933976, -0.18071283,  0.01593283,  0.05430052],\n",
       "       dtype=float32),\n",
       " array([-6.41059828e-02, -3.44853590e-02, -1.76629339e-01, -2.63226256e-01,\n",
       "         9.20985551e-02,  3.66706232e-02, -3.50917148e-01, -2.61055852e-01,\n",
       "        -5.12222515e-02, -7.26840432e-02, -6.19170154e-02, -4.09410056e-02,\n",
       "        -1.55747752e-02,  1.27053139e-01,  1.17560493e-01,  1.78830430e-04,\n",
       "        -2.58840175e-02, -1.76677338e-01,  3.18296524e-01,  1.44903016e-01,\n",
       "         2.03002716e-01,  2.06327646e-02, -9.25853470e-02,  1.32718756e-01,\n",
       "        -5.46157675e-02, -6.38783014e-02, -1.88181246e-01, -2.54865875e-01,\n",
       "         3.93556695e-02, -1.37586489e-01,  8.88133115e-02,  5.14398060e-03,\n",
       "         4.39815746e-02,  2.14545916e-01,  1.48198827e-02,  3.73753752e-02,\n",
       "         1.46048848e-01, -4.65909139e-02, -1.33509359e-01, -1.74448795e-01,\n",
       "        -2.97000381e-02, -5.43450909e-02, -2.22675192e-01, -6.67990280e-02,\n",
       "         9.90695613e-02,  1.81570494e-01,  5.97071912e-02, -6.09228665e-02,\n",
       "        -1.99285389e-01, -1.16202542e-01,  3.76006278e-02,  1.29059993e-01,\n",
       "         1.42236390e-01,  1.37660479e-01, -9.50608431e-02,  8.87030943e-03,\n",
       "        -3.98077434e-02,  2.69821473e-02, -1.25840593e-01,  2.42073269e-01,\n",
       "        -5.46836220e-03, -2.13055845e-01,  2.27813139e-02, -7.75880599e-02,\n",
       "         2.61077241e-03, -2.22131105e-02, -5.78063116e-02, -7.69594362e-02,\n",
       "         2.50349501e-01, -1.39769023e-01,  1.61071276e-01, -2.01424084e-01,\n",
       "        -1.36409371e-03,  2.70485504e-01,  1.20761038e-01,  6.27376057e-02,\n",
       "         2.93062431e-02, -1.44378370e-01,  6.24412200e-02, -1.55881591e-01,\n",
       "         1.10737323e-01, -1.93779734e-01, -2.66560489e-02,  4.94529403e-02,\n",
       "         1.49220826e-01, -1.93901299e-01, -2.01022711e-01, -3.94030846e-01,\n",
       "         8.68065985e-02,  1.09963294e-01,  1.59797744e-01, -3.50078550e-02,\n",
       "         1.42971577e-01,  9.84260841e-02, -1.76535359e-01,  2.57374624e-02,\n",
       "         1.98342091e-01, -1.03140552e-01, -1.60086418e-01,  1.72502486e-02,\n",
       "        -1.84852166e-02,  1.32823881e-01, -1.00751465e-01,  1.22167833e-01,\n",
       "         9.81448341e-02,  1.24678469e-01,  8.71578351e-02, -1.63500785e-01,\n",
       "        -1.71581343e-01,  6.48420221e-02,  1.08599914e-01, -3.11288384e-01,\n",
       "         2.21492209e-01, -1.25240543e-01,  8.68084540e-02,  3.87363228e-01,\n",
       "         1.96473396e-02,  1.62905730e-01, -3.30758442e-02, -5.92133994e-02,\n",
       "        -1.49271675e-01,  2.39752892e-01, -1.40845612e-02,  5.39790013e-02,\n",
       "         5.50979880e-02, -1.26425791e-01, -1.69390231e-01,  1.85667755e-02,\n",
       "         1.05757895e-01,  1.57508498e-01,  3.38323101e-02, -5.12920998e-02,\n",
       "        -8.56353395e-02, -9.54459378e-02, -1.30955581e-02,  1.17258104e-01,\n",
       "         1.51716896e-01,  6.81455515e-02,  3.78129401e-01, -9.88960143e-02,\n",
       "        -7.13778009e-02, -7.41034128e-02,  1.16181830e-01,  1.37140406e-01,\n",
       "         1.95224397e-01,  6.18112394e-03, -5.76599102e-02, -8.22261093e-02,\n",
       "        -5.33272385e-02,  2.59693904e-01, -1.98590213e-02, -2.00156547e-01,\n",
       "        -3.03705468e-01,  2.63290939e-02, -2.11204112e-02, -6.22659600e-02,\n",
       "        -1.13982004e-01,  4.00872750e-02, -1.06911297e-01, -1.69815885e-01,\n",
       "         1.57034712e-01,  5.57191599e-03,  4.65394914e-02, -6.28600206e-02,\n",
       "         1.04659907e-01,  1.83682187e-01,  8.05052159e-02, -4.74087959e-02,\n",
       "        -3.08543027e-02, -2.03059577e-02,  1.33945724e-01,  1.21001712e-01,\n",
       "         3.50121427e-02, -8.18047421e-02, -1.34534788e-01, -1.80953154e-01,\n",
       "        -7.54327982e-02, -4.04227703e-03, -7.37093276e-02, -1.67518349e-01,\n",
       "        -1.43511072e-01,  9.66810528e-02, -3.61538365e-02, -2.13076421e-01,\n",
       "        -1.44118697e-01, -8.51659793e-02, -3.07399230e-01,  2.00196116e-01,\n",
       "        -1.33706612e-02, -7.37614673e-02,  4.33958019e-02, -5.47498881e-02,\n",
       "        -8.06261948e-02,  4.49082987e-02,  7.27200420e-02, -1.08742321e-01,\n",
       "        -1.21112273e-01,  1.30246397e-01,  1.38338945e-01,  1.56423367e-01,\n",
       "         1.76728902e-01, -1.43611912e-01,  3.39489211e-01,  2.49558922e-01,\n",
       "         1.11715531e-02,  1.18283814e-01, -4.22873915e-02, -1.19260095e-02,\n",
       "         6.11211271e-02,  8.55673481e-02, -1.38995718e-01, -1.13018712e-01,\n",
       "         5.82459913e-03, -1.02543208e-01,  6.05830515e-02,  1.63870443e-01,\n",
       "        -1.69050172e-02, -1.40045736e-02, -1.53512640e-01, -1.34465218e-01,\n",
       "        -2.13645047e-01, -8.45552041e-02,  8.29467204e-02,  6.30895285e-03,\n",
       "        -1.17766605e-01,  5.50813167e-01, -6.80218571e-02,  1.55500673e-02,\n",
       "         2.27563250e-02,  6.43673749e-02,  1.78774177e-01,  2.78414520e-01,\n",
       "        -1.74168835e-01,  4.10862751e-02,  1.06710448e-02, -2.78673479e-01,\n",
       "        -1.02770655e-03,  3.86939006e-02,  3.40532793e-02,  9.44593782e-03,\n",
       "         3.84946426e-02,  6.13899432e-02,  7.49220131e-02, -4.26367643e-02,\n",
       "        -1.14403229e-01,  8.80699906e-02, -2.30209399e-01,  1.47061418e-01,\n",
       "         1.00938863e-01, -1.25945866e-01, -2.08304538e-02, -6.06200139e-04,\n",
       "         1.05700656e-04, -2.28704612e-01, -3.21232668e-02,  5.07900301e-02,\n",
       "         7.60171362e-02,  1.78655189e-01, -3.94272013e-03, -1.28008423e-01,\n",
       "        -8.55517796e-02, -3.51344090e-02,  4.51893970e-02,  2.20598463e-02,\n",
       "        -1.65429361e-01, -1.56671332e-01,  5.17393267e-02, -6.70334270e-02,\n",
       "         2.75967707e-02,  2.54517648e-01, -2.01178547e-01, -1.13487492e-01,\n",
       "         3.83445360e-01,  9.36867590e-02, -5.24467919e-02, -1.23798044e-02,\n",
       "         2.06960576e-01,  3.41702597e-02,  1.60477648e-01,  2.24749036e-01,\n",
       "        -1.24790744e-01, -1.26366663e-01, -2.67206973e-01, -5.21125563e-02,\n",
       "         5.18005969e-02, -1.03942005e-01, -1.11046138e-01,  1.58061992e-01,\n",
       "         1.28651722e-02, -3.24097706e-01, -1.48558632e-01, -1.03270517e-04,\n",
       "        -2.16060277e-01,  7.92069685e-02,  9.80254239e-02,  5.57513220e-02,\n",
       "         2.27203401e-01, -1.94653480e-01,  9.53878221e-03,  5.91878316e-02])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bh9L2pqL3gp"
   },
   "source": [
    "## 计算相似度矩阵\n",
    "\n",
    "计算句子之间的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_fNZpOLxbM"
   },
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 116)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-learn in /home/kkb/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kkb/.local/lib/python3.8/site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/kkb/.local/lib/python3.8/site-packages (from scikit-learn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/kkb/.local/lib/python3.8/site-packages (from scikit-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/kkb/.local/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVeHkvf0MO1j"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTAAe-q3L4xM"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 116)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PageRank 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: networkx in /home/kkb/.local/lib/python3.8/site-packages (2.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install networkx -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAQUnNRWL_tA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.008374943442106517,\n",
       " 1: 0.007978205307968224,\n",
       " 2: 0.009207523379251372,\n",
       " 3: 0.006999123048343878,\n",
       " 4: 0.007149307086544447,\n",
       " 5: 0.008328962440949581,\n",
       " 6: 0.008977194683099255,\n",
       " 7: 0.008478785557742802,\n",
       " 8: 0.008604739608661444,\n",
       " 9: 0.008800946945173844,\n",
       " 10: 0.008714062958314782,\n",
       " 11: 0.008096660284159158,\n",
       " 12: 0.00904684969219654,\n",
       " 13: 0.008186422791110759,\n",
       " 14: 0.00853655873416692,\n",
       " 15: 0.008113322342993251,\n",
       " 16: 0.009161725924405407,\n",
       " 17: 0.009430311274159294,\n",
       " 18: 0.009066001076271061,\n",
       " 19: 0.008326791725156046,\n",
       " 20: 0.00964562678926463,\n",
       " 21: 0.009474077745634005,\n",
       " 22: 0.008858956906685742,\n",
       " 23: 0.008791475465876858,\n",
       " 24: 0.009160722116273376,\n",
       " 25: 0.009604468025192433,\n",
       " 26: 0.008851034361431084,\n",
       " 27: 0.007950903823582283,\n",
       " 28: 0.009443924083227162,\n",
       " 29: 0.008765515542341049,\n",
       " 30: 0.006984972531393602,\n",
       " 31: 0.008557127675575328,\n",
       " 32: 0.00831015269752609,\n",
       " 33: 0.007919780606325973,\n",
       " 34: 0.008297521189016216,\n",
       " 35: 0.009489972168748223,\n",
       " 36: 0.009439877133544246,\n",
       " 37: 0.009171886057256969,\n",
       " 38: 0.0079098902173281,\n",
       " 39: 0.007753420510452041,\n",
       " 40: 0.008537147214003324,\n",
       " 41: 0.007182760032820825,\n",
       " 42: 0.008654936617887346,\n",
       " 43: 0.008928461449071182,\n",
       " 44: 0.004528248571332124,\n",
       " 45: 0.008966524501202146,\n",
       " 46: 0.006833305168130042,\n",
       " 47: 0.006843230428798646,\n",
       " 48: 0.009121275055668437,\n",
       " 49: 0.008556920612218582,\n",
       " 50: 0.009398533005021846,\n",
       " 51: 0.006303672362300705,\n",
       " 52: 0.009173197185782557,\n",
       " 53: 0.00854994582669567,\n",
       " 54: 0.008650696268678959,\n",
       " 55: 0.008292104098649009,\n",
       " 56: 0.009299533131872807,\n",
       " 57: 0.008916078590853195,\n",
       " 58: 0.009012608617312113,\n",
       " 59: 0.00888887028826388,\n",
       " 60: 0.008834906035577102,\n",
       " 61: 0.009416837781912045,\n",
       " 62: 0.008837959034575044,\n",
       " 63: 0.009057809998629448,\n",
       " 64: 0.00966940072157536,\n",
       " 65: 0.00874086604821214,\n",
       " 66: 0.008690498138096494,\n",
       " 67: 0.00902153823934075,\n",
       " 68: 0.007340146737387774,\n",
       " 69: 0.009371872128107854,\n",
       " 70: 0.009503691520490078,\n",
       " 71: 0.008374791252747826,\n",
       " 72: 0.00973212256217756,\n",
       " 73: 0.008814498017154086,\n",
       " 74: 0.009347048299433726,\n",
       " 75: 0.008651586938256425,\n",
       " 76: 0.00646616692602564,\n",
       " 77: 0.008531117234439176,\n",
       " 78: 0.007811540660179949,\n",
       " 79: 0.008756281092986973,\n",
       " 80: 0.009085861582084872,\n",
       " 81: 0.008589971881453912,\n",
       " 82: 0.0068844108576153916,\n",
       " 83: 0.009211304648388409,\n",
       " 84: 0.00829784030150837,\n",
       " 85: 0.00848917264381897,\n",
       " 86: 0.00867049576982748,\n",
       " 87: 0.008243730549643115,\n",
       " 88: 0.008824265702926746,\n",
       " 89: 0.008790242290783122,\n",
       " 90: 0.008882922458699318,\n",
       " 91: 0.008695723326670549,\n",
       " 92: 0.00850370389978832,\n",
       " 93: 0.00775671504758006,\n",
       " 94: 0.009310309564250309,\n",
       " 95: 0.008529484180848684,\n",
       " 96: 0.009225567634294139,\n",
       " 97: 0.008270112776554756,\n",
       " 98: 0.0092242502362042,\n",
       " 99: 0.00925754883042577,\n",
       " 100: 0.009323649200963388,\n",
       " 101: 0.009752093701863845,\n",
       " 102: 0.00899050393086021,\n",
       " 103: 0.009380802526721412,\n",
       " 104: 0.008815020349923795,\n",
       " 105: 0.009350568362286609,\n",
       " 106: 0.008746321939804716,\n",
       " 107: 0.008927710053571635,\n",
       " 108: 0.009343154865158703,\n",
       " 109: 0.009333973715995397,\n",
       " 110: 0.008719083476143079,\n",
       " 111: 0.008877426320362586,\n",
       " 112: 0.00838723067218433,\n",
       " 113: 0.008552202184926841,\n",
       " 114: 0.00908510596628609,\n",
       " 115: 0.008103046206262196}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQCcvT3yO5Xj"
   },
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "jwxtPBlgO_Gk",
    "outputId": "94f7a32b-fcd3-4295-ec49-4fb69e49342e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京时间11月18日，亚特兰大老鹰队在主场以110比99取得胜利，本场比赛老鹰队球星特雷-杨拿到了18分外加11次助攻，成为球队取胜的关键球员\n",
      "北京时间11月18日，湖人败给雄鹿，拉塞尔-威斯布鲁克表现不是特别抢眼，但失误赛季第二少，助攻赛季最高\n",
      "因此，字母哥成为了马布里在2005年对阵湖人的比赛之后，首位能够对阵湖人的时候砍下45+分且命中率75+%的球员\n",
      "湖人队如今每场比赛让对方在篮下的出手是全联盟最多的，原因很简单：球队的外线防守几乎被人一步就过，四处漏风，对方的持球型锋线谁都可以轻松突破第一道锋线杀入禁区\n",
      "在防守端浓眉已经算拼尽全力了，可是效果却惨不忍睹：本赛季浓眉在场的时候，湖人每百回合丢109.5分\n",
      "因此，相比起字母哥的出色状态，浓眉哥在本赛季以来的状态一般，而本场比赛浓眉哥则是15投9中，贡献了18分9板4助2帽的数据\n",
      "可本赛季，他前27次三分出手只命中了4球，创下了NBA历史赛季前25次出手三分以上的命中率新低\n",
      "湖人队本赛季每场比赛丢112.3分已经排到了联盟第三，球队的防守效率也只是联盟第15\n",
      "今天字母哥上半场几乎球球都冲着浓眉去，浓眉却完全做不出有效干扰，让字母哥半场13投12中，彻底打爆了湖人禁区\n",
      "今天对阵雄鹿赛前，媒体晒出数据：字母哥最近三场命中的三分数量（6记）等于浓眉整个赛季命中的三分数量（6记）\n"
     ]
    }
   ],
   "source": [
    "# Specify number of sentences to form the summary\n",
    "sn = 10\n",
    "\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank 的处理流程（single-domain-single-document）\n",
    "\n",
    "- 1.拿出一篇文章\n",
    "- 2.把文本分割成一个个的句子并分词\n",
    "- 3.对每个单词和句子做向量化\n",
    "- 4.计算单词/句子之间的相似度，并存储到矩阵中\n",
    "- 5.把相似度矩阵转化为图，其中单词/句子是节点，相似度是节点间的权重\n",
    "- 6.取到 top-k 权重的单词/句子，作为最终的关键词和关键句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取第一篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北京时间11月18日，密尔沃基雄鹿以109-102战胜了洛杉矶湖人，字母哥在今天的比赛中23投18中，三分球 4投3中，砍下47分9篮板。因此，美媒统计了关于字母哥的一项数据，他也成为了马布里之后能够砍下这项数据的第一人。\\r\\n\\u3000\\u3000根据数据统计，在今天的比赛中，字母哥轰下了45分且命中率达到75%以上。因此，字母哥成为了马布里在2005年对阵湖人的比赛之后，首位能够对阵湖人的时候砍下45+分且命中率75+%的球员。\\r\\n\\u3000\\u3000最终，在字母哥的带领下，雄鹿有惊无险在主场战胜了湖人。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nba-articles.csv')\n",
    "article1 = df['article_text'][2]\n",
    "article1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对所有句子进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['北京',\n",
       "  '时间',\n",
       "  '11',\n",
       "  '月',\n",
       "  '18',\n",
       "  '日',\n",
       "  '密尔沃基',\n",
       "  '雄鹿',\n",
       "  '109',\n",
       "  '102',\n",
       "  '战胜',\n",
       "  '洛杉矶',\n",
       "  '湖人',\n",
       "  '字母',\n",
       "  '哥',\n",
       "  '今天',\n",
       "  '比赛',\n",
       "  '中',\n",
       "  '23',\n",
       "  '投',\n",
       "  '18',\n",
       "  '中',\n",
       "  '三分球',\n",
       "  '4',\n",
       "  '投',\n",
       "  '3',\n",
       "  '中',\n",
       "  '砍',\n",
       "  '下',\n",
       "  '47',\n",
       "  '分',\n",
       "  '9',\n",
       "  '篮板'],\n",
       " ['美',\n",
       "  '媒',\n",
       "  '统计',\n",
       "  '字母',\n",
       "  '哥',\n",
       "  '一项',\n",
       "  '数据',\n",
       "  '成为',\n",
       "  '马布里',\n",
       "  '之后',\n",
       "  '能够',\n",
       "  '砍',\n",
       "  '下',\n",
       "  '这项',\n",
       "  '数据',\n",
       "  '第一',\n",
       "  '人'],\n",
       " ['数据',\n",
       "  '统计',\n",
       "  '今天',\n",
       "  '比赛',\n",
       "  '中',\n",
       "  '字母',\n",
       "  '哥',\n",
       "  '轰下',\n",
       "  '45',\n",
       "  '分且',\n",
       "  '命中率',\n",
       "  '达到',\n",
       "  '75',\n",
       "  '以上'],\n",
       " ['字母',\n",
       "  '哥',\n",
       "  '成为',\n",
       "  '马布里',\n",
       "  '2005',\n",
       "  '年',\n",
       "  '对阵',\n",
       "  '湖人',\n",
       "  '比赛',\n",
       "  '之后',\n",
       "  '首位',\n",
       "  '能够',\n",
       "  '对阵',\n",
       "  '湖人',\n",
       "  '砍',\n",
       "  '下',\n",
       "  '45',\n",
       "  '分且',\n",
       "  '命中率',\n",
       "  '75',\n",
       "  '球员'],\n",
       " ['最终', '字母', '哥', '带领', '下', '雄鹿', '有惊无险', '主场', '战胜', '湖人']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SentenceSegmentation()\n",
    "sentences = ss.segment(article1)\n",
    "ws = WordSegmentation()\n",
    "words = ws.segment_sentences(sentences)\n",
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(items):\n",
    "    for x in items:\n",
    "        # 终止条件，检验是否为可迭代对象\n",
    "        if hasattr(x,'__iter__') and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_words = list(set(flatten(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['102', '三分球', '洛杉矶', '篮板', '能够']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用向量表示单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = []\n",
    "\n",
    "for i in flatten_words:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i])/(len(i)+0.001)\n",
    "    word_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算相似度矩阵\n",
    "\n",
    "计算单词之间的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 57)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(flatten_words), len(flatten_words)])\n",
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(flatten_words)):\n",
    "    for j in range(len(flatten_words)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(word_vectors[i].reshape(1,300), word_vectors[j].reshape(1,300))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 57)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PageRank 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "45\n",
      "18\n",
      "102\n",
      "47\n",
      "2005\n",
      "75\n",
      "4\n",
      "23\n",
      "三分球\n"
     ]
    }
   ],
   "source": [
    "ranked_words = sorted(((scores[i],s) for i,s in enumerate(flatten_words)), reverse=True)\n",
    "\n",
    "# Specify number of sentences to form the summary\n",
    "sn = 10\n",
    "\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    print(ranked_words[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取关键句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "\n",
    "for i in words:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i])/(len(i)+0.001)\n",
    "    else:\n",
    "        v = np.zeros((300,))\n",
    "\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "# Specify number of sentences to form the summary\n",
    "sn = 8\n",
    "\n",
    "# Generate summary\n",
    "for i in range(sn):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TestRank_Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
