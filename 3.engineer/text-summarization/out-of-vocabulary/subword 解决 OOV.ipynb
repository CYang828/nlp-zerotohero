{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword 解决 OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watermark\n",
      "  Downloading watermark-2.0.2-py2.py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from watermark) (7.16.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (57.1.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.18.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (3.0.19)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (2.9.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython->watermark) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect->ipython->watermark) (0.7.0)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "BSlience 2021-12-10 12:43:04 \n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.16.1\n",
      "\n",
      "numpy 1.19.5\n",
      "sentencepiece not installed\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from typing import Dict, Tuple, List, Set\n",
    "\n",
    "%watermark -a 'BSlience' -d -t -v -p numpy,sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding(BPE)\n",
    "\n",
    "**Implementation From Scratch**\n",
    "\n",
    "Start by initializing the vocabulary with character vocabulary plus a special end of word symbol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w </w>': 5,\n",
       " 'l o w e r </w>': 2,\n",
       " 'n e w e s t </w>': 6,\n",
       " 'w i d e s t </w>': 3,\n",
       " 'h a p p i e r </w>': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming we've extracted from our raw text and this is the character\n",
    "# vocabulary that we've ended up with, along with their frequency\n",
    "vocab = {\n",
    "    'l o w </w>': 5,\n",
    "    'l o w e r </w>': 2,\n",
    "    'n e w e s t </w>': 6,\n",
    "    'w i d e s t </w>': 3,\n",
    "    'h a p p i e r </w>': 2\n",
    "}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then count the frequency of each consecutive character pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"Get counts of pairs of consecutive symbols.\"\"\"\n",
    "\n",
    "    pairs = {}\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        # count occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            current_frequency = pairs.get(pair, 0)\n",
    "            pairs[pair] = current_frequency + frequency\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o'): 7,\n",
       " ('o', 'w'): 7,\n",
       " ('w', '</w>'): 5,\n",
       " ('w', 'e'): 8,\n",
       " ('e', 'r'): 4,\n",
       " ('r', '</w>'): 4,\n",
       " ('n', 'e'): 6,\n",
       " ('e', 'w'): 6,\n",
       " ('e', 's'): 9,\n",
       " ('s', 't'): 9,\n",
       " ('t', '</w>'): 9,\n",
       " ('w', 'i'): 3,\n",
       " ('i', 'd'): 3,\n",
       " ('d', 'e'): 3,\n",
       " ('h', 'a'): 2,\n",
       " ('a', 'p'): 2,\n",
       " ('p', 'p'): 2,\n",
       " ('p', 'i'): 2,\n",
       " ('i', 'e'): 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_stats = get_pair_stats(vocab)\n",
    "pair_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the most frequent, and merge them together into a new token whenever we encounter them in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(best_pair: Tuple[str, str], vocab_in: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "\n",
    "    vocab_out = {}\n",
    "\n",
    "    # re.escape\n",
    "    # ensures the characters of our input pair will be handled as is and\n",
    "    # not get mistreated as special characters in the regular expression.\n",
    "    pattern = re.escape(' '.join(best_pair))\n",
    "    replacement = ''.join(best_pair)\n",
    "\n",
    "    for word_in in vocab_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        word_out = re.sub(pattern, replacement, word_in)\n",
    "        vocab_out[word_out] = vocab_in[word_in]\n",
    "\n",
    "    return vocab_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular round, the `e,s` consecutive pair was identified as the most frequent pair, then in the new vocabulary, all the e,s was merged together into a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n"
     ]
    }
   ],
   "source": [
    "best_pair = max(pair_stats, key=pair_stats.get)\n",
    "print(best_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w </w>': 5,\n",
       " 'l o w e r </w>': 2,\n",
       " 'n e w es t </w>': 6,\n",
       " 'w i d es t </w>': 3,\n",
       " 'h a p p i e r </w>': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab = merge_vocab(best_pair, vocab)\n",
    "new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o'): 7,\n",
       " ('o', 'w'): 7,\n",
       " ('w', '</w>'): 5,\n",
       " ('w', 'e'): 2,\n",
       " ('e', 'r'): 4,\n",
       " ('r', '</w>'): 4,\n",
       " ('n', 'e'): 6,\n",
       " ('e', 'w'): 6,\n",
       " ('w', 'es'): 6,\n",
       " ('es', 't'): 9,\n",
       " ('t', '</w>'): 9,\n",
       " ('w', 'i'): 3,\n",
       " ('i', 'd'): 3,\n",
       " ('d', 'es'): 3,\n",
       " ('h', 'a'): 2,\n",
       " ('a', 'p'): 2,\n",
       " ('p', 'p'): 2,\n",
       " ('p', 'i'): 2,\n",
       " ('i', 'e'): 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'l o w </w>': 5,\n",
    " 'l o w e r </w>': 2,\n",
    " 'n e w es t </w>': 6,\n",
    " 'w i d es t </w>': 3,\n",
    " 'h a p p i e r </w>': 2}\n",
    "\n",
    "pair_stats = get_pair_stats(vocab)\n",
    "pair_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was only 1 iteration of the merging process, we can iteratively perform this merging step until we reach the number of merges or the number of tokens we would like to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 0\n",
      "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('e', 's')\n",
      "\n",
      "iteration 1\n",
      "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('es', 't')\n",
      "\n",
      "iteration 2\n",
      "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('est', '</w>')\n",
      "\n",
      "iteration 3\n",
      "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('l', 'o')\n",
      "\n",
      "iteration 4\n",
      "vocabulary:  {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('lo', 'w')\n",
      "\n",
      "iteration 5\n",
      "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('n', 'e')\n",
      "\n",
      "iteration 6\n",
      "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('ne', 'w')\n",
      "\n",
      "iteration 7\n",
      "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('new', 'est</w>')\n",
      "\n",
      "iteration 8\n",
      "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('low', '</w>')\n",
      "\n",
      "iteration 9\n",
      "vocabulary:  {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
      "best pair: ('e', 'r')\n",
      "\n",
      "final vocabulary:  {'low</w>': 5, 'low er </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i er </w>': 2}\n",
      "\n",
      "byte pair encoding:  {('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('e', 'r'): 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    'l o w </w>': 5,\n",
    "    'l o w e r </w>': 2,\n",
    "    'n e w e s t </w>': 6,\n",
    "    'w i d e s t </w>': 3,\n",
    "    'h a p p i e r </w>': 2\n",
    "}\n",
    "\n",
    "# we store the best pair during each iteration for encoding new vocabulary, more on this later\n",
    "bpe_codes = {}\n",
    "num_merges = 10  # hyperparameter\n",
    "for i in range(num_merges):\n",
    "    print('\\niteration', i)\n",
    "    pair_stats = get_pair_stats(vocab)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    bpe_codes[best_pair] = i\n",
    "\n",
    "    print('vocabulary: ', vocab)\n",
    "    print('best pair:', best_pair)\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "\n",
    "print('\\nfinal vocabulary: ', vocab)\n",
    "print('\\nbyte pair encoding: ', bpe_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Encodings\n",
    "\n",
    "Now that we've see the process of \"learning\" the byte pair encodings. We now turn our attention to the process of \"applying\" them to new vocabulary.\n",
    "\n",
    "While possible:\n",
    "\n",
    "- Get all the bigram symbol for the word that we wish to encode.\n",
    "- Find the symbol pair in our byte pair codes that appeared first among the symbols that were merged.\n",
    "- Apply the merge on the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'o', 'w', 'e', 's', 't', '</w>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first convert an input word to the list of character format\n",
    "original_word = 'lowest'\n",
    "word = list(original_word)\n",
    "word.append('</w>')\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word: List[str]) -> Set[Tuple[str, str]]:\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'), ('l', 'o'), ('o', 'w'), ('s', 't'), ('t', '</w>'), ('w', 'e')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the set of possible bigram symbol\n",
    "pairs = get_pairs(word)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', 's')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt to find it in the byte pair codes\n",
    "bpe_codes_pairs = [(pair, bpe_codes[pair]) for pair in pairs if pair in bpe_codes]\n",
    "pair_to_merge = min(bpe_codes_pairs, key=itemgetter(1))[0]\n",
    "pair_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_word(word: List[str], pair_to_merge: Tuple[str, str]) -> List[str]:\n",
    "    first, second = pair_to_merge\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        try:\n",
    "            j = word.index(first, i)\n",
    "            new_word.extend(word[i:j])\n",
    "            i = j\n",
    "        except ValueError:\n",
    "            new_word.extend(word[i:])\n",
    "            break\n",
    "\n",
    "        if i < len(word) - 1 and word[i + 1] == second:\n",
    "            new_word.append(first + second)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word.append(first)\n",
    "            i += 1\n",
    "\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'o', 'w', 'es', 't', '</w>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stitch together the new word\n",
    "new_word = create_new_word(word, pair_to_merge)\n",
    "new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous couple of code chunks shows one iteration of applying the byte pair codes to encode a new word. The next one puts everything together into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(original_word: str, bpe_codes: Dict[Tuple[str, str], int]) -> List[str]:\n",
    "    if len(original_word) == 1:\n",
    "        return original_word\n",
    "\n",
    "    word = list(original_word)\n",
    "    word.append('</w>')\n",
    "\n",
    "    while True:\n",
    "        pairs = get_pairs(word)\n",
    "        bpe_codes_pairs = [(pair, bpe_codes[pair]) for pair in pairs if pair in bpe_codes]\n",
    "        if not bpe_codes_pairs:\n",
    "            break\n",
    "\n",
    "        pair_to_merge = min(bpe_codes_pairs, key=itemgetter(1))[0]\n",
    "        word = create_new_word(word, pair_to_merge)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low', 'est</w>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_word = 'lowest'\n",
    "encode(original_word, bpe_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'): 0,\n",
       " ('es', 't'): 1,\n",
       " ('est', '</w>'): 2,\n",
       " ('l', 'o'): 3,\n",
       " ('lo', 'w'): 4,\n",
       " ('n', 'e'): 5,\n",
       " ('ne', 'w'): 6,\n",
       " ('new', 'est</w>'): 7,\n",
       " ('low', '</w>'): 8,\n",
       " ('e', 'r'): 9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the result, we can see that even though the word lowest did not appear in our \"training\" data, but because \"low\" and and ending \"est\" are both byte pair codes that were learned, the word got encoded into low est."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencepiece\n",
    "\n",
    "Upon seeing an educational implementation, we will give a more efficient implementation, [sentencepiece](https://github.com/google/sentencepiece#what-is-sentencepiece), a swing. Consider reading the overview section of the software for a high-level introduction of the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-16 20:49:55--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt.1’\n",
      "\n",
      "botchan.txt.1       100%[===================>] 272.25K   679KB/s    in 0.4s    \n",
      "\n",
      "2021-05-16 20:49:56 (679 KB/s) - ‘botchan.txt.1’ saved [278779/278779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download some sample text for demonstration\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the subword unit, we call the SentencePieceTrainer.train method by passing in our parameters. I've specified some of the commonly used parameters in the example below. As for what are the available options, I find it helpful to just search in the source code of the parameter parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 23 kB/s            \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=data/botchan.txt --model_type=wordpiece --model_prefix=wordpiece --vocab_size=1000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "Invalid argument: unknown enumeration value of \"wordpiece\" as kModelType_Map (<string>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Invalid argument: unknown enumeration value of \"wordpiece\" as kModelType_Map\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
    "\n",
    "input_file = 'data/botchan.txt'\n",
    "max_num_words = 1000\n",
    "model_type = 'wordpiece'\n",
    "model_prefix = 'wordpiece'\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "bos_id = 2\n",
    "eos_id = 3\n",
    "\n",
    "sentencepiece_params = ' '.join([\n",
    "    '--input={}'.format(input_file),\n",
    "    '--model_type={}'.format(model_type),\n",
    "    '--model_prefix={}'.format(model_prefix),\n",
    "    '--vocab_size={}'.format(max_num_words),\n",
    "    '--pad_id={}'.format(pad_id),\n",
    "    '--unk_id={}'.format(unk_id),\n",
    "    '--bos_id={}'.format(bos_id),\n",
    "    '--eos_id={}'.format(eos_id)\n",
    "])\n",
    "print(sentencepiece_params)\n",
    "SentencePieceTrainer.train(sentencepiece_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments regarding the parameters:\n",
    "\n",
    "input expects a .txt file on disk. Hence, if we have a python variable, e.g. a list of sentences that we wish to learn the subword unit using sentencepiece, we would have to make an additional step to write it to a file on disk.\n",
    "\n",
    "model_type can take in bpe, unigram, char, word, allowing us to experiment with different tokenization schemes. The unigram method was not introduced in this notebook.\n",
    "pad_id, specifying these ids can be important with we're using sentencepiece in conjunction with other libraries, e.g. 1 library may have already by default preserved the token id 0 for padding characters, in that case, we can explicitly specifying that padding id to sentencepiece to be consistent.\n",
    "\n",
    "Upon training the model, we can load it, the model resides in model_prefix.model, where model_prefix is a parameter that we also get to specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sp = SentencePieceProcessor()\n",
    "sp.load(\"{}.model\".format(model_prefix))\n",
    "print('Found %s unique tokens.' % sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showcasing some common operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "# given a new text, we can convert it to subword units\n",
    "original = 'This is a test'\n",
    "encoded_pieces = sp.encode_as_pieces(original)\n",
    "print(encoded_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475, 98, 6, 4, 264]\n"
     ]
    }
   ],
   "source": [
    "# or convert it to numeric id for downstream modeling\n",
    "encoded_ids = sp.encode_as_ids(original)\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decode: piece/id => text\n",
    "# we can convert the subword units back to the original text\n",
    "decoded_pieces = sp.decode_pieces(encoded_pieces)\n",
    "print(decoded_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# we can convert the numeric id back to the original text\n",
    "decoded_ids = sp.decode_ids(encoded_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "▁This\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "original = '▁This'\n",
    "\n",
    "# finding the numeric id of the particular subword unit\n",
    "piece_id = sp.piece_to_id('▁This')\n",
    "print(piece_id)\n",
    "\n",
    "# obtaining the subword unit of a particular numeric id\n",
    "print(sp.id_to_piece(piece_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our introduction to one of the subword tokenization methods, Byte Pair Encoding and one of the open-source packages that's available out their sentencepiece.\n",
    "\n",
    "Some other options out there at the time of writing this documentation includes, [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe) and [fastBPE](https://github.com/glample/fastBPE). YouTokenToMe claims to be faster than both sentencepiece and fastBPE, and sentencepiece supports additional subword tokenization method.\n",
    "\n",
    "Subword tokenization is a commonly used technique in modern NLP pipeline, and it's definitely worth understanding and adding to our toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhJ8dIMpTj-N",
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenization doesn't have to be slow !\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner\n",
    "should find a way to map raw input strings to a representation understandable by a trainable model.\n",
    "\n",
    "One very simple approach would be to split inputs over every space and assign an identifier to each word. This approach\n",
    "would look similar to the code below in python\n",
    "\n",
    "```python\n",
    "s = \"very long corpus...\"\n",
    "words = s.split(\" \")  # Split over space\n",
    "vocabulary = dict(enumerate(set(words)))  # Map storing the word to it's corresponding id\n",
    "```\n",
    "\n",
    "This approach might work well if your vocabulary remains small as it would store every word (or **token**) present in your original\n",
    "input. Moreover, word variations like \"cat\" and \"cats\" would not share the same identifiers even if their meaning is \n",
    "quite close.\n",
    "\n",
    "![tokenization_simple](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n",
    "\n",
    "### Subtoken Tokenization\n",
    "\n",
    "To overcome the issues described above, recent works have been done on tokenization, leveraging \"subtoken\" tokenization.\n",
    "**Subtokens** extends the previous splitting strategy to furthermore explode a word into grammatically logicial sub-components learned\n",
    "from the data.\n",
    "\n",
    "Taking our previous example of the words __cat__ and __cats__, a sub-tokenization of the word __cats__ would be [cat, ##s]. Where the prefix _\"##\"_ indicates a subtoken of the initial input. \n",
    "Such training algorithms might extract sub-tokens such as _\"##ing\"_, _\"##ed\"_ over English corpus.\n",
    "\n",
    "As you might think of, this kind of sub-tokens construction leveraging compositions of _\"pieces\"_ overall reduces the size\n",
    "of the vocabulary you have to carry to train a Machine Learning model. On the other side, as one token might be exploded\n",
    "into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence's length. \n",
    " \n",
    "![subtokenization](https://nlp.fast.ai/images/multifit_vocabularies.png)\n",
    " \n",
    "Among all the tokenization algorithms, we can highlight a few subtokens algorithms used in Transformers-based SoTA models : \n",
    "\n",
    "- [Byte Pair Encoding (BPE) - Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)\n",
    "- [Word Piece - Japanese and Korean voice search (Schuster, M., and Nakajima, K., 2015)](https://research.google/pubs/pub37842/)\n",
    "- [Unigram Language Model - Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, T., 2018)](https://arxiv.org/abs/1804.10959)\n",
    "- [Sentence Piece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo and John Richardson, 2018)](https://arxiv.org/abs/1808.06226)\n",
    "\n",
    "Going through all of them is out of the scope of this notebook, so we will just highlight how you can use them.\n",
    "\n",
    "### @huggingface/tokenizers library \n",
    "Along with the transformers library, we @huggingface provide a blazing fast tokenization library\n",
    "able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine.\n",
    "\n",
    "The library is written in Rust allowing us to take full advantage of multi-core parallel computations in a native and memory-aware way, on-top of which \n",
    "we provide bindings for Python and NodeJS (more bindings may be added in the future). \n",
    "\n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide\n",
    "these various components: \n",
    "\n",
    "- **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to\n",
    "lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. \n",
    "- **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to\n",
    "pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant\n",
    " of your input data.\n",
    "- **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA\n",
    "models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "- **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according\n",
    "to the `PreTokenizer` we used previously.\n",
    "- **Trainer**: Provides training capabilities to each model.\n",
    "\n",
    "For each of the components above we provide multiple implementations:\n",
    "\n",
    "- **Normalizer**: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...\n",
    "- **PreTokenizer**: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...\n",
    "- **Model**: WordLevel, BPE, WordPiece\n",
    "- **Post-Processor**: BertProcessor, ...\n",
    "- **Decoder**: WordLevel, BPE, WordPiece, ...\n",
    "\n",
    "All of these building blocks can be combined to create working tokenization pipelines. \n",
    "In the next section we will go over our first pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SHp0qeSTj-V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alright, now we are ready to implement our first tokenization pipeline through `tokenizers`. \n",
    "\n",
    "For this, we will train a Byte-Pair Encoding (BPE) tokenizer on a quite small input for the purpose of this notebook.\n",
    "We will work with [the file from Peter Norving](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjYp9Ppru_nAhUBzIUKHfbUAG8QFjAAegQIBhAB&url=https%3A%2F%2Fnorvig.com%2Fbig.txt&usg=AOvVaw2ed9iwhcP1RKUiEROs15Dz).\n",
    "This file contains around 130.000 lines of raw text that will be processed by the library to generate a working tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3A-6tag7Tj-W",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/zhangchunyang/opt/anaconda3/lib/python3.8/site-packages (0.10.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UucvxrKPTj-W",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "BIG_FILE_URL = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n",
    "\n",
    "# Let's download the file and save it somewhere\n",
    "from requests import get\n",
    "with open('big.txt', 'wb') as big_f:\n",
    "    response = get(BIG_FILE_URL, )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        big_f.write(response.content)\n",
    "    else:\n",
    "        print(\"Unable to get the file: {}\".format(response.reason))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2adXYywTj-X",
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    " \n",
    "Now that we have our training data we need to create the overall pipeline for the tokenizer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HQQLvztBTj-X",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
    "# the overall pipeline for various well-known tokenization algorithm. \n",
    "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFKC(),\n",
    "    Lowercase()\n",
    "])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxY29vBXTj-X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The overall pipeline is now ready to be trained on the corpus we downloaded earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "bJZhNREKTj-Y",
    "outputId": "9b719c02-46c0-4965-a022-72fbd8571625",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
    "tokenizer.train(files=[\"big.txt\"], trainer=trainer)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IZQ5vUZTj-a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Et voilà ! You trained your very first tokenizer from scratch using `tokenizers`. Of course, this \n",
    "covers only the basics, and you may want to have a look at the `add_special_tokens` or `special_tokens` parameters\n",
    "on the `Trainer` class, but the overall process should be very similar.\n",
    "\n",
    "We can save the content of the model to reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "t8QfZqEtTj-a",
    "outputId": "860e2d3a-bf8e-4309-89ec-598c31829348",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see the generated files in the output.\n",
    "tokenizer.model.save('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDxG95ZrTj-a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let load the trained model and start using out newly trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_cCXHtuOTj-b",
    "outputId": "71ffae35-ef39-49f0-f70a-803e206776a7",
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['Ġthis', 'Ġis', 'Ġa', 'Ġsimple', 'Ġin', 'put', 'Ġto', 'Ġbe', 'Ġtoken', 'ized']\n",
      "Decoded string:  this is a simple input to be tokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-7d4de219ae4f>:2: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
      "  tokenizer.model = BPE('vocab.json', 'merges.txt')\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "tokenizer.model = BPE('vocab.json', 'merges.txt')\n",
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd3iLyokTj-b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "- normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "- original_str: The input string as it was provided\n",
    "- tokens: The generated tokens with their string representation\n",
    "- input_ids: The generated tokens with their integer representation\n",
    "- attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "- special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "- type_ids: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "- overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece(2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "\n",
    "        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "files = [\"data/big.txt\"]\n",
    "bert_tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['[CLS]', 'this', 'is', 'a', 'simple', 'in', '##p', '##ut', 'to', 'be', 'token', '##ized', '[SEP]']\n",
      "Decoded string: this is a simple in ##p ##ut to be token ##ized\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "encoding = bert_tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = bert_tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=botchan.txt --model_type=unigram --model_prefix=sentencepiece --vocab_size=1000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
    "\n",
    "input_file = 'data/botchan.txt'\n",
    "max_num_words = 1000\n",
    "model_type = 'unigram'\n",
    "model_prefix = 'sentencepiece'\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "bos_id = 2\n",
    "eos_id = 3\n",
    "\n",
    "sentencepiece_params = ' '.join([\n",
    "    '--input={}'.format(input_file),\n",
    "    '--model_type={}'.format(model_type),\n",
    "    '--model_prefix={}'.format(model_prefix),\n",
    "    '--vocab_size={}'.format(max_num_words),\n",
    "    '--pad_id={}'.format(pad_id),\n",
    "    '--unk_id={}'.format(unk_id),\n",
    "    '--bos_id={}'.format(bos_id),\n",
    "    '--eos_id={}'.format(eos_id)\n",
    "])\n",
    "print(sentencepiece_params)\n",
    "SentencePieceTrainer.train(sentencepiece_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sp = SentencePieceProcessor()\n",
    "sp.load(\"{}.model\".format(model_prefix))\n",
    "print('Found %s unique tokens.' % sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
