{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业描述\n",
    "根据课上所学内容，在weibo_senti_100k情感分析数据集上使用Gensim包的CBOW算法训练词向量，并且使用Annoy建立词向量索引，加速搜索速度。\n",
    "\n",
    "\n",
    "+ 认真阅读注释，对你做对该题至关重要\n",
    "\n",
    "+ 作业已经给出大部分程序实现代码，你只需要在`######## your code ~n line ########` 与 `######## your code end ########` 行之间根据提示补充完毕相应的代码即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 下载地址：[百度网盘](https://pan.baidu.com/s/1DoQbki3YwqkuwQUOj64R_g#list/path=%2F)\n",
    "+ 数据概览： 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条\n",
    "+ 推荐实验： 情感/观点/评论 倾向性分析\n",
    "+ 数据来源： [github](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/weibo_senti_100k/intro.ipynb)\n",
    "+ 原数据集： 新浪微博，情感分析标记语料共12万条，网上搜集，具体作者、来源不详"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据集下载并解压\n",
    "把下载后的文件放在与当前jupyter同级目录下的data文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.读取文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了使下面的代码演示更加舒适，这里开启全局禁用警告提示\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以看到数据集有两列，第一列是标签列（对于训练词向量任务，没有用），1代表积极的情绪，0代表消极的情绪；第二列是review（评论）列，我们只使用标签列训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1              ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]\n",
       "1      1  @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...\n",
       "2      1  姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...\n",
       "3      1                                         美~~~~~[爱你]\n",
       "4      1                                  梦想有多大，舞台就有多大![鼓掌]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = './data/'\n",
    "pd_all = pd.read_csv(path + 'weibo_senti_100k.csv')\n",
    "pd_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数目（总体）：119988\n",
      "评论数目（正向）：59993\n",
      "评论数目（负向）：59995\n"
     ]
    }
   ],
   "source": [
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题一：从pd_all中提取review列的数据，并且把每一行的数据存储到data中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一共119988条数据.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## your code ~n line ######## 提示：这里data是一个列表，列表中每一个元素就是一条评论，方法有多种，可自行发挥\n",
    "\n",
    "\n",
    "data = \n",
    "######## your code end ########\n",
    "'一共%s条数据.' % (len(data), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.中文分词\n",
    "这里我们使用jieba进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "# 实现分词函数\n",
    "def tokenizer(text):\n",
    "    #精确模式\n",
    "    tokens = jieba.lcut(text, cut_all=False)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题二：把数据处理成训练词向量需要的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 119988/119988 [00:31<00:00, 3811.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "sentence_tokens = []\n",
    "for paragraph in tqdm(data):\n",
    "    # 提示：对data中的每一个条评论进行分词，每个分词后的句子，词与词之间用空格隔开，并追加到sentence_tokens中\n",
    "    ######## your code ~n line ########\n",
    "    tokens = \n",
    "    ######## your code end line ########\n",
    "    sentence_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一共119988个句子.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence_tokens[10004]\n",
    "'一共%s个句子.' % (len(sentence_tokens), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于数据量比较大这里将其保存在文件中，每行一个句子，句子中词与词用空格分开\n",
    "with open('sentence_tokens_zh.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim被称为“为人类进行主题建模”的自然语言处理包。但实际上远不止这些。它是一个领先和最先进的软件包，可以用于处理文本、文本向量（如Word2Vec、FastText等）和构建主题模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: gensim==4.2.0 in /Users/lmq/anaconda3/envs/nlp_core/lib/python3.7/site-packages (4.2.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/lmq/anaconda3/envs/nlp_core/lib/python3.7/site-packages (from gensim==4.2.0) (1.7.3)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/lmq/anaconda3/envs/nlp_core/lib/python3.7/site-packages (from gensim==4.2.0) (6.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/lmq/anaconda3/envs/nlp_core/lib/python3.7/site-packages (from gensim==4.2.0) (1.21.6)\r\n"
     ]
    }
   ],
   "source": [
    "# 安装gensim\n",
    "!pip install gensim==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 中有较多的参数需要指定，这里逐一进行说明，想知道更加细致的讲解，还是建议去看官方文档\n",
    "param = {\n",
    "    'sentences':None, # 是一个可以迭代的对象，对于大语料集，建议使用BrownCorpus,Text8Corpus或LineSentence构建。\n",
    "    'corpus_file':None, # 语料的路径，一般与LineSentence配合使用来提升训练的性能\n",
    "    'vector_size':100, # 词向量大小，默认100\n",
    "    'alpha':0.025, # 初始学习速率\n",
    "    'window':7, # 窗口大小，默认5\n",
    "    'min_count':5, # 词频低于min_count以下的词舍去\n",
    "    'max_vocab_size':None, # 最大的词典大小\n",
    "    'sample':1e-3, # 配置哪些高频字随机降采样的阈值，取值范围(0, 1e-5)\n",
    "    'seed':1, # 随机种子，可保证每次训练的结果一致\n",
    "    'workers':6, # 训练使用的线程数量\n",
    "    'min_alpha':0.0001, # 随着训练的进行，学习率将从“alpha”线性下降到“min_alpha”\n",
    "    'sg':0, # 选择1使用skip-gram模型，0使用cbow模型\n",
    "    'hs':0, # 选择1使用hierarchical softmax进行优化，选择0而且“negative”参数是非0，则使用负采样\n",
    "    'negative':5, # 负采样词的个数\n",
    "    'ns_exponent':0.75, # 用于形成负采样分布的指数\n",
    "    'cbow_mean':1, # 如果为0，则使用上下文词向量之和。如果1，则使用平均值，仅在使用cbow时适用\n",
    "    'hashfxn':hash, # 用于随机初始化权重的哈希函数，以提高训练再现性\n",
    "    'epochs':5, # 训练的轮次\n",
    "    'sorted_vocab':1, # 如果为1，则在指定单词索引之前，按频率降序对词汇进行排序。\n",
    "    'batch_words':10000, # 每一批的传递给线程的单词的数量，默认为10000\n",
    "    'compute_loss':False, # 如果是True计算并存储损失值；可以使用gensim.models.word2vec.Word2Vec.get_latest_training_loss方法获取\n",
    "    'max_final_vocab':None, # 限制目标词表的大小\n",
    "    'shrink_windows':True,  # 有效窗口大小从[1，window]均匀采样\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从gensim中导出Word2Vec包\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题三：使用LineSentence实现大规模文本的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.训练模型\n",
    "filename = 'sentence_tokens_zh.txt'\n",
    "# 提示：初始化LineSentence类，赋值给text。[LineSentence类官方教程](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence)\n",
    "####### your code ~1 line ######## \n",
    "text = \n",
    "####### your code end ######## \n",
    "param['sentences'] = text\n",
    "wv_model = Word2Vec(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.保存模型\n",
    "# binary设置为True，训练的词向量将会以二进制的方式存储\n",
    "wv_model.wv.save_word2vec_format('./wv_model/word2vec_zh.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题四：加载训练的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.加载模型（词向量）\n",
    "from gensim.models import KeyedVectors\n",
    "# 提示：gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "####### your code ~1 line ######## \n",
    "wv_model = \n",
    "####### your code end ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三个词的词向量矩阵的维度是： (3, 100) 。\n",
      "-------------------------------我是分隔符------------------------\n",
      "喜欢和喜爱的余弦相似度是： 0.56644905 。\n",
      "-------------------------------我是分隔符------------------------\n",
      "和有趣比较相关的词有 强大 ，余弦距离是： 0.7989455461502075\n",
      "和有趣比较相关的词有 复杂 ，余弦距离是： 0.7865893244743347\n",
      "和有趣比较相关的词有 好玩 ，余弦距离是： 0.7752323746681213\n",
      "和有趣比较相关的词有 有用 ，余弦距离是： 0.7646296620368958\n",
      "和有趣比较相关的词有 有意思 ，余弦距离是： 0.7602453231811523\n",
      "和有趣比较相关的词有 深刻 ，余弦距离是： 0.7590806484222412\n",
      "和有趣比较相关的词有 简单 ，余弦距离是： 0.7489534020423889\n",
      "和有趣比较相关的词有 熟悉 ，余弦距离是： 0.7469128370285034\n",
      "和有趣比较相关的词有 单纯 ，余弦距离是： 0.74482262134552\n",
      "和有趣比较相关的词有 迷人 ，余弦距离是： 0.7436302304267883\n"
     ]
    }
   ],
   "source": [
    "# 取出词语对应的词向量。\n",
    "vec = wv_model[['糟糕','有趣','喜欢']]\n",
    "print('三个词的词向量矩阵的维度是：', vec.shape,'。')\n",
    "print('-------------------------------我是分隔符------------------------')\n",
    "# 计算两个词的相似程度。\n",
    "print('喜欢和喜爱的余弦相似度是：', wv_model.similarity('喜欢', '喜爱'),'。')\n",
    "print('-------------------------------我是分隔符------------------------')\n",
    "# 得到和某个词比较相关的词的列表\n",
    "sim1 = wv_model.most_similar('有趣',topn=10)\n",
    "for key in sim1:\n",
    "    print('和有趣比较相关的词有',key[0],'，余弦距离是：',key[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和完美比较相关的词有 美妙 ，余弦距离是： 0.7493830323219299\n",
      "和完美比较相关的词有 色彩 ，余弦距离是： 0.7492667436599731\n",
      "和完美比较相关的词有 独特 ，余弦距离是： 0.7221761345863342\n",
      "和完美比较相关的词有 丰富 ，余弦距离是： 0.7146338820457458\n",
      "和完美比较相关的词有 拥有 ，余弦距离是： 0.712502121925354\n",
      "和完美比较相关的词有 惬意 ，余弦距离是： 0.7077858448028564\n",
      "和完美比较相关的词有 具有 ，余弦距离是： 0.7062147259712219\n",
      "和完美比较相关的词有 曼妙 ，余弦距离是： 0.7031504511833191\n",
      "和完美比较相关的词有 结合 ，余弦距离是： 0.702284038066864\n",
      "和完美比较相关的词有 呈现 ，余弦距离是： 0.6994835138320923\n",
      "CPU times: user 11.6 ms, sys: 1.56 ms, total: 13.2 ms\n",
      "Wall time: 2.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#%%time 显示整个cell的运行时间\n",
    "key_word = '完美'\n",
    "sim1 = wv_model.most_similar(key_word,topn=10)\n",
    "for key in sim1:\n",
    "    print('和'+ key_word +'比较相关的词有',key[0],'，余弦距离是：',key[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Annoy加速查询\n",
    "\n",
    "Annoy教程 [github链接](https://github.com/spotify/annoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方demo:\n",
    "```\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "f = 40  # 向量的维度\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "for i in range(1000):\n",
    "    v = [random.gauss(0, 1) for z in range(f)] # 随机生成一个向量\n",
    "    t.add_item(i, v) # 添加向量到t中\n",
    "t.build(10) #  10颗树，树越多精度越高\n",
    "t.save('test.ann') # 保存模型\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "print(u.get_nns_by_item(0, 1000)) # 查询1000近邻的数据\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: annoy==1.17.0 in /Users/lmq/anaconda3/envs/nlp_core/lib/python3.7/site-packages (1.17.0)\r\n"
     ]
    }
   ],
   "source": [
    "# 安装annoy\n",
    "!pip install annoy==1.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "vector_file_path = './wv_model/word2vec_zh.txt'\n",
    "save_file = './wv_model/word2vec_zh.annoy'\n",
    "def get_word_vec():\n",
    "    with open(vector_file_path,'r',encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 掠过第一行的表头\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # 去除两端换行符\n",
    "            line=line.strip()  \n",
    "            line=line.split(' ')\n",
    "            word = line[0]\n",
    "            vec = [float(i) for i in line[1:]]\n",
    "            assert len(vec)==param['vector_size']\n",
    "            yield word, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题五：实现annoy建立索引的过程\n",
    "可以根据官方的demo作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(emb_size=param['vector_size'],metric='angular',num_trees=100):\n",
    "    # 提示：新建AnnoyIndex对象t，并指定参数\n",
    "    ####### your code ~1 line ######## \n",
    "    t=\n",
    "    ####### your code end ######## \n",
    "    num=-1\n",
    "    for word,vec in get_word_vec():\n",
    "        num+=1\n",
    "        # 提示：遍历往t中添加向量\n",
    "        ####### your code ~1 line ######## \n",
    "        \n",
    "        ####### your code end ######## \n",
    "    t.build(num_trees)\n",
    "    t.save(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_annoy_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_model = AnnoyIndex(param['vector_size'],'angular')\n",
    "annoy_model.load(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过wv_model拿到词与id、id与词的对应关系\n",
    "word2id = wv_model.key_to_index\n",
    "id2word = wv_model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = '完美'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用`Annoy`比使用`most_similar`函数查询Top 10相似词的速度较快些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和完美比较相关的词有 完美 ，距离是： 0.00013219143147580326\n",
      "和完美比较相关的词有 美妙 ，距离是： 0.7079789638519287\n",
      "和完美比较相关的词有 色彩 ，距离是： 0.708142876625061\n",
      "和完美比较相关的词有 独特 ，距离是： 0.7454178333282471\n",
      "和完美比较相关的词有 丰富 ，距离是： 0.7554683089256287\n",
      "和完美比较相关的词有 拥有 ，距离是： 0.7582848072052002\n",
      "和完美比较相关的词有 惬意 ，距离是： 0.7644792199134827\n",
      "和完美比较相关的词有 曼妙 ，距离是： 0.7705186605453491\n",
      "和完美比较相关的词有 结合 ，距离是： 0.77164226770401\n",
      "和完美比较相关的词有 呈现 ，距离是： 0.7752633094787598\n",
      "CPU times: user 928 µs, sys: 776 µs, total: 1.7 ms\n",
      "Wall time: 873 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topn = 10\n",
    "raw_res = annoy_model.get_nns_by_item(i=word2id[key_word],n=topn,include_distances=True)\n",
    "for key, sim in zip(*raw_res):\n",
    "    print('和'+ key_word +'比较相关的词有',id2word[key],'，距离是：',sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_core",
   "language": "python",
   "name": "nlp_core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
