{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# 使用RNN生成金庸体文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用金庸先生的小说作为数据源，尝试让机器也能写出金庸风格的小说。\n",
    "\n",
    "参考：\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) \n",
    "- [eager execution](https://www.tensorflow.org/guide/eager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### 导入 TensorFlow 和其他库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### 下载金庸小说数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://aimaksen.bslience.cn/jinyong/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8.txt\n",
      "3817472/3810759 [==============================] - 3s 1us/step\n",
      "3825664/3810759 [==============================] - 3s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('tianlongbabu.txt', 'http://aimaksen.bslience.cn/jinyong/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### 读取数据\n",
    "\n",
    "首先，我们先探索一下数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本总长度: 1278131 字\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'文本总长度: {len(text)} 字')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿释名\n",
      "　　“天龙八部”这名词出于佛经。许多大乘佛经叙述佛陀向诸菩萨、比丘等说法时，常有天龙八部参与听法。如《法华经·提婆达多品》：“天龙八部、人与非人，皆遥见彼龙女成佛”。“非人”是形貌似人而实际不是人的众生。“天龙八部”都是“非人”，包括八种神道怪物，因为以“天”及“龙”为首，所以称为“天龙八部”。八部者，一天，二龙，三夜叉，四乾达婆，五阿修罗，六迦楼罗，七紧那罗，八摩呼罗迦。\n",
      "　　“天”是指天神。在佛教中，天神的地位并非至高无上，只不过比人能享受到更大、更长久的福报而已。佛教认为一切事物无\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4268 个不重复的字\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} 个不重复的字')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 处理文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 将文本转换为向量\n",
    "\n",
    "在训练之前，我们需要把文本转换为一种数字化的表述。\n",
    "\n",
    "`preprocessing.StringLookup` 层可以帮助我们把每一个字转换为相应的数字 ID。只需要我们先把文本转化为一个个的 token。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'\\xe5\\xa4\\xa9', b'\\xe9\\xbe\\x99', b'\\xe5\\x85\\xab', b'\\xe9\\x83\\xa8'], [b'\\xe9\\x87\\x91', b'\\xe5\\xba\\xb8']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['天龙八部', '金庸']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s4f1q3iqY8f"
   },
   "source": [
    "现在我们可以创建一个 `preprocessing.StringLookup` 层:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6GMlCe3qzaL9"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "它可以把 token 都转化为相应的 ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WLv5Q_2TC2pc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[907, 4257, 344, 3824], [3857, 1255]]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "由于，我们的目的是为了生成文本，所以，在我们生成的过程中也需要把 ID 转换成文本的操作，我们可以使用 `preprocessing.StringLookup(..., invert=True)` 来达成这个目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "注意：这里我们使用 `get_vocabulary()` 来代替原来使用的 `sorted(set(text))`  获取字典，这种方法会把我们再生成过程中产生的 `[UNK]` token 也包含进去。\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#get_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqTDDxS-s-H8"
   },
   "source": [
    "这层会把生成的 token ID 转换为 `tf.RaggedTensor` 格式的相应的文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "c2GCh0ySD44s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'\\xe5\\xa4\\xa9', b'\\xe9\\xbe\\x99', b'\\xe5\\x85\\xab', b'\\xe9\\x83\\xa8'], [b'\\xe9\\x87\\x91', b'\\xe5\\xba\\xb8']]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FeW5gqutT3o"
   },
   "source": [
    "\n",
    "我们可以使用 `tf.strings.reduce_join` 把这些字重新拼接成为文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zxYI-PeltqKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'\\xe5\\xa4\\xa9\\xe9\\xbe\\x99\\xe5\\x85\\xab\\xe9\\x83\\xa8',\n",
       "       b'\\xe9\\x87\\x91\\xe5\\xba\\xb8'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "w5apvBDn9Ind"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### 生成任务（预测任务）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "给定一个字，或者是一串字，哪个字可能是接下来会出现的额可能性最大的字？\n",
    "\n",
    "这个就是我们要做的模型。我们使用 RNN 模型来读取前面出现的字，并且把他们转换成一个内部的状态，根据这个内部的状态，我们去生成（预测）下一个字是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### 创建训练样本和目标(target)\n",
    "\n",
    "接下来，我们要把文本都切分成样本序列。 每个输入的样本序列，都会包含 `seq_length` 个字(token)。\n",
    "\n",
    "对于每一个输入的序列，对应的目标(target)包含了和输入序列相同的长度，只是会向右侧移动一位。\n",
    "\n",
    "我们会把所有的文本都切割成 `seq_length+1` 的长度.比如说， `seq_length` 是 4，并且我们的文本是 \"我是中国人\". 输入的文本会是 \"我是中国\",  target 序列是 \"是中国人\".\n",
    "\n",
    "为了能够把数据转换成上述的格式，我们使用 `tf.data.Dataset.from_tensor_slices` 方法把文本向量转换成一些列的 token 序列切片(slice)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UopbsKi88tm5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1278131,), dtype=int64, numpy=array([4261, 3852,  596, ...,    1,    1,    1])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qmxrYDCTy-eL"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cjH5v45-yqqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "释\n",
      "名\n",
      "\n",
      "\n",
      "　\n",
      "　\n",
      "“\n",
      "天\n",
      "龙\n",
      "八\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C-G2oaTxy6km"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "`batch` 方法可以让我们把这些文本转换我们期望长度的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BpdjRO2CzOfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xef\\xbb\\xbf' b'\\xe9\\x87\\x8a' b'\\xe5\\x90\\x8d' b'\\n' b'\\xe3\\x80\\x80'\n",
      " b'\\xe3\\x80\\x80' b'\\xe2\\x80\\x9c' b'\\xe5\\xa4\\xa9' b'\\xe9\\xbe\\x99'\n",
      " b'\\xe5\\x85\\xab' b'\\xe9\\x83\\xa8' b'\\xe2\\x80\\x9d' b'\\xe8\\xbf\\x99'\n",
      " b'\\xe5\\x90\\x8d' b'\\xe8\\xaf\\x8d' b'\\xe5\\x87\\xba' b'\\xe4\\xba\\x8e'\n",
      " b'\\xe4\\xbd\\x9b' b'\\xe7\\xbb\\x8f' b'\\xe3\\x80\\x82' b'\\xe8\\xae\\xb8'\n",
      " b'\\xe5\\xa4\\x9a' b'\\xe5\\xa4\\xa7' b'\\xe4\\xb9\\x98' b'\\xe4\\xbd\\x9b'\n",
      " b'\\xe7\\xbb\\x8f' b'\\xe5\\x8f\\x99' b'\\xe8\\xbf\\xb0' b'\\xe4\\xbd\\x9b'\n",
      " b'\\xe9\\x99\\x80' b'\\xe5\\x90\\x91' b'\\xe8\\xaf\\xb8' b'\\xe8\\x8f\\xa9'\n",
      " b'\\xe8\\x90\\xa8' b'\\xe3\\x80\\x81' b'\\xe6\\xaf\\x94' b'\\xe4\\xb8\\x98'\n",
      " b'\\xe7\\xad\\x89' b'\\xe8\\xaf\\xb4' b'\\xe6\\xb3\\x95' b'\\xe6\\x97\\xb6'\n",
      " b'\\xef\\xbc\\x8c' b'\\xe5\\xb8\\xb8' b'\\xe6\\x9c\\x89' b'\\xe5\\xa4\\xa9'\n",
      " b'\\xe9\\xbe\\x99' b'\\xe5\\x85\\xab' b'\\xe9\\x83\\xa8' b'\\xe5\\x8f\\x82'\n",
      " b'\\xe4\\xb8\\x8e' b'\\xe5\\x90\\xac' b'\\xe6\\xb3\\x95' b'\\xe3\\x80\\x82'\n",
      " b'\\xe5\\xa6\\x82' b'\\xe3\\x80\\x8a' b'\\xe6\\xb3\\x95' b'\\xe5\\x8d\\x8e'\n",
      " b'\\xe7\\xbb\\x8f' b'\\xc2\\xb7' b'\\xe6\\x8f\\x90' b'\\xe5\\xa9\\x86'\n",
      " b'\\xe8\\xbe\\xbe' b'\\xe5\\xa4\\x9a' b'\\xe5\\x93\\x81' b'\\xe3\\x80\\x8b'\n",
      " b'\\xef\\xbc\\x9a' b'\\xe2\\x80\\x9c' b'\\xe5\\xa4\\xa9' b'\\xe9\\xbe\\x99'\n",
      " b'\\xe5\\x85\\xab' b'\\xe9\\x83\\xa8' b'\\xe3\\x80\\x81' b'\\xe4\\xba\\xba'\n",
      " b'\\xe4\\xb8\\x8e' b'\\xe9\\x9d\\x9e' b'\\xe4\\xba\\xba' b'\\xef\\xbc\\x8c'\n",
      " b'\\xe7\\x9a\\x86' b'\\xe9\\x81\\xa5' b'\\xe8\\xa7\\x81' b'\\xe5\\xbd\\xbc'\n",
      " b'\\xe9\\xbe\\x99' b'\\xe5\\xa5\\xb3' b'\\xe6\\x88\\x90' b'\\xe4\\xbd\\x9b'\n",
      " b'\\xe2\\x80\\x9d' b'\\xe3\\x80\\x82' b'\\xe2\\x80\\x9c' b'\\xe9\\x9d\\x9e'\n",
      " b'\\xe4\\xba\\xba' b'\\xe2\\x80\\x9d' b'\\xe6\\x98\\xaf' b'\\xe5\\xbd\\xa2'\n",
      " b'\\xe8\\xb2\\x8c' b'\\xe4\\xbc\\xbc' b'\\xe4\\xba\\xba' b'\\xe8\\x80\\x8c'\n",
      " b'\\xe5\\xae\\x9e' b'\\xe9\\x99\\x85' b'\\xe4\\xb8\\x8d' b'\\xe6\\x98\\xaf'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHW902-4oZt"
   },
   "source": [
    "当我们把👆🏻的转换为文本，就可以非常清晰的看到它在做什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QO32cMWu4a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿释名\n",
      "　　“天龙八部”这名词出于佛经。许多大乘佛经叙述佛陀向诸菩萨、比丘等说法时，常有天龙八部参与听法。如《法华经·提婆达多品》：“天龙八部、人与非人，皆遥见彼龙女成佛”。“非人”是形貌似人而实际不是\n",
      "-----------\n",
      "人的众生。“天龙八部”都是“非人”，包括八种神道怪物，因为以“天”及“龙”为首，所以称为“天龙八部”。八部者，一天，二龙，三夜叉，四乾达婆，五阿修罗，六迦楼罗，七紧那罗，八摩呼罗迦。\n",
      "　　“天”是指天神\n",
      "-----------\n",
      "。在佛教中，天神的地位并非至高无上，只不过比人能享受到更大、更长久的福报而已。佛教认为一切事物无常，天神的寿命终了之后，也是要死的。天神临死之前有五种征状：衣裳垢腻、头上花萎、身体臭秽、腋下汗出、不乐本\n",
      "-----------\n",
      "座（第五个征状或说是“玉女离散”），这就是所谓“天人五衰”，是天神最大的悲哀。帝释是众天神的领袖。\n",
      "　　“龙”是指龙神。佛经中的龙，和我国传说中的龙大致差不多，不过没有脚，有时大蟒蛇也称为龙。事实上，中\n",
      "-----------\n",
      "国人对龙和龙王的观念，一部分从佛经中而来。佛经中有五龙王、七龙王、八龙王等等名称。古印度人对龙很尊敬，认为水中生物以龙的力气最大，陆上生物以象的力气最大，因此对德行崇高的人尊称之为“龙象”，如“西来龙象\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(str(text_from_ids(seq).numpy(), encoding='utf-8'))\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "为了能够训练，我们需要一个 `(input, label)` 对的数据集，`input` 和 `label` 都是序列，在每一步输入是当前的字，输出是下一个字。\n",
    "\n",
    "下面的函数就是实现上面的功能，把每个序列当成是 input，复制，在每一步偏移并对齐文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WxbDTJTw5u_P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['我', '是', '北', '京'], ['是', '北', '京', '人'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"我是北京人\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "B9iKPXkw5xwa"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : ﻿释名\n",
      "　　“天龙八部”这名词出于佛经。许多大乘佛经叙述佛陀向诸菩萨、比丘等说法时，常有天龙八部参与听法。如《法华经·提婆达多品》：“天龙八部、人与非人，皆遥见彼龙女成佛”。“非人”是形貌似人而实际不\n",
      "Target: 释名\n",
      "　　“天龙八部”这名词出于佛经。许多大乘佛经叙述佛陀向诸菩萨、比丘等说法时，常有天龙八部参与听法。如《法华经·提婆达多品》：“天龙八部、人与非人，皆遥见彼龙女成佛”。“非人”是形貌似人而实际不是\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", str(text_from_ids(input_example).numpy(), encoding='utf-8'))\n",
    "    print(\"Target:\", str(text_from_ids(target_example).numpy(), encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 创建训练使用的 batches\n",
    "\n",
    "我们可以使用 `tf.data` 把文本分割成可被管理的序列。但是在放到模型里之前，我们需要把它 shuffle 并且打包成 batches。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "接下来，我们要使用 `keras.Model` 子类构建模型。 (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
    "\n",
    "这个模型有三层：\n",
    "\n",
    "* `tf.keras.layers.Embedding`: 输入层. 一个训练好的 lookup table 能够把每个 character-ID 映射成一个 `embedding_dim` 大小;\n",
    "* `tf.keras.layers.GRU`: 一个大小为 `units=rnn_units` 的 GRU 结构 (这里也可以使用 LSTM )\n",
    "* `tf.keras.layers.Dense`: 输出层, 带有 `vocab_size` 大小的输出层. 它会对字典中的每一个字输出一个。这些就是模型对于每个字的 log-likelihood 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Dimensionality of the output space.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "对于每个字，模型查找 embedding，每个时间步使用 GRU 作为输入，并且使用 Dense 层生成 logics 用来预测下个字的 log-likelihood。\n",
    "\n",
    "![A drawing of the data passing through the model](http://aimaksen.bslience.cn/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKbfm04amhXk"
   },
   "source": [
    "Note: 为了训练，这里也可以使用 `keras.Sequential` 模型. 为了生成文本，我们需要管理 RNN 的内部状态。 但是，使用预置的方式(upfront)会比放在后面再去重新编排模型结构更加的简单。更多的细节请参考 [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## 试用模型\n",
    "\n",
    "现在尝试的去运行模型，看看是否和预期的一样。\n",
    "\n",
    "首先，检查一下输出的 shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 4269) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "在上面的例子中，序列的长度是 `100`，但是模型可以被跑在任何长度的输入上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1092864   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  4375725   \n",
      "=================================================================\n",
      "Total params: 9,406,893\n",
      "Trainable params: 9,406,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "为了从模型中获取实际的生成文本，我们需要从输出的分布中进行采样，获取真实的字的分片。这个分布是通过 logits 定义的。\n",
    "\n",
    "注意: 这里从分布中 _sample_ 是非常重要的，我们可以使用分布的 _argmax_ 很容易的从模型中的获取输出。\n",
    "\n",
    "把这个方式试验在在第一个样本上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "这会给我们，每个时间步，对于下个字的序号的预测:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3840, 2454, 1764, 1512, 2000, 3075, 1812, 2689,   14,  536,   83,\n",
       "       1547, 3771, 1140,  649, 1183, 3015,  695, 3501, 2684, 4155, 2302,\n",
       "       2302, 2353, 3014, 2844, 1861, 1337, 2405, 2373,  584, 1867, 2360,\n",
       "         77, 2576, 2799, 3978,  854, 1829, 3399, 4005, 1249, 2273, 2854,\n",
       "       2444, 1144,  767, 3463, 1773, 2463,  760, 1400,  129, 2538, 1035,\n",
       "        766, 1108,   40, 1702,  552, 3323, 3405, 3709, 3897, 3145,  222,\n",
       "       2785, 1748, 1607, 1734, 3133,  852, 2323,  382, 1097, 4047, 1786,\n",
       "       1976,  244, 3561,  378,  412, 3839, 2538, 3658, 3683,  327, 2051,\n",
       "       2110,   91, 2669, 3271, 3851,  818, 1948,  763, 4244,  721, 3539,\n",
       "       1705])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "把这些转化成可读的文本：（此时模型还没有被训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 有些卷舌之音，咬字不正，就像是外国人初学中土言语一般。\n",
      "　　阿朱见少女活泼天真，笑道：“你才长得俊呢，我更加喜欢你！”阿朱久在姑苏，这时说的是中州官话，语音柔媚，可也不甚准确。\n",
      "　　那渔人本要发怒，见\n",
      "\n",
      "Next Char Predictions:\n",
      " 酬玷斤房楚胫是磁C压丝抗通山咂巢耕哲诳碜骏火火煞耐簸朔忡犹爱号朦熄世皙笑陈垒景裘隶度漓粝玉岌嗥设施球嗓恼争痒孤嗤尖t撅叉蛟裳辖银舍余竟散挛放自垂炷冻寻靴既梳侦责决分酪痒踵车兀歪沈中砧蔚采囱格嗜黝啄谭撒\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", str(text_from_ids(input_example_batch[0]).numpy(), encoding='utf-8'))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", str(text_from_ids(sampled_indices).numpy(), encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "这时候，我们可以把这个问题当成一个标准的分类的问题。给定之前的 RNN 状态和这个时间步的输入，预测下一个字的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### 指定一个优化器和一个损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "标准的 `tf.keras.losses.sparse_categorical_crossentropy` 损失函数可以在这个模型中生效。\n",
    "\n",
    "因为我们的模型返回的是 logits，所以我们需要设置 `from_logits` 标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ZOeWdgxNFDXq"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 4269)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         8.358918\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkvUIneTFiow"
   },
   "source": [
    "一个刚被初始化的模型，还是一个非常不准确的状态，所有的输出的 logits 应该都是比较相似的大小。为了去检查，现在的 mean loss 的指数应该比较接近于词典的大小。如果比这个值大的话，说明这个模型的初始化的并不合理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "MAJfS5YoFiHf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4268.075"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "使用 `tf.keras.Model.compile` 配置训练过程，使用 `tf.keras.optimizers.Adam` 默认参数和损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### 配置 checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XBUUavgF56"
   },
   "source": [
    "使用 `tf.keras.callbacks.ModelCheckpoint` 确保 checkpoints 能够在训练过程中保存:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### 执行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "197/197 [==============================] - 173s 869ms/step - loss: 5.7837\n",
      "Epoch 2/20\n",
      "197/197 [==============================] - 173s 873ms/step - loss: 4.5910\n",
      "Epoch 3/20\n",
      "197/197 [==============================] - 173s 875ms/step - loss: 4.1563\n",
      "Epoch 4/20\n",
      "197/197 [==============================] - 170s 857ms/step - loss: 3.8844\n",
      "Epoch 5/20\n",
      "197/197 [==============================] - 172s 865ms/step - loss: 3.6819\n",
      "Epoch 6/20\n",
      "197/197 [==============================] - 171s 863ms/step - loss: 3.5123\n",
      "Epoch 7/20\n",
      "197/197 [==============================] - 170s 859ms/step - loss: 3.3598\n",
      "Epoch 8/20\n",
      "197/197 [==============================] - 170s 857ms/step - loss: 3.2177\n",
      "Epoch 9/20\n",
      "197/197 [==============================] - 173s 871ms/step - loss: 3.0808\n",
      "Epoch 10/20\n",
      "197/197 [==============================] - 173s 871ms/step - loss: 2.9441\n",
      "Epoch 11/20\n",
      "197/197 [==============================] - 173s 871ms/step - loss: 2.8101\n",
      "Epoch 12/20\n",
      "197/197 [==============================] - 173s 872ms/step - loss: 2.6781\n",
      "Epoch 13/20\n",
      "197/197 [==============================] - 171s 866ms/step - loss: 2.5463\n",
      "Epoch 14/20\n",
      "197/197 [==============================] - 174s 878ms/step - loss: 2.4209\n",
      "Epoch 15/20\n",
      "197/197 [==============================] - 175s 883ms/step - loss: 2.3001\n",
      "Epoch 16/20\n",
      "197/197 [==============================] - 174s 878ms/step - loss: 2.1904\n",
      "Epoch 17/20\n",
      "197/197 [==============================] - 167s 846ms/step - loss: 2.0851\n",
      "Epoch 18/20\n",
      "197/197 [==============================] - 168s 849ms/step - loss: 1.9900\n",
      "Epoch 19/20\n",
      "197/197 [==============================] - 167s 845ms/step - loss: 1.9059\n",
      "Epoch 20/20\n",
      "197/197 [==============================] - 168s 851ms/step - loss: 1.8315\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIdQ8c8NvMzV"
   },
   "source": [
    "最简单的生成文本的方式就是把模型放到一个 loop 中，跟踪模型的内部的状态。\n",
    "\n",
    "![To generate text the model's output is fed back to the input](http://aimaksen.bslience.cn/text_generation_training.png)\n",
    "\n",
    "每个时间步我们会调用模型并且传给模型一个文本和一个内部变量，模型会返回一个预测值和一个新的状态，不断地重复这个过程，生成文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "下面的是一个 step 的预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fqMOuDutnOxK"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yDoa0G3IgQ"
   },
   "source": [
    "把上面的过程放到一个 loop 中来生成文本。我们会发现，模型已经知道什么时候创建一个章节，模型金庸写作的词典。由于训练的 epochs 还比较的小，还不能够生成连贯的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乔峰带阿朱回到北方，乔峰对她说：\"我们两人永远留在这里！\"行这里安慰。”鲍千灵道：“多谢乔帮主，哼，向来请下罪名，考乔峰自行，咱们大宋挺兵万家，能有敌眼？”\n",
      "　　三名老前躬身说道：“巴兄哥，在下这‘生乔’，箫某行侠和众兄之行，可一等相差。当真有甚盼报之乐？”随手将信物长老杂启开口，在守口止步走，说道：“咱们是少林寺方丈玄慈、师伯、玄寂、二弟、三弟，大理国三十三绝，因缘在此后。表哥于丐帮诸兄弟身上多伤的探子，明日三晚六方宫奉。”又有“恭敬”两个小字，便不回公主带同重向。萧峰又羞服侍女，心下意思，不禁又在半空里苦挨腾腾般嚷了起来。\n",
      "　　暗暗运忆小舟从青城派走了，在少林寺僧人数十三个同时辰中取入的帛轴，不与从幻成中取将回来。\n",
      "　　玄不见眼见乌老大续道还说，本寺微微曾谈，一束手中找脱藏经等，杀得“先先”也不下去了。一人催道：“止观禅寺开阵，果然乌老大冲施之上，给两人的杖迹把进，掌心手中摇了七八七十来的。老衲远开，请取客气。”他大喜之下，急忙解跃，出气止一张牙，只转过身来，阿碧见鸠摩智口中说什么也完，鸠摩智便须分批“凌波微步”六字，至中没半点头绪，人人均是大伤心，幸亏一篑，沉点向众位东望，说道：“上前出来！只要回家告退，请稍加休息，然回到兴州身边。”\n",
      "　　段誉心道：“先前愁苦内息，将这三招‘一字半能再算错了，只怕这株山堂也须小弄眼算之上，也有片刻隐隐，再到一个‘龙’字十大破块，就石上磨‘写字，乃是激动无端。只怕性命已不易救，我方得太过神通了这般，在下不可妄了出去。”\n",
      "　　忽听鸠摩智道：“慕容公子和四大两人都是知道，慕容氏是这件事的，便不可在地底写些什么武功秘势。”段誉听包不同言语相触，便欲觅地不往，站不起身。\n",
      "　　王语嫣一直不发，心知去伯父、鸠摩智却也在陆地输学，但听慕容复说亲，一烦恼便割他绑缚，不禁年轻感盛，心中霎时间便想：“她造福于我，杀无杀少，从此友有几位大难，在下此刻了不少功劳宫间的那位恩师，王姑娘那个年轻姑娘，是个家慕容公子，想念经动情景，不真心意气躁，只盼能无法不救你一家吧。这……这老义武功比在‘颠倒货色”是以用底极高之若，若能猜得他心生灵念，自由自能。\n",
      "　　次日段誉和鸠摩智陪过去年，却再也不以为意，实不知他应选吉少。否则从小王府中险无归险，大有十六步而收。段誉记得既是牟尼堂，听说群豪有个多世的机停了，若不是出其喜意，心想通皇太叔这部位如奉伯父性命，只因虚竹很亲自如，面临生死，只而不把说过，这份决不能当他出门犯戒。他若杀 \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.086065053939819\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['乔峰带阿朱回到北方，乔峰对她说：\"我们两人永远留在这里！\"'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "最简单的提高结果的方式就是训练更多的 epochs (试试 `EPOCHS = 30`).\n",
    "\n",
    "你还可以使用一个不同的开始的句子做实验，或者尝试增加 RNN 的层数提高模型的准确率，或者是调整 temperature 参数生成一些具有随机性的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OfbI4aULmuj"
   },
   "source": [
    "如果你想模型生成文本更快，最简单的方式就是你可以使用 batch 的方式生成文本。下面的例子就是给定 5 个输入，同时进行生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ZkLu7Y8UCMT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乔峰去了，语气凄凉，增缘相去，陈长老勉强袖言淫动，自己可知便杀了义父、愤怒之仇，都想从当汪帮主朗声说道：“吴长老，乔贵帮甚敬重宽，你也不必大错过大，最不是本领的不老是，我又生兴他也好了？又是江湖之上，还道要抢援的丐帮总是大理国，转过了一个觉，又何必惹人人欺相之身？”\n",
      "　　他说耶律洪基一箭将出来推对方位，说道：“押三军之路向前，冲着各路快追。”耶律洪基笑道：“遵命大宋官兵下属，当年和气与同东、共领大方，都是臣活人奴才好朋友。”萧峰道：“但那辽军的家伙只要出一个千百人，但皇帝要否换烧，蜂皮肉丝不雄多，就是有十二官的首领。辽兵的千刀万剐，斩草干部要冲木桥，他还活得一下便逃了。”萧峰道：“我自尽了几百招的，只盼收弟。在下三年有一之多，但后来干扰褚万里，明天一救，三天一死，岂不干了。”乔峰道：“遮掩藏书，头一战百余万里没有下足。”东首席上数百个十个响，打入东河的中严，格格的笑道：“皇上鸿目神乎，要废我日后兴。卒官元雄年分西，拜见公子，请出来迎打。”\n",
      "　　正是前朝便开，每年一名山东泰山已走入二十房里，有饭的朱香、执法僧意令邻林有余人拜了下去，赶后闯下。人人均感不忿，心想：“苦命钻研，迷满之别。其实与太姻重重，临无之间，他也不想必由自恃读经文学佛法，最要先生罪过经书，那时不便枯荣。本来师父都藏在中间，若能为虚竹这点意，倘若将他喝了，这筋斗骨断，定是积贮败辽，回到家后便即一举；以皇太后三年食物以牛竭为粮留守戒，又说何人故强行事，众人就此必小。其时敌国千里，众无常往。只一口推投，连牛派也不杀戒。武僧在临头之外，也属太皇太后两派的好处安。萧君王恩政之辞，我想必将《易筋经》为难平土的首山，取他驱回宫去接安。”段誉拱手道：“那时在卫辉定平要告知。”萧峰又道：“洪基，这么大胆，富贵数千百人，只要再多数十人赏，以免隐隐以为报明。\n",
      "　　渐行如此，每一百棍都已尽集百川，比之当更繁复，不多点力，便是两条的鼻子方远。萧峰早已瞧得干不净的丑八怪，寻思：“这铁杖流为真婆。我乃皇太后太皇太后，只怕穆贵妃又有又恐怕不尽数号令我要害死父母母恶，为父是杀我妻子，而死粉身谷时时轻柔收气，他便想到正面逃走。嗯，好酒么劝你还没什么君子难劝，他们离长之后，再迟声扰扰，因此哭扰煎养，也不能将来喝酒吃喝水吧。”耶律洪基笑道：“好不但随便送几位大爷锋头，我自今端起了，若不立酒，你便是大祸之望，别让敌人趁机逃回南本，说我辽国不怎么样 \n",
      "\n",
      "________________________________________________________________________________\n",
      "阿朱。阿朱、阿碧两人称晓，将“燕云十六骑经”（Thihhluniongsiongh\n",
      " ihghg thohg 京夏熙河\n",
      "　　夏人十分了先人曾为人以来遮去自掩，守在帮主，便请示舍商议等。\n",
      "　　看了良久良久，大大放心，寻思：“皇上的僧人自当尽可光彩，心头感激，都道：“慕容复若畏杀人物，必定闯了先下，何以命他与鲜卑困危毁？”寻思：“心魔神法，盘算了服了太皇太后无心，如此我不杀。”鸠摩智道：“如此说得如此，明王既落入本理，手中尚有许指头戴虎，以龙明月廿，皆念骨灰，一旦上注。（苟且拜取四位高僧，岂有心神之乐？”\n",
      "　　乔峰道：“老衲犯了几十几人，此刻杀生下抄？乔某居威风畅，死无葬身，何以不敢惊扰的机密转去？”萧远山叹道：“色阵风光，这才干光。那是这株公事，那也罢不，当然得罪你慕容家到他处死的金链几分，如何会知这才从侧而来。有言不语当众之心为礼。”\n",
      "　　两人默默无言，不觉有几句话清朗行山，由观自己全拟扶字。\n",
      "　　雁门关上七八名乡下机道之内制讶极，伯父耶律洪基又何必赌酒？当年在清诏大智的祖境内地以临兴州南地之时的接洛表妹匆匆忙拔出决心，生出搏大阵分。\n",
      "　　耶律洪基道：“萧大王，自不能言了呢。”段誉道：“如此一介南下，咱们只不是公主之政。”耶律洪基道：“咱们身边不痛痛楚地那个黄金刚衣带杀人皮，如何是好？”耶律洪基道：“御官除此甚舍，仍以臣赏做此误打。唉！只小人一个年快。公主殿下有何作故？为贼要想找我，倘若大辽皇帝，我是你错了的军儿，以来便想杀我回王爷。四大恶人若在万难。”说着重骨打下，身子一披高处，鼻孔朝静，手中情形狼狈。却如神木王鼎，附近东向西北方南疆，班中十余里：平南北疆。段誉率部连出九年回王之时，阿朱欢声称别，两口回入虎子，投将他右。拉过桥头，取出一匹马驰，当的说这是清修假。选择辽帝武功很有独等，领兵不见了，原来是少林寺中几次的高僧每一行，便不致不怕，心道：“慧净快快！”这步急改指尖声断曲，直到汪锁住。\n",
      "　　这一带变故毕恐，他心中所想，只在旷阳灵灵，若得玄渡卧落的大眼，更是冲上。到得片刻，方主意气之中，全身剧震，都不禁钦佩。\n",
      "　　但他大声道：“你假惺惺了装腔吗？‘秋水你垂’，我若不糟了，我也不知道呢？”他沉着微笑道：“我的事出宫，一个男子会抱着我说，要乱刀分尸，瞧出来见我。”那女子道：“你不急做主意么？”\n",
      "　　不由得松了起来，缓缓迈开布来，第二僧又匆匆飞起一落。\n",
      "　　段誉寻思：“这 \n",
      "\n",
      "________________________________________________________________________________\n",
      "段誉的四下女子却不懂话，只得暗叫：“惭愧，险些儿也好，为了我的哥哥又说，就是我的花园子，跟你铮七声叫，咱们在曼陀山庄荒野的山边一停。你一路钱死没了，骑在殊。”段誉自由自而至于“鱼贯情”、苗木婉清在听香之榭内的名字，相近庄上庄茶。窗外有人有叫旁仆妇女在段誉身边烧了几眼，见他一双模样寞的纤手，便不能再向段誉跨去。\n",
      "　　他凝自思对，一时心想到了如海，要突然瞧不出的言语，只是她羞不得定了好一阵喜欢，卓不凡长剑身子，不住摇头，说道：“镇南王，九翼道人敢不快杀我好命么？”\n",
      "　　只听那老僧慢慢爬出，寻思：“待我们救命，只有更陷绝境才不便到底在什么地方。第一是不大理睬，只学入了贵境时还有难而前的剑气，已有七十二步步法了。众人越想越出力加强，但手掌越激越快，经脉必早将上面打多大敌，无数抵挡，但刻间石壁掉诸穴，便与趣事惊慌甚为，一本事命他出手相救，当众宣练武功，不如同己手足无措，云中鹤随意差劲，盘算不还手下暗算，实是这九步拳而被他击落之后，却如不住手忙地跑了断魂。否则他们不便闪身真曲，所见有机正要袭招镇南王宫，推了暗器，急忙喝完，左手攀住一足尖石，叫道：“大和尚追来。一男人胆，拥着他，我只道你没杀我的。你对不住，我再不放狗箭头……”那美妇指点段誉。木婉清仍是段誉。\n",
      "　　段延庆肚子痛舍，均想：“要投降我让他为什么要亲我为我，无不可同？但好眼睁睁瞧瞧，和尚跟你服侍呢。”马夫人呜咽，不由得慌了一跳，颤声道：“姓段的，你怎么了？”一名满腮恨极魁梧小年纪，脸颊更是笑容。正是适才见过一个男女的话状，一颗心灰包、谭婆泼者，而入卫辉、结交情，及不中选定，在城中饱位多少。\n",
      "　　这一步跨进山来，见到一个僧侣道其行，声音随时异常，但见乔峰听来对方有危难之事，此人倒也不可怕罪。”见有铁索找去汴梁，见他右手乱舞、段誉刺死，均想那老人叫自己生死不必。小康，老兄百姓却丝毫发起！”但见高手的止一刀蛇，下四少年，四五五十名，兵威丐帮，岂非奇投，众而受伤，这一跤摔倒在胸。这一步若将破穴打走，追风子飞去传来之人到了他胸口。\n",
      "　　乔峰长叹后声，号角声也走不出来，好袍、手臂、啪的缚住却满了一缕戒刀，似要缓兵复护，宁可不防他上些。这两人均不免打动这些消遣，决意难以忍耐，伸手探子，回头向那张望僧人横爬过去，竟尔心神功乱，只得将到玄渡一掌，将这一下又推了过来。第三骑上又奔将过来，但他含威的仇人也都攻上了掌，但诸保昆在武林中也会显然不及， \n",
      "\n",
      "________________________________________________________________________________\n",
      "丐帮中大人体力所为今蛇之困，却也无可轻破，但丐帮中毒手比他是何等直性的苦楚少年，说不定会中包丁春秋一个“天长地久杀”，说道：“薛贤医，那就此了此经，嫁了你一派人家，倒也难以图抗。”\n",
      "　　只听吴长老、执迷人齐名，匆匆、吐蕃、有这等差不多毒之事，再也不敢提防，全身未答，远远舵客自己不便查明。玄生神出自言，和虚竹的破绽作状。此事大半未出，本来质问成事，实不通于阿紫，但他与萧峰北颇有雄痛难出，实在游坦之所害，而这时一提长叹，决不死去，但这时眼眼如火，除了个铁匠大丈夫，以后语气为止，免得甚为难得，只因此刻既无大英雄大有云归。萧峰少年心情，乃契丹胡虏，原本是星宿派群雄了“星宿老仙”，还知道包不同和风波恶要杀手杀他，都想来救援钟二哥，于是确定不自禁起他身，“江昂、眼斜们凭我若死，想必死在他手里，说乔峰曾向前离开大仇。”偏不答道：“乔帮主是我的公子爷！老子惭愧，未死我能不知。不料我错了一大做丫鬟，好生何以思戏惹人？啊哟，我给你引糟了。”\n",
      "　　那白世镜却从半空中飘了一会，轻声纵然欺到，那也不为，用力捣怒，喝道：“你和大哥打在哪里，更好令他是不能当这赵钱孙同家手了，今日代我打你一掌，你就不让他还在帮主叫你方能留身了风褚先一手于臂。”\n",
      "　　段正淳摇头道：“错了么？这法名枉你出帮主号，言辞，你众兄弟，你到自己如遗下，为什么偏不快出来？”\n",
      "　　那女子向着炕边瞧了几眼，转头向那人道：“转马向北有方挥杖，西北方不再坐。”段誉道：“出来诸事，原本不能插让大理国家臣。我侄丐帮主人与四位年前素手交好，岂不干糟？”向他正中礼道：“令孩也已厉害，这个年轻易壮，数十年前的快头，立时便不分手毙么？这样手掌斗地，他自然会有毒指，是不是？”范百龄连续拍手，向玄生胸伸的右掌，作势要左。游坦之早不能自圆，这四人当可没法，何以此事？\n",
      "　　 \n",
      "　　原来这雁门关上都有七八个了，但一来百会有千千万九人这一处重手投入了八块后领，心中惧怕之极。他知再听到那里，湖上再也帮，在下有一步固然远不。朱丹臣大声喝道：“且慢！‘火焰刀’乱刀交好的没将，否则的了，他怎么还会杀伤人？”马夫人道：“小师父，你下令我气息么？”阿朱微笑道：“世上岂有此理？既是第一天可还的管事？”崔绿华道：“你是武林义尊之首，还请便有。我不过听那真章。”\n",
      "　　马夫人缓缓避开，微微一笑，说道：“好吧，我跟你揉了。”段誉忙道：“好，多谢！哼！你有什么话？”\n",
      "　　南海鳄神一招矮 \n",
      "\n",
      "________________________________________________________________________________\n",
      "虚竹对她威震四理的胡言，语调为甚，慢慢立定，伸手接住她手指，说道：“姊夫，你良夜之中，这位老朋友到底是谁，那是不肯的放手下令我伸手洗么？你不能惜跟那小和尚的。”他说：“好一个治得好的。”在他怀中取出一颗药丸吞吞的抓，向他双目直击，瞧他奇起，这才暗暗运气，待他指着虚竹的掌击，姜二人伤了一阵。\n",
      "　　乌老大颤声道：“你的性命不管？”\n",
      "　　兰剑笑了一声，只见四角荡热，似平地复从西边壁上树顶上奔去。涧边远处有个金天色童姥响起：“会追到了适才恭请教训。”心想：“不平道人是表少爷头上，全心合什、万仙子不上、地不牢引我。听那女人述说情景，记了女中不对，我和你就决不会有。这般若是别人，我怎不肯骂？”但见马鞭仍一块大石，东西一软，一丛一开，拦在王语嫣身旁，而王语嫣和南海鳄神、叶二娘、南海鳄神听他是“易筋子”原已百了，脱口便咬着一把棋。从下腾起一头，低声又不下去，他几块肉淋淋的中一片淡红，不由得忧中，心道：“不惜，见到了他这几句话，却是不要。”\n",
      "　　那少女微笑道：“卓先生，你从前便跪下溜了出来，何以不成材的家将来杀我？”\n",
      "　　显然忙道：“师叔，你有一个天老了些人，比什么都要来？”乌老大道：“我无量山终不过是心愿意思的。居然一时，你在一起之间，为人给他二人合力。我便杀过人，却去救我命师伯祖，那是有命的伯母家中人，来得传讯。段誉初天也时，第一个是这等容身！”便转了回来。王语嫣这一次段正淳的口子声调笑问：“段公子，今日咱们同回原归，你师父怎地练这山道门？你师父虽然不是“恶凶恶”的，迁运剑”便即将一笔的伤势一招，都难免得急于大果安静。\n",
      "　　这声音娇如滴在自己身畔，放声软倒，喝道：“是了，皇府之前，早去了这部掌。”段誉心道：“我服了药丸，岂有此理马夫人便跟我说？人拿此话，若不骂死，他摔死了，可真累得了阿紫的一对所养。”但觉此事关怀阴汗，却也没从意思索。\n",
      "　　他最喜清楚楚王将宝物件死伤之时，敌人逃到萧峰，却也并不答话。南海鳄神左手自在暗器，说道：“你将背负布置、灵鹫宫九部马，东方给七天八千成去，还是兵金用便遇害，携手杀你，也就是你厉害！大伙儿行走，只不过一些叫做‘非还手余力’。别人，你是我的朋友，只怕……没面没有？”乔峰道：“好！谁跟你来？”刀白凤道：“你引我回去，瞧中我只是打你不负人的……”她既说的那个“脸”字，噗的一声，跟着汪汪的磷声自然而言，更增气不可。\n",
      "　　马夫人道：“若不是我叫你其实瞧恶清灵啊 \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.653726100921631\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['乔峰', '阿朱', '段誉', '丐帮', '虚竹'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "for res in result:\n",
    "    print(str(res.numpy(), encoding='utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlUQzwu6EXam"
   },
   "source": [
    "## 导出生成器\n",
    "\n",
    "这个 single-step 模型可以被轻松的 [saved and restored](https://www.tensorflow.org/guide/saved_model), 允许你在任意位置使用`tf.saved_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "3Grk32H_CzsC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fbd4c4b12e8>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fbd4c4b12e8>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "_Z9bb_wX6Uuu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:，一眨眼间时到火柱，还差着是真正的笑。神仙姊姊可笑不出的光芒，说道：“姑娘，你的险狠会有好东西道一直，再来找个报了。”游坦之道：“姑娘对尊不对容貌是美女，是美丑脸，什么兜也？”\n",
      "　　只见乌老大捕生生内\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## 提高:自定义训练\n",
    "\n",
    "上面的训练方式比较的简单，不能够给你充分的控制。\n",
    "\n",
    "我们可以使用 teacher-forcing 来阻止不好的预测生成又重新的输入到模型中，那么模型将永远也不会覆盖掉这些错误了。\n",
    "\n",
    "所以，现在我们要看看怎么实现这样一个训练的循环。 这个例子给了一个很好的开始，比如说，你想要实现 _curriculum  learning_ 去稳定模型 open-loop 输出。\n",
    "\n",
    "自定义训练过程里最终要的部分就是自定义一个 step 的计算过程。\n",
    "\n",
    "使用 `tf.GradientTape` 去跟踪 gradients. 你可以从这里学到更多 [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "基本的过程如下：\n",
    "\n",
    "1. 执行魔性并且使用 `tf.GradientTape` 计算损失\n",
    "2. 计算更新，并且使用 optimizer 应用到模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "x0pZ101hjwW0"
   },
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        inputs, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.loss(labels, predictions)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oc-eJALcK8B"
   },
   "source": [
    "上面的 `train_step` 实现遵循 [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). 这个是可选的，但是它可以让你改变你的模型的训练行为，并且依然使用 keras' `Model.compile` 和 `Model.fit` 方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "XKyWiZ_Lj7w5"
   },
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "U817KUm7knlm"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "o694aoBPnEi9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 169s 846ms/step - loss: 5.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd4c0840f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8nAtKHVoInR"
   },
   "source": [
    "或者你需要更复杂的控制，你可以写一个完全自定义的训练的 loop："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "d4tSNwymzf-q",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9512\n",
      "Epoch 1 Batch 50 Loss 4.7906\n",
      "Epoch 1 Batch 100 Loss 4.6118\n",
      "Epoch 1 Batch 150 Loss 4.4033\n",
      "\n",
      "Epoch 1 Loss: 4.5912\n",
      "Time taken for 1 epoch 167.37 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 4.2720\n",
      "Epoch 2 Batch 50 Loss 4.2705\n",
      "Epoch 2 Batch 100 Loss 4.1763\n",
      "Epoch 2 Batch 150 Loss 4.0766\n",
      "\n",
      "Epoch 2 Loss: 4.1649\n",
      "Time taken for 1 epoch 167.73 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 3.8965\n",
      "Epoch 3 Batch 50 Loss 3.8972\n",
      "Epoch 3 Batch 100 Loss 3.9550\n",
      "Epoch 3 Batch 150 Loss 3.7996\n",
      "\n",
      "Epoch 3 Loss: 3.8942\n",
      "Time taken for 1 epoch 167.61 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 3.7211\n",
      "Epoch 4 Batch 50 Loss 3.7356\n",
      "Epoch 4 Batch 100 Loss 3.6499\n",
      "Epoch 4 Batch 150 Loss 3.7698\n",
      "\n",
      "Epoch 4 Loss: 3.6894\n",
      "Time taken for 1 epoch 168.13 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 3.4039\n",
      "Epoch 5 Batch 50 Loss 3.5685\n",
      "Epoch 5 Batch 100 Loss 3.6680\n",
      "Epoch 5 Batch 150 Loss 3.4729\n",
      "\n",
      "Epoch 5 Loss: 3.5205\n",
      "Time taken for 1 epoch 168.72 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 3.3640\n",
      "Epoch 6 Batch 50 Loss 3.2722\n",
      "Epoch 6 Batch 100 Loss 3.3714\n",
      "Epoch 6 Batch 150 Loss 3.3505\n",
      "\n",
      "Epoch 6 Loss: 3.3679\n",
      "Time taken for 1 epoch 167.93 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 3.1300\n",
      "Epoch 7 Batch 50 Loss 3.1349\n",
      "Epoch 7 Batch 100 Loss 3.2785\n",
      "Epoch 7 Batch 150 Loss 3.2228\n",
      "\n",
      "Epoch 7 Loss: 3.2250\n",
      "Time taken for 1 epoch 167.64 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 2.9964\n",
      "Epoch 8 Batch 50 Loss 3.0904\n",
      "Epoch 8 Batch 100 Loss 3.0097\n",
      "Epoch 8 Batch 150 Loss 3.0055\n",
      "\n",
      "Epoch 8 Loss: 3.0872\n",
      "Time taken for 1 epoch 167.21 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 2.8184\n",
      "Epoch 9 Batch 50 Loss 2.8536\n",
      "Epoch 9 Batch 100 Loss 2.9662\n",
      "Epoch 9 Batch 150 Loss 3.0681\n",
      "\n",
      "Epoch 9 Loss: 2.9518\n",
      "Time taken for 1 epoch 167.61 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 2.8080\n",
      "Epoch 10 Batch 50 Loss 2.7715\n",
      "Epoch 10 Batch 100 Loss 2.8363\n",
      "Epoch 10 Batch 150 Loss 2.8738\n",
      "\n",
      "Epoch 10 Loss: 2.8177\n",
      "Time taken for 1 epoch 167.90 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
