{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello，同学们，通过第一节课的学习，你一定学到了不少实操的技能吧！那么我们趁热打铁，赶紧把本节课学到的技能应用起来吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节课的作业要求：\n",
    "\n",
    "+ 参考课上的实战代码实现对教育栏目下的新闻正文内容的爬取，并以文件的形式保存，并且选择任何一种分词方式对每篇新闻进行分词，借助Sklearn中的工具实现每篇新闻的TF-IDF的向量表示。\n",
    "+ 认真阅读注释，对你做对该题至关重要\n",
    "+ 作业已经给出大部分程序实现代码，你只需要在`######## your code ~n line ########` 与 `######## your code end ########` 行之间根据提示补充完毕相应的代码即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 语料获取途径-爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 requests 请求库对人民网教育栏目下[滚动新闻](http://edu.people.com.cn/GB/227065/index1.html)的爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第一步：安装requests请求库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install requests==2.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第二步：编写请求函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "# 模拟浏览器请求头，这里提供了一定量的请求头存储在.data/user_agent.txt中，来模仿不同的浏览器发送请求，应对反爬虫机制\n",
    "user_agents = [ua.strip() for ua in open('data/user_agent.txt', 'r', encoding='utf-8').readlines()]\n",
    "def request_server(url, user_agents=user_agents):\n",
    "    \"\"\"\n",
    "    模拟浏览器发送get请求\n",
    "    :param url: 请求的链接\n",
    "    :return: 服务器返回的数据封装城的Response对象\n",
    "    \"\"\"\n",
    "    headers = {'user-agent':random.choice(user_agents)}\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第三步：构造解析详情页链接的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里用lxml演示,首先安装lxml\n",
    "! pip install lxml==4.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "from lxml import html\n",
    "def extract_detail_url(html_str):\n",
    "    \"\"\"\n",
    "    解析详每页中的详情页链接\n",
    "    :param html_str: 每页的网页源代码\n",
    "    :return: 当前页详情页链接\n",
    "    \"\"\"\n",
    "    tree = html.fromstring(html_str)  # /html/body/div[4]/div[1]/div[2]/ul/li/a/@href\n",
    "    d_urls = tree.xpath('/html/body/div[5]/div[1]/div[2]/ul/li/a/@href')\n",
    "    for d_url in d_urls:\n",
    "        yield d_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第四步：构造解析文章内容的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题一：填写匹配正文内容的xpath路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(html_str):\n",
    "    \"\"\"\n",
    "    提取正文内容\n",
    "    :param html_str:网页源代码\n",
    "    :return:正文内容\n",
    "    \"\"\"\n",
    "    tree = html.fromstring(html_str)  # /html/body/div[1]/div[7]/div[1]/div[3]/p/text()\n",
    "    ######## your code ~ 1 line  ######## 提示：这里填写匹配文章正文内容的xpath路径\n",
    "    documents = \n",
    "    ######## your code end ########\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第五步：编写主函数\n",
    "    + 整个设计思路是while层构造每页的链接，for层构造每页中详情页的链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题二：去除文本中的特殊字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     base_url = 'http://military.people.com.cn'  # http://edu.people.com.cn/n1/2022/0726/c1006-32485754.html\n",
    "    base_url = 'http://edu.people.com.cn'\n",
    "    page_index = 1\n",
    "    data_path = 'data/news_edu.txt'\n",
    "    f = open(data_path, 'w', encoding='utf-8')\n",
    "    while True:\n",
    "        # 构造每页url\n",
    "        url = base_url + '/GB/227065/index%s.html' % page_index\n",
    "        response = request_server(url=url)\n",
    "        # 判断异常页面（超出正常页码范围会返回404），跳出翻页\n",
    "        if response.status_code != 200: \n",
    "            break\n",
    "        for d_url in extract_detail_url(response.text):\n",
    "            try:\n",
    "                # 构造详情页链接\n",
    "                d_url = base_url + d_url\n",
    "                response = request_server(url=d_url)\n",
    "                # 编码设置，不然会乱吗\n",
    "                response.encoding = 'gbk'\n",
    "                cs = extract_content(response.text)\n",
    "                 ######## your code ~1 line ######## 提示：这里通过列表推导式的方式，去除cs列表中每一个元素中的\\n\\t\n",
    "                cs = \n",
    "                 ######## your code end ########\n",
    "                f.write(''.join(cs) + '\\n')\n",
    "            except Exception as e:\n",
    "                print(e.__str__())\n",
    "        print('第%s页完成。' % page_index)\n",
    "        page_index += 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装jieba分词工具\n",
    "!pip install jieba==0.42.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题三：实现jieba的三种分词模式。\n",
    "+ jieba分词使用演示。https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "# 实现分词函数\n",
    "def tokenizer(text, mode=1):\n",
    "    # text：type is str\n",
    "    # return tokens :type is list\n",
    "    if mode==1:\n",
    "        #精确模式\n",
    "        ######## your code ~1 line ######## 提示：实现jieba分词的精确模式\n",
    "        tokens = \n",
    "        ######## your code end ########\n",
    "    elif mode==2:\n",
    "        #全模式\n",
    "        ######## your code ~1 line ######## 提示：实现jieba分词的全模式\n",
    "        tokens = \n",
    "        ######## your code end ########\n",
    "    else:\n",
    "        #搜索引擎模式\n",
    "        ######## your code ~1 line ######## 提示：实现jieba分词的搜索引擎模式\n",
    "        tokens = \n",
    "        ######## your code end ########\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_copus(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载语料\n",
    "data_path = 'data/news_edu.txt'\n",
    "documents = load_copus(data_path)\n",
    "# 对语料进行分词\n",
    "documents = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "documents[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 TF-IDF实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题四：使用Scikit-learn完成爬取文本documents的TF-IDF表示。\n",
    "+ 可以参考[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)的官方文档，完成本次作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "# 利用fit_transform得到TF-IDF矩阵\n",
    "######## your code ~1 line ########\n",
    "tfidf_matrix = \n",
    "######## your code end ######## \n",
    "\n",
    "# 利用get_feature_names得到词表\n",
    "######## your code ~1 line ########\n",
    "\n",
    "######## your code end ########\n",
    "# 得到每个单词所对应的ID\n",
    "# print(tfidf_vec.vocabulary_)\n",
    " \n",
    "# 输出TF-IDF矩阵\n",
    "# print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
