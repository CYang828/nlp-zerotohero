{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCS-d1T3znj2"
   },
   "source": [
    "# 使用 TextCNN 做文本分类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjZMYxFoznj9"
   },
   "source": [
    "这是一个关于使用Ignite训练神经网络模型、建立实验和验证模型的教程。\n",
    "\n",
    "在这个实验中，我们将复制[Convolutional Neural Networks for Sentence Classification by Yoon Kim](https://arxiv.org/abs/1408.5882)! 本文使用CNN进行文本分类，这项任务通常保留给RNN、逻辑回归和朴素贝叶斯。\n",
    "\n",
    "我们希望能够对IMDB电影评论进行分类，并预测评论是正面的还是负面的。IMDB电影评论数据集包括25000个正面和25000个负面示例。数据集由文本和标签对组成。这是二元分类问题。我们将使用PyTorch创建模型，使用torchtext导入数据，使用Ignite训练和监控模型！\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sovYyC0Zznj-"
   },
   "source": [
    "## 所需依赖项\n",
    "\n",
    "在本例中，我们只需要torchtext和spacy软件包，假设`torch`和`ignite`已经安装。我们可以使用“pip”安装它：\n",
    "\n",
    "`pip install torchtext==0.9.1 spacy`\n",
    "\n",
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XHAD9x7znj_"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-ignite torchtext==0.9.1 spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZty7-RWznkA"
   },
   "source": [
    "## 导入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKTazeAkznkB"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxThg0YTznkD"
   },
   "source": [
    "`torchtext` 是一个为NLP任务提供多个数据集的库，类似于`torchvision`。下面我们导入以下内容：\n",
    "* **datasets**: 下载NLP数据集的模块.\n",
    "* **GloVe**: 下载和使用预训练 GloVe embedings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrXE-f7jznkD"
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivAnTyEfznkE"
   },
   "source": [
    "我们导入torch、nn和function模块来创建我们的模型！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbEFAWr0znkE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q22BGKi8znkF"
   },
   "source": [
    "`Ignite` 是一个帮助在PyTorch中训练神经网络的高级库。它附带了一个“Engine”，用于设置训练循环、各种度量、处理程序和一个有用的contrib部分！ \n",
    "\n",
    "下面我们导入以下内容：\n",
    "* **Engine**: 在数据集的每个批上运行给定的process_function，并在运行时发出事件。\n",
    "* **Events**: 允许用户将函数附加到“引擎”，以在特定事件中触发函数。Eg: `EPOCH_COMPLETED`, `ITERATION_STARTED`, etc.\n",
    "* **Accuracy**: 用于计算数据集精度的度量，适用于二进制、多类和多标签情况。 \n",
    "* **Loss**: 将损失函数作为参数的常规度量，计算数据集上的损失。\n",
    "* **RunningAverage**: 在培训期间附加到发动机的一般度量。\n",
    "* **ModelCheckpoint**: 检查点模型的处理程序。\n",
    "* **EarlyStopping**: 处理程序根据分数函数停止训练。\n",
    "* **ProgressBar**: 用于创建tqdm进度条的处理程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enczLgLTznkH"
   },
   "outputs": [],
   "source": [
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import EarlyStopping, ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.utils import manual_seed\n",
    "\n",
    "SEED = 1234\n",
    "manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-39hgxiUMCq9"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZYyXYB5znkH"
   },
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irv_ebeDb8yV"
   },
   "source": [
    "我们首先使用`torchtext.data.utils`设置标记器。\n",
    "\n",
    "标记器的工作是将句子分成“标记”。你可以在[wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis)上了解更多信息.\n",
    "我们将使用“spacy”库中的标记器，这是一个流行的选择。如果您想使用默认设置或任何其他您想要的设置，请随时切换到“basic_english”。\n",
    "\n",
    "docs: https://pytorch.org/text/stable/data_utils.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNRd5Z_KMANB"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZknfGdqedSjN"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Ignite is a high-level library for training and evaluating neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvAmyqHygcZg"
   },
   "source": [
    "接下来，下载IMDB训练和测试数据集。`torchtext.datasets` API返回直接拆分的训练/测试数据集，无需预处理信息。每个拆分都是一个迭代器，逐行生成原始文本和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_jNgWXHhMBQ"
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = datasets.IMDB(split=(\"train\", \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNKvG9b7jadd"
   },
   "source": [
    "现在我们设置了训练、验证和测试拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzJG7Uh_L9q-"
   },
   "outputs": [],
   "source": [
    "# We are using only 1000 samples for faster training\n",
    "# set to -1 to use full data\n",
    "N = 1000\n",
    "\n",
    "# We will use 80% of the `train split` for training and the rest for validation\n",
    "train_frac = 0.8\n",
    "_temp = list(train_iter)\n",
    "\n",
    "\n",
    "random.shuffle(_temp)\n",
    "_temp = _temp[: (N if N > 0 else len(_temp))]\n",
    "n_train = int(len(_temp) * train_frac)\n",
    "\n",
    "train_list = _temp[:n_train]\n",
    "validation_list = _temp[n_train:]\n",
    "test_list = list(test_iter)\n",
    "test_list = test_list[: (N if N > 0 else len(test_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-qYvdeplMIs"
   },
   "source": [
    "让我们浏览一个数据样本，看看它是什么样子。\n",
    "\n",
    "每个数据样本都是以下格式的元组：`(label, text)`.\n",
    "\n",
    "标签的值为“pos”或“negative”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrlLB7PxkIW_"
   },
   "outputs": [],
   "source": [
    "random_sample = random.sample(train_list, 1)[0]\n",
    "print(\" text:\", random_sample[1])\n",
    "print(\"label:\", random_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN5cHrazmMDG"
   },
   "source": [
    "现在我们有了数据集拆分，让我们构建词汇表。为此，我们将使用`torchtext.voab`中的`Voab`类。重要的是，我们基于训练数据集构建词汇表，因为验证和测试在我们的实验中是**unseen**。\n",
    "\n",
    "`Vocab` 允许我们使用预训练的**GloVe**100维单词向量。这意味着每个单词由100个浮点数描述！如果您想了解更多有关这方面的信息，这里有一些参考资料。\n",
    "* [StanfordNLP - GloVe](https://github.com/stanfordnlp/GloVe)\n",
    "* [DeepLearning.ai Lecture](https://www.coursera.org/lecture/nlp-sequence-models/glove-word-vectors-IxDTG)\n",
    "* [Stanford CS224N Lecture by Richard Socher](https://www.youtube.com/watch?v=ASn7ExxLZws)\n",
    "\n",
    "请注意，GloVe 下载大小约为900MB，因此下载可能需要一些时间。\n",
    "\n",
    "“Vocab”类的实例具有以下属性：\n",
    "* `extend` 用于扩展词汇表\n",
    "* `freqs` 是每个单词频率的字典\n",
    "* `itos` 是词汇表中所有单词的列表。\n",
    "* `stoi` 是将每个单词映射到索引的字典。\n",
    "* `vectors` 是下载嵌入的torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_ukillQMKsh"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for (label, line) in train_list:\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "vocab = Vocab(\n",
    "    counter, min_freq=10, vectors=GloVe(name=\"6B\", dim=100, cache=\"/tmp/glove/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYYGwfYsM2Pr"
   },
   "outputs": [],
   "source": [
    "print(\"The length of the new vocab is\", len(vocab))\n",
    "new_stoi = vocab.stoi\n",
    "print(\"The index of '<BOS>' is\", new_stoi[\"<BOS>\"])\n",
    "new_itos = vocab.itos\n",
    "print(\"The token at index 2 is\", new_itos[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y72cqB6Qhqt"
   },
   "source": [
    "我们现在创建`text_transform`和`label_transfer`，它们是可调用的对象，例如这里的`lambda` func，用于处理来自数据集迭代器（或类似“list”的可迭代对象）的原始文本和标签数据。您可以在`text_transform`中的句子中添加特殊符号，如`<BOS>`和`<EOS>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_9hw21lP1nG"
   },
   "outputs": [],
   "source": [
    "text_transform = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_transform = lambda x: 1 if x == \"pos\" else 0\n",
    "\n",
    "# Print out the output of text_transform\n",
    "print(\"input to the text_transform:\", \"here is an example\")\n",
    "print(\"output of the text_transform:\", text_transform(\"here is an example\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtZSEqjJQPxM"
   },
   "source": [
    "为了生成数据批，我们将使用`torch.utils.data.DataLoader`。您可以通过在数据加载器中定义带有`collate_fn`参数的函数来自定义数据批处理。在这里，在`collate_batch`函数中，我们处理原始文本数据并添加填充以动态匹配批次中最长的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHHIEfpRP4TV"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IQd3EVbQvTo"
   },
   "outputs": [],
   "source": [
    "batch_size = 8  # A batch size of 8\n",
    "\n",
    "\n",
    "def create_iterators(batch_size=8):\n",
    "    \"\"\"Heler function to create the iterators\"\"\"\n",
    "    dataloaders = []\n",
    "    for split in [train_list, validation_list, test_list]:\n",
    "        dataloader = DataLoader(split, batch_size=batch_size, collate_fn=collate_batch)\n",
    "        dataloaders.append(dataloader)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CudYIZitUNgx"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = create_iterators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "787zNPm6RtKE"
   },
   "outputs": [],
   "source": [
    "next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2azJGL6znkM"
   },
   "source": [
    "让我们实际探索迭代器的输出是什么，这样我们将知道模型的输入是什么，如何将标签与输出进行比较，以及如何为Ignite的“Engine”设置process_functions。\n",
    "\n",
    "* `batch[0][0]` is the label of a single example. We can see that `vocab.stoi` was used to map the label that originally text into a float.\n",
    "* `batch[1][0]` is the text of a single example. Similar to label, `vocab.stoi` was used to convert each token of the example's text into indices.\n",
    "\n",
    "现在，让我们打印前10批`train_iterator`的句子长度。我们在这里看到，所有批都具有不同的长度，这意味着迭代器按预期工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga2xDXfyznkN"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(\"batch[0][0] : \", batch[0][0])\n",
    "print(\"batch[1][0] : \", batch[1][[0] != 1])\n",
    "\n",
    "lengths = []\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    x = batch[1]\n",
    "    lengths.append(x.shape[0])\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "print(\"Lengths of first 10 batches : \", lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsUrKRr3znkO"
   },
   "source": [
    "## TextCNN 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pldMpTv-znkO"
   },
   "source": [
    "以下是模型的复制，以下是模型操作：\n",
    "* **Embedding**: Embeds a batch of text of shape (N, L) to (N, L, D), where N is batch size, L is maximum length of the batch, D is the embedding dimension. \n",
    "\n",
    "* **Convolutions**: Runs parallel convolutions across the embedded words with kernel sizes of 3, 4, 5 to mimic trigrams, four-grams, five-grams. This results in outputs of (N, L - k + 1, D) per convolution, where k is the kernel_size. \n",
    "\n",
    "* **Activation**: ReLu activation is applied to each convolution operation.\n",
    "\n",
    "* **Pooling**: Runs parallel maxpooling operations on the activated convolutions with window sizes of L - k + 1, resulting in 1 value per channel i.e. a shape of (N, 1, D) per pooling. \n",
    "\n",
    "* **Concat**: The pooling outputs are concatenated and squeezed to result in a shape of (N, 3D). This is a single embedding for a sentence.\n",
    "\n",
    "* **Dropout**: Dropout is applied to the embedded sentence. \n",
    "\n",
    "* **Fully Connected**: The dropout output is passed through a fully connected layer of shape (3D, 1) to give a single output for each example in the batch. sigmoid is applied to the output of this layer.\n",
    "\n",
    "* **load_embeddings**: This is a method defined for TextCNN to load embeddings based on user input. There are 3 modes - rand which results in randomly initialized weights, static which results in frozen pretrained weights, nonstatic which results in trainable pretrained weights. \n",
    "\n",
    "\n",
    "让我们注意，该模型适用于可变文本长度！嵌入句子中的单词的想法，使用卷积、最大化池和concantenation将句子嵌入为单个向量！该单个向量通过具有S形的完全连接层以输出单个值。该值可以解释为句子为正（接近1）或负（接近0）的概率。\n",
    "\n",
    "模型期望的最小文本长度是模型的最小内核大小的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63z1tffDznkO"
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        kernel_sizes,\n",
    "        num_filters,\n",
    "        num_classes,\n",
    "        d_prob,\n",
    "        mode,\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.d_prob = d_prob\n",
    "        self.mode = mode\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.load_embeddings()\n",
    "        self.conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=embedding_dim,\n",
    "                    out_channels=num_filters,\n",
    "                    kernel_size=k,\n",
    "                    stride=1,\n",
    "                )\n",
    "                for k in kernel_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(d_prob)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length = x.shape\n",
    "        x = self.embedding(x.T).transpose(1, 2)\n",
    "        x = [F.relu(conv(x)) for conv in self.conv]\n",
    "        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        if \"static\" in self.mode:\n",
    "            self.embedding.weight.data.copy_(vocab.vectors)\n",
    "            if \"non\" not in self.mode:\n",
    "                self.embedding.weight.data.requires_grad = False\n",
    "                print(\"Loaded pretrained embeddings, weights are not trainable.\")\n",
    "            else:\n",
    "                self.embedding.weight.data.requires_grad = True\n",
    "                print(\"Loaded pretrained embeddings, weights are trainable.\")\n",
    "        elif self.mode == \"rand\":\n",
    "            print(\"Randomly initialized embeddings are used.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unexpected value of mode. Please choose from static, nonstatic, rand.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G3TW4c-znkO"
   },
   "source": [
    "## 创建模型、优化器和损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7nH55oXznkP"
   },
   "source": [
    "下面我们创建TextCNN模型的一个实例，并在**static**模式下加载嵌入。将模型放置在设备上，然后建立二元交叉熵损失函数和Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM_7LQE3znkP"
   },
   "outputs": [],
   "source": [
    "vocab_size, embedding_dim = vocab.vectors.shape\n",
    "\n",
    "model = TextCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    kernel_sizes=[3, 4, 5],\n",
    "    num_filters=100,\n",
    "    num_classes=1,\n",
    "    d_prob=0.5,\n",
    "    mode=\"static\",\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjxbAwvIznkP"
   },
   "source": [
    "## 使用 Ignite 训练和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8Rl7spqznkQ"
   },
   "source": [
    "### 训练器引擎 - process_function\n",
    "\n",
    "Ignite的引擎允许用户定义一个process_function来处理给定的批，这适用于数据集的所有批。这是一个通用类，可用于训练和验证模型！process_function有两个参数engine和batch。\n",
    "\n",
    "让我们了解一下训练的功能：\n",
    "\n",
    "* Sets model in train mode. \n",
    "* Sets the gradients of the optimizer to zero.\n",
    "* Generate x and y from batch.\n",
    "* Performs a forward pass to calculate y_pred using model and x.\n",
    "* Calculates loss using y_pred and y.\n",
    "* Performs a backward pass using loss to calculate gradients for the model parameters.\n",
    "* model parameters are optimized using gradients and optimizer.\n",
    "* Returns scalar loss. \n",
    "\n",
    "以下是trainig过程中的单个操作。此process_function将附加到训练引擎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4ncIcYcznkQ"
   },
   "outputs": [],
   "source": [
    "def process_function(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y, x = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiiQr_GYznkQ"
   },
   "source": [
    "### 评估器引擎 - process_function\n",
    "\n",
    "与训练过程函数类似，我们设置了一个函数来评估单个批次。以下是eval_function的作用：\n",
    "\n",
    "* Sets model in eval mode.\n",
    "* With torch.no_grad(), no gradients are calculated for any succeding steps.\n",
    "* Generates x and y from batch.\n",
    "* Performs a forward pass on the model to calculate y_pred based on model and x.\n",
    "* Returns y_pred and y.\n",
    "\n",
    "Ignite建议将指标附加到评估者而不是培训者，因为在培训过程中，模型参数不断变化，最好在静态模型上评估模型。这些信息很重要，因为培训和评估的功能不同。训练返回单个标量损失。求值返回y_pred和y，因为该输出用于计算整个数据集的每批度量。\n",
    "\n",
    "Ignite中的所有度量都需要y_pred和y作为附加到引擎的函数的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9-G-9iVznkR"
   },
   "outputs": [],
   "source": [
    "def eval_function(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y, x = batch\n",
    "        y = y.to(device)\n",
    "        x = x.to(device)\n",
    "        y = y.float()\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcmIEZuNznkS"
   },
   "source": [
    "### 实例化训练和评估引擎\n",
    "\n",
    "Below we create 3 engines, a trainer, a training evaluator and a validation evaluator. You'll notice that train_evaluator and validation_evaluator use the same function, we'll see later why this was done! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1CxFQs_znkS"
   },
   "outputs": [],
   "source": [
    "trainer = Engine(process_function)\n",
    "train_evaluator = Engine(eval_function)\n",
    "validation_evaluator = Engine(eval_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVu91uVtznkS"
   },
   "source": [
    "### 指标-运行平均值、准确性和损失\n",
    "\n",
    "首先，我们将附加一个运行平均值度量，以跟踪每个批次的标量损失输出的运行平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-98lPU9znkS"
   },
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mufkp6mnznkS"
   },
   "source": [
    "现在，我们要使用两个指标进行评估-准确性和损失。这是一个二元问题，因此对于损失，我们可以简单地将二元交叉熵函数作为Loss_function传递。\n",
    "\n",
    "为了准确，Ignite 要求y_pred和y仅由0和1组成。由于我们的模型输出来自一个Sigmoid层，值介于0和1之间。我们需要编写一个函数来转换`engine.state.output`由y_pred和y组成的输出。\n",
    "\n",
    "Below `thresholded_output_transform` does just that, it rounds y_pred to convert y_pred to 0's and 1's, and then returns rounded y_pred and y. This function is the output_transform function used to transform the `engine.state.output` to achieve `Accuracy`'s desired purpose.\n",
    "\n",
    "Now, we attach `Loss` and `Accuracy` (with `thresholded_output_transform`) to train_evaluator and validation_evaluator. \n",
    "\n",
    "To attach a metric to engine, the following format is used:\n",
    "* `Metric(output_transform=output_transform, ...).attach(engine, 'metric_name')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAK6nXEbznkS"
   },
   "outputs": [],
   "source": [
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(y_pred)\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkcC2R4qznkT"
   },
   "outputs": [],
   "source": [
    "Accuracy(output_transform=thresholded_output_transform).attach(\n",
    "    train_evaluator, \"accuracy\"\n",
    ")\n",
    "Loss(criterion).attach(train_evaluator, \"bce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLtT5f11znkT"
   },
   "outputs": [],
   "source": [
    "Accuracy(output_transform=thresholded_output_transform).attach(\n",
    "    validation_evaluator, \"accuracy\"\n",
    ")\n",
    "Loss(criterion).attach(validation_evaluator, \"bce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbS2h_2eznkU"
   },
   "source": [
    "### 进度条\n",
    "\n",
    "接下来，我们创建一个Ignite的Progress bar实例，并将其连接到trainer，并将`engine.state.metrics`键传递给它跟踪的度量。\n",
    "在这里，进度条将被跟踪 `engine.state.metrics['loss']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qteztuB3znkU"
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, [\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4DxUwXfznkU"
   },
   "source": [
    "### 早停法-跟踪验证损失\n",
    "\n",
    "Now we'll set up a Early Stopping handler for this training process. EarlyStopping requires a score_function that allows the user to define whatever criteria to stop training. In this case, if the loss of the validation set does not decrease in 5 epochs, the training process will stop early.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPM6-USgznkU"
   },
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics[\"bce\"]\n",
    "    return -val_loss\n",
    "\n",
    "\n",
    "handler = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\n",
    "validation_evaluator.add_event_handler(Events.COMPLETED, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfeL6EkhznkU"
   },
   "source": [
    "### 在特定事件中将自定义函数附加到引擎\n",
    "\n",
    "下面，您将看到如何定义自己的自定义函数，并将它们附加到培训过程的各种`Events`中。\n",
    "\n",
    "下面的函数都实现了类似的任务，它们打印在数据集上运行的计算器的结果。一个函数对训练评估器和数据集执行此操作，而另一个函数则对验证执行此操作。另一个区别是这些功能在培训机引擎中的附加方式。\n",
    "\n",
    "第一种方法涉及使用装饰器，语法很简单 - `@` `trainer.on(Events.EPOCH_COMPLETED)`，表示修饰函数将附加到训练器，并在每个历元结束时调用。\n",
    "\n",
    "第二种方法涉及使用trainer的add_event_handler方法 - `trainer.add_event_handler(Events.EPOCH_COMPLETED, custom_function)`。这实现了与上述相同的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XsmcAA2znkV"
   },
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    train_evaluator.run(train_iterator)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    avg_accuracy = metrics[\"accuracy\"]\n",
    "    avg_bce = metrics[\"bce\"]\n",
    "    pbar.log_message(\n",
    "        \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "            engine.state.epoch, avg_accuracy, avg_bce\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def log_validation_results(engine):\n",
    "    validation_evaluator.run(valid_iterator)\n",
    "    metrics = validation_evaluator.state.metrics\n",
    "    avg_accuracy = metrics[\"accuracy\"]\n",
    "    avg_bce = metrics[\"bce\"]\n",
    "    pbar.log_message(\n",
    "        \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "            engine.state.epoch, avg_accuracy, avg_bce\n",
    "        )\n",
    "    )\n",
    "    pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAQEr88cznkW"
   },
   "source": [
    "### 模型检查点\n",
    "\n",
    "最后，我们要检查这个模型。这样做很重要，因为训练过程可能很耗时，如果由于某种原因在训练过程中出现问题，模型检查点有助于从故障点重新开始训练。\n",
    "\n",
    "下面，我们将使用Ignite的`ModelCheckpoint`处理程序在每个历元结束时检查模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Gl6WT0YznkW"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    \"/tmp/models\", \"textcnn\", n_saved=2, create_dir=True, save_as_state_dict=True\n",
    ")\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\"textcnn\": model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxCIriIEznkW"
   },
   "source": [
    "### 发动引擎\n",
    "\n",
    "接下来，我们将运行训练器20个周期并监控结果。下面我们可以看到，Progress bar打印每次迭代的损失，并打印我们在自定义函数中指定的训练和验证结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPe46cQOznkX"
   },
   "outputs": [],
   "source": [
    "trainer.run(train_iterator, max_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpqXiZUsznkY"
   },
   "source": [
    "就这样！我们已经成功地训练和评估了用于文本分类的卷积神经网络。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
