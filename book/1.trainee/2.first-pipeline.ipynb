{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rl0y0i2miQ-g"
   },
   "source": [
    " # 你的第一个 NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天，我们就来看看一个 NLP 任务，该该怎么样一步步的处理吧。\n",
    "\n",
    "本篇文章主要解决：\n",
    "1. NLP 的流程\n",
    "2. 流程中每个环节主要做的事情\n",
    "\n",
    "\n",
    "这里要提前说明以下几点，以防止会陷入到一些固化的思维中：\n",
    "1. NLP 的处理流程不总是线性的\n",
    "2. 经常在处理过程中会有循环\n",
    "3. 这些任务都要具体的根据特定得任务来思考设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK！那我们就开始进入 NLP 任务的 Pipeline 中，通常一个完整的 Pipeline 是下面这样的:\n",
    "\n",
    "![](http://aimaksen.bslience.cn/nlp-pipeline.png)\n",
    "- 问题定义\n",
    "- 数据获取\n",
    "- 数据探索\n",
    "- 数据清理和预处理\n",
    "- 分割数据集\n",
    "- 特征工程\n",
    "- 建模\n",
    "- 评估\n",
    "- 推理\n",
    "- 部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题定义 (Problem Define)\n",
    "\n",
    "通常在工作中，当我们识别到或者接到一个需求的时候，这个问题往往是一个非常具体的问题，并且有可能不是一个最小化的问题。比如说，你的老板可能会和你说，我们接下来要做一个业务，这个业务呢能够自动的识别出学生作业里面的错别字、语法错误等，并且呢也能给出一些对于这个作业的简要的评价，甚至说能不能给它再打个分数。\n",
    "这是一个非常具体的问题，使我们的用户真实的需求的描述，但是，只有这个并不能帮助我们去做算法，做 NLP 的任务。所以，我们需要把这个业务问题转化成一个算法问题，或者几个算法问题，这些算法或者服务相互的配合，能够帮助我们解决用户的需求。\n",
    "那么，这个把业务转化为几个算法，每个算法又都是什么样的形式，就是我们需要确定的。只有确定了问题，我们才能更好的去用各种各样的算法工具解决这些问题。\n",
    "\n",
    "> 好的问题比回答更重要。\n",
    "\n",
    "这句话在很多场景中都非常的重要，在算法领域中也尤其的重要，如果这一步就走错了，那么后面所做的众多努力，都会白费。所以，我们需要谨慎的对待这个过程，甚至在必要的情况下，我们敢于在后面的环节中提出对问题的置疑，以防止浪费更多的财力物力投入到一个错误的方向。\n",
    "之前我的导师和我说过，等到了一定的阶段，提出问题的能力会成为至关重要的能力，他能决定一个人在专业领域力成就的高度。并且还推荐了锻炼提出问题方面的书，这里也分享给大家，希望大家也能对这个平时我们可能不太关注的能力引起关注，并且训练提高。\n",
    "\n",
    "![question1.jpg](assets/question1.jpg)\n",
    "\n",
    "如果有做学术论文选题的也可以看看[这一篇文章](https://pit.ifeng.com/a/20160627/49249371_0.shtml)，是北大的一位法学教授讲如何选题的。\n",
    "\n",
    "能够比较准确的定义问题，我们就能够大概知道我们的这个大的问题，需要通过解决几个小问题解决，并且我们也能够知道每个小问题的范式是什么样的，它所需要的数据是哪些。有了这样的前提，接下来我们就可以来聊一聊，如何获取数据了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXzi_tJTiQ-l"
   },
   "source": [
    "## 数据获取 (Data Acquisition)\n",
    "\n",
    "数据获取工作可以说是在整个 Machine Learning 的过程中，最为核心的工作。但是这件事是个比较艺术的活，它并没有一个完美的解，我们接下来就来聊聊通常情况下，我们该怎么获取数据，又为什么它是个比较艺术的活。\n",
    "\n",
    "![data-acquire.jpg](assets/data-acquire.jpg)\n",
    "\n",
    "- 我们可以从图中看到，首先我们要判断，当前是否有不够的数据，如果有充足的数据，那我们就可以直接进行机器学习的建模过程。\n",
    "- 如果没有充足的数据，则我们要向右走，看看是否有什么额外的数据。但是这里大家要注意，到底多少数据才叫充足，这个是没有一个确定的值的，并且在现实的工作中，往往我们并不具备获取充足数据的条件，比如说工期的限制、资源的限制、基础架构的限制等等。所以，在这里我们一定要建立一种意识，就是我们的数据要和算法模型协同的往前走，而不是追求一步就到最终点。\n",
    "- 接下来我们再来看看额外数据。额外数据往往是我们通过其他的渠道获取到的数据，由于自然语言数据相对来说是比较好获得的数据，我们可以从其他的网站上去进行抓取，补充进我们自己的数据中。但是，这里我们一定要选择好额外的数据，这些数据要尽可能的接近于我们想要解决任务的分布，或者说要接近我们“目标用户”所产生的的数据。这也是需要一些方法的，在之后的内容中我们可以再来详细的聊聊怎么来分析文本数据的分布情况。\n",
    "- 我们再来向右看，当我们没有额外数据的补充，或者当我们补充了额外数据之后依然觉得不够，接下来我们就要考虑，使用一些生成的方法或者增强的方法，来生产新的数据了，这个我们也在后面详细的说。\n",
    "\n",
    "在上面我们说使用额外数据的时候，有一种经验的方法是我们经常使用的，就是如果我们所做的任务，之前有相关的学术的数据集的话，那我们就可以快速的使用学术数据来快速的验证我们的算法。这个过程可以大大的加快我们从需求产生到业务落地的速度，但是比较遗憾的是，目前学术界的数据实在太少，能和我们场景匹配的可能性相对比较少。下面也给大家提供一些寻找这些数据集的地方：\n",
    "- [Google Dataset Search](https://datasetsearch.research.google.com/) 谷歌出品的数据集搜索引擎\n",
    "- [Paper with Code](https://paperswithcode.com/)一个 benchmark 和对应代码寻找的地方，除了找数据集也可以找论文和代码看\n",
    "- [Kaggle](https://www.kaggle.com/) 比赛平台，上面有很多公司和网友贡献的数据集\n",
    "- [Open Data on AWS](https://aws.amazon.com/cn/opendata) 亚马逊的开放数据平台\n",
    "\n",
    "这些资源里包含了学术数据集、竞赛数据集、还有我们之前提到的自己去网络上获取的原始数据，这些数据虽然都是属于补充的数据，但是它们之间也存在着一些差异性，我把它们放到下面的表格里，方便进行对比：\n",
    "\n",
    "![data-acquire.jpg](assets/compare-dataset.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZFcHw3RiQ-m"
   },
   "source": [
    "### 不理想的情况的处理\n",
    "\n",
    "- 带有有限注释/标签的初始数据集\n",
    "- 基于正则表达式或启发式标记的初始数据集\n",
    "- 公共数据集 (cf. [Google Dataset Search](https://datasetsearch.research.google.com/) or [kaggle](https://www.kaggle.com/))\n",
    "- 不完整的数据\n",
    "- 产品上的干预\n",
    "- 数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gQ5KXvaiQ-m"
   },
   "source": [
    "### 数据增强\n",
    "\n",
    "- 这是一种利用语言相似性来生产新数据的技术。\n",
    "- 常见的策略包括:\n",
    "    - 同义词替换 (synonym replacement)\n",
    "    - 相关词替换 (based on association metrics)\n",
    "    - 回译 (Back translation)\n",
    "    - 替换实体 (Replacing entities)\n",
    "    - 增加噪音 (e.g. spelling errors, random words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xdlt9U-iQ-n"
   },
   "source": [
    "## 文本抽取和清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diq2nst3iQ-n"
   },
   "source": [
    "### 文本抽取\n",
    "\n",
    "- 从原始文本中抽取数据\n",
    "    - HTML\n",
    "    - PDF\n",
    "- 相关 vs. 非相关信息\n",
    "    - 非语义信息 (non-textual information)\n",
    "    - 标签 (markup)\n",
    "    - 元数据 (metadata)\n",
    "- 编码格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlnuEXD_iQ-o"
   },
   "source": [
    "#### 从网页中提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gewVKa5wiQ-o",
    "outputId": "594dbc59-a1ca-4aa5-aed8-3d1784ed18ae"
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    " \n",
    " \n",
    "url = 'https://news.google.com/topstories?hl=zh-CN&gl=CN&ceid=CN:zh-Hans'\n",
    "r = requests.get(url)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content,'html.parser')\n",
    "title = soup.find_all('a', class_='DY5T1d')\n",
    "first_art_link = title[1]['href'].replace('.','https://news.google.com',1)\n",
    "\n",
    "print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding='utf8'\n",
    "soup_art = BeautifulSoup(art_request.text,'html.parser')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "for text in art_texts:\n",
    "    print(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD9mogK3iQ-q"
   },
   "source": [
    "#### 从扫描的 PDF 中提取文本\n",
    "\n",
    "需要安装 OCR 提取工具 tesseract，安装教程见 https://nanonets.com/blog/ocr-with-tesseract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypZR_UF9iQ-q",
    "outputId": "9cf587e6-aefa-4449-a888-12f79339864c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "\n",
    "YOUR_DEMO_DATA_PATH = \"data/\"  # please change your file path\n",
    "filename = YOUR_DEMO_DATA_PATH+'pdf-firth-text.png'\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdKv1R92iQ-q"
   },
   "source": [
    "#### Unicode 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE0X28p_iQ-q",
    "outputId": "5505f6c9-ae58-406f-f868-813fab0cd053"
   },
   "outputs": [],
   "source": [
    "text = 'I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8') # encode the strings in bytes\n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvXiND8KiQ-r",
    "outputId": "40c22da5-a6f1-4631-c3f5-6e77b37dda2f"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQyvzlT1iQ-t"
   },
   "source": [
    "- 详细请查阅 [unicodedata documentation](https://docs.python.org/3/library/unicodedata.html) \n",
    "- 其他有用的库\n",
    "    - 拼写检查 (Spelling check): pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddFBa9whiQ-t"
   },
   "source": [
    "### 文本清洗\n",
    "\n",
    "- 预备知识\n",
    "    - Sentence segmentation\n",
    "    - Word tokenization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ce_hYOBiQ-t"
   },
   "source": [
    "#### 分段和标记化 (Segmentation and Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0_4-fYhiQ-t",
    "outputId": "1806db13-61d2-4a42-8f89-72e22c9cfd05"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxX_eM6DiQ-u"
   },
   "source": [
    "- 经常使用的预处理 （preprocessing）\n",
    "    - 停用词 (Stopword) 移除\n",
    "    - Stemming 和lemmatization\n",
    "    - 数字或标点移除\n",
    "    - 大小写标准化\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVrAuhfJiQ-u"
   },
   "source": [
    "#### 删除停用词、标点符号和数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD1S8lsZiQ-u",
    "outputId": "962624f2-e21b-4ef5-decb-3c3892de766d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8BRzw57iQ-u"
   },
   "source": [
    "#### Stemming 和 lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GoRYlcUiQ-u",
    "outputId": "98d2a94b-60ce-4b65-e877-db22b1cfe195"
   },
   "outputs": [],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiwUYRdTiQ-v",
    "outputId": "e2c09901-82ba-46df-f462-806f901e4003"
   },
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo-GlGYhiQ-v"
   },
   "source": [
    "- 和任务相关的预处理 (preprocessing)\n",
    "    - Unicode 标准化\n",
    "    - 语言检测 (Language detection)\n",
    "    - 混合编码 (Code mixing)\n",
    "    - 同音异形 (Transliteration) (e.g., using piyin for Chinese words in English-Chinese code-switching texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oOQG_bbiQ-v"
   },
   "source": [
    "- 自动化标注 (Automatic annotations)\n",
    "    - 语法标记 (POS tagging)\n",
    "    - 解析 (Parsing)\n",
    "    - 命名实体识别 (Named Entity Recognition)\n",
    "    - 指代消解 (Coreference resolution)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbfufLaciQ-v"
   },
   "source": [
    "### 预处理的重要提醒\n",
    "\n",
    "- 并不是所有的预处理过程都是必要的，要根据具体情况分析，预处理所带来的的好处和弊端\n",
    "- 这些步骤不是顺序的\n",
    "- 这些步骤取决于任务的\n",
    "- 预处理的目标\n",
    "    - 文本标准化 (Text Normalization)\n",
    "    - 文本单词化 (Text Tokenization)\n",
    "    - 文本增补和丰富 (Text Enrichment/Annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMbPXmUOiQ-v"
   },
   "source": [
    "## 特征工程 (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xob6EYgsiQ-v"
   },
   "source": [
    "### 什么是特征工程 (feature engineering)?\n",
    "\n",
    "- 它是指将提取和预处理的文本输入机器学习算法的过程\n",
    "- 它旨在将文本的特征捕捉到一个数字向量中，该向量可以被ML算法理解。(Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- 简言之，它涉及到如何有意义地定量表示文本, i.e., text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F15WIiBGiQ-w"
   },
   "source": [
    "### 传统机器学习算法的特征工程 \n",
    "\n",
    "- 基于词的频率表\n",
    "- 文字袋表示法 \n",
    "- 特定于域的词频列表 \n",
    "- 基于领域特定知识的手工特征 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lr8MoHGiQ-w"
   },
   "source": [
    "### 深度学习的特征工程\n",
    "\n",
    "- DL直接将文本作为模型的输入\n",
    "- DL模型能够从文本中学习特征 (e.g., embeddings)\n",
    "- 其代价是，该模型往往难以解释\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr3qBK09iQ-w"
   },
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi7Z_24AiQ-w"
   },
   "source": [
    "### 从简单到复杂\n",
    "\n",
    "- 从启发式或规则开始\n",
    "- 不同 ML 模型的实验\n",
    "    - 从启发式到特征\n",
    "    - 从手动注释到自动提取\n",
    "    - 特征重要性 (Feature importance/weights) \n",
    "- 找到最佳的模型\n",
    "    - Ensemble 和 stacking\n",
    "    - 重做 feature engineering\n",
    "    - 迁移学习 (Transfer learning)\n",
    "    - 重新应用启发式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j3mLjs6iQ-w"
   },
   "source": [
    "## 评估 (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL6pEOyMiQ-x"
   },
   "source": [
    "### 为什么要 evaluation?\n",
    "\n",
    "- 我们需要知道我们建立的模型有多好 \n",
    "- 与评估方法相关的因素 \n",
    "    - 建模方法 (Model building)\n",
    "    - 部署 (Deployment)\n",
    "    - 生产 (Production)\n",
    "- ML度量与业务度量 （ML metrics vs. Business metrics）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD4RGO1biQ-x"
   },
   "source": [
    "### 内在评价与外在评价\n",
    "\n",
    "- 以垃圾邮件分类系统为例 Take spam-classification system as an example\n",
    "- 内在评价:\n",
    "    - 垃圾邮件分类/预测的精度和召回率\n",
    "- 外在评价:\n",
    "    - 用户在垃圾邮件上花费的时间\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPVYewLiQ-x"
   },
   "source": [
    "### 一般性原则\n",
    "\n",
    "- 在外部评估之前先进行内部评估。\n",
    "- 外部评估成本更高，因为它通常涉及人工智能团队以外的项目干系人\n",
    "- 只有当我们在内在评价中获得一致的好结果时，我们才应该进行外在评价\n",
    "- 内在的不良结果往往意味着外在的不良结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LArXgxVLiQ-x"
   },
   "source": [
    "### 通用的 Intrinsic Metrics\n",
    "\n",
    "- 评估指标选择原则 \n",
    "- 标签的数据类型 (ground truths)\n",
    "    - 二元 (Binary) (e.g., sentiment)\n",
    "    - 序型 (Ordinal) (e.g., informational retrieval)\n",
    "    - 分类 (Categorical) (e.g., POS tags)\n",
    "    - 文本 (Textual) (e.g., named entity, machine translation, text generation)\n",
    "- 自动与人工评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRtQF5YiiQ-x"
   },
   "source": [
    "## 后建模阶段 (Post-Modeling Phases)\n",
    "\n",
    "- 在生产环境中部署模型 (e.g., web service)\n",
    "- 定期监控系统性能 \n",
    "- 用新的数据更新系统"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
