{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokémon Name Generation with Keras\n",
    "\n",
    "Generate new unique Pokémon names with a LSTM using Andrej Karpathy's famous [Char-RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) which he used to generate poetry. There are more information in the blog, but the concept is fairly simple. We want the build a next-character-in-text predictor. We will do this by using a window of fixed length as our input and the next char as output and then train a LSTM to perform this task. Since the network won't understand raw characters we need to encode each character to a character vectors with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_length = 1    # The step length we take to get our samples from our corpus\n",
    "epochs = 100       # Number of times we train on our full data\n",
    "batch_size = 32    # Data samples in each training step\n",
    "latent_dim = 64    # Size of our LSTM\n",
    "dropout_rate = 0.2 # Regularization with dropout\n",
    "model_path = os.path.realpath('./poke_gen_model.h5') # Location for the model\n",
    "load_model = False # Enable loading model from disk\n",
    "store_model = True # Store model to disk after training\n",
    "verbosity = 1      # Print result for each epoch\n",
    "gen_amount = 10    # How many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "I have made a .txt where I have stored the names of Pokémon as rows. I have also done some ealy preprocessing like removing special characters and only using lowercase characters. To generate other things than Pokémon names the rows in this file can simply be replaced with some other text that one wishes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Pokénames from file:\n",
      "bulbasaur\n",
      "chikorita\n",
      "treecko\n",
      "turtwig\n",
      "victini\n",
      "chespin\n",
      "rowlet\n",
      "ivysaur\n",
      "bayleef\n",
      "grovyle\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "input_path = os.path.realpath('./input/names.txt')\n",
    "\n",
    "input_names = []\n",
    "\n",
    "print('Reading Pokénames from file:')\n",
    "with open(input_path) as f:\n",
    "    for name in f:\n",
    "        name = name.rstrip()\n",
    "        if len(input_names) < 10:\n",
    "            print(name)\n",
    "        input_names.append(name)\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Concatenate all Pokémon names into a long string corpus.\n",
    "- Build dicionaries to translate chars to indices in a binary char vector.\n",
    "- Find a suitable sequence window, I base it on the longest name I find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 27\n",
      "Corpus length: 6734\n",
      "Number of names:  799\n",
      "Longest name:  12\n"
     ]
    }
   ],
   "source": [
    "# Make it all to a long string\n",
    "concat_names = '\\n'.join(input_names).lower()\n",
    "\n",
    "# Find all unique characters by using set()\n",
    "chars = sorted(list(set(concat_names)))\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Build translation dictionaries, 'a' -> 0, 0 -> 'a'\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Use longest name length as our sequence window\n",
    "max_sequence_length = max([len(name) for name in input_names])\n",
    "\n",
    "print('Total chars: {}'.format(num_chars))\n",
    "print('Corpus length:', len(concat_names))\n",
    "print('Number of names: ', len(input_names))\n",
    "print('Longest name: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a training set where we take samples with sequence length as our input and the next char as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 6722\n",
      "First 10 sequences and next chars:\n",
      "X=[bulbasaur ch]   y=[i]\n",
      "X=[ulbasaur chi]   y=[k]\n",
      "X=[lbasaur chik]   y=[o]\n",
      "X=[basaur chiko]   y=[r]\n",
      "X=[asaur chikor]   y=[i]\n",
      "X=[saur chikori]   y=[t]\n",
      "X=[aur chikorit]   y=[a]\n",
      "X=[ur chikorita]   y=[ ]\n",
      "X=[r chikorita ]   y=[t]\n",
      "X=[ chikorita t]   y=[r]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Loop over our data and extract pairs of sequances and next chars\n",
    "for i in range(0, len(concat_names) - max_sequence_length, step_length):\n",
    "    sequences.append(concat_names[i: i + max_sequence_length])\n",
    "    next_chars.append(concat_names[i + max_sequence_length])\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('X=[{}]   y=[{}]'.replace('\\n', ' ').format(sequences[i], next_chars[i]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding our data into char vectors by using the translation dictionary from earlier.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- 'a'   => [1, 0, 0, ..., 0]\n",
    "\n",
    "- 'b'   => [0, 1, 0, ..., 0]\n",
    "\n",
    "- 'c'   => [0, 0, 1, ..., 0]\n",
    "\n",
    "- 'abc' => [[1, 0, 0, ..., 0], [0, 1, 0, ..., 0], [0, 0, 1, ..., 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (6722, 12, 27)\n",
      "Y shape: (6722, 27)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=np.bool)\n",
    "Y = np.zeros((num_sequences, num_chars), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, char in enumerate(sequence):\n",
    "        X[i, j, char2idx[char]] = 1\n",
    "    Y[i, char2idx[next_chars[i]]] = 1\n",
    "    \n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "Build a standard LSTM network with: \n",
    "\n",
    "- Input shape: (max_sequence_length x num_chars) - representing our sequences.\n",
    "- Output shape: num_chars - representing the next char coming after each sequence.\n",
    "- Output activation: Softmax - since only one value should be 1 in output char vector.\n",
    "- Loss: Categorical cross-entrophy - standard loss for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\simon.larsson\\AppData\\Local\\Continuum\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\simon.larsson\\AppData\\Local\\Continuum\\anaconda3\\envs\\datascience\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                23552     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 25,307\n",
      "Trainable params: 25,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, \n",
    "               input_shape=(max_sequence_length, num_chars),  \n",
    "               recurrent_dropout=dropout_rate))\n",
    "model.add(Dense(units=num_chars, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Watching the loss, doing cross-validation and all that good stuff is not that important here. The best model will not be found by optimizing some metric. We just want to strike a balance between a model that just output gibberish like 'sadsdaddddd' and model that memorizes the names it was trained on. For this it is better to just inspect the output and judge from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n",
      "WARNING:tensorflow:From C:\\Users\\simon.larsson\\AppData\\Local\\Continuum\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Finished training - time elapsed: 2.1919691920280457 min\n",
      "Storing model at: C:\\Projects\\Github\\pokemon-name-generator\\poke_gen_model.h5\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model.load_weights(model_path)\n",
    "else:\n",
    "    \n",
    "    start = time.time()\n",
    "    print('Start training for {} epochs'.format(epochs))\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbosity)\n",
    "    end = time.time()\n",
    "    print('Finished training - time elapsed:', (end - start)/60, 'min')\n",
    "    \n",
    "if store_model:\n",
    "    print('Storing model at:', model_path)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generate names by starting with a real sequence from the corpus, continuously predicting the next char while updating the sequence. To get diversity the correct char is selected from a probability distribution based on the models prediction. This can also be furthered by something called temperature, which I didn't use here.\n",
    "\n",
    "I also added some postprocessing to remove things I did not like manually. Some of this could possibly be done by teaking the network, but I was happy with the way the names looked overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 new names are being generated\n",
      "Generated 1\n",
      "Generated 2\n",
      "Generated 3\n",
      "Generated 4\n",
      "Generated 5\n",
      "Generated 6\n",
      "Generated 7\n",
      "Generated 8\n",
      "Generated 9\n",
      "Generated 10\n"
     ]
    }
   ],
   "source": [
    "# Start sequence generation from end of the input sequence\n",
    "sequence = concat_names[-(max_sequence_length - 1):] + '\\n'\n",
    "\n",
    "new_names = []\n",
    "\n",
    "print('{} new names are being generated'.format(gen_amount))\n",
    "\n",
    "while len(new_names) < gen_amount:\n",
    "    \n",
    "    # Vectorize sequence for prediction\n",
    "    x = np.zeros((1, max_sequence_length, num_chars))\n",
    "    for i, char in enumerate(sequence):\n",
    "        x[0, i, char2idx[char]] = 1\n",
    "\n",
    "    # Sample next char from predicted probabilities\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    probs /= probs.sum()\n",
    "    next_idx = np.random.choice(len(probs), p=probs)   \n",
    "    next_char = idx2char[next_idx]   \n",
    "    sequence = sequence[1:] + next_char\n",
    "\n",
    "    # New line means we have a new name\n",
    "    if next_char == '\\n':\n",
    "\n",
    "        gen_name = [name for name in sequence.split('\\n')][1]\n",
    "\n",
    "        # Never start name with two identical chars, could probably also\n",
    "        if len(gen_name) > 2 and gen_name[0] == gen_name[1]:\n",
    "            gen_name = gen_name[1:]\n",
    "\n",
    "        # Discard all names that are too short\n",
    "        if len(gen_name) > 2:\n",
    "            \n",
    "            # Only allow new and unique names\n",
    "            if gen_name not in input_names + new_names:\n",
    "                new_names.append(gen_name.capitalize())\n",
    "\n",
    "        if 0 == (len(new_names) % (gen_amount/ 10)):\n",
    "            print('Generated {}'.format(len(new_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are the results. I personally cannot tell the difference between generated names and names of Pokémon I dont know. Sometimes there are giveaways, but overall the names are convincing and diverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 generated names:\n",
      "Vinilita\n",
      "Goltar\n",
      "Fooby\n",
      "Slavali\n",
      "Douffa\n",
      "Fernoon\n",
      "Reiopic\n",
      "Terriclue\n",
      "Houana\n",
      "Dible\n"
     ]
    }
   ],
   "source": [
    "print_first_n = min(10, gen_amount)\n",
    "\n",
    "print('First {} generated names:'.format(print_first_n))\n",
    "for name in new_names[:print_first_n]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_output = '\\n'.join(sorted(new_names))\n",
    "output_path = os.path.realpath('./output/generated_names.txt')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(concat_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
