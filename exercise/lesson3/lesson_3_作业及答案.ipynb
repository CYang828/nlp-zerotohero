{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业描述\n",
    "根据课上所学内容，在weibo_senti_100k情感分析数据集上使用Gensim包的CBOW算法训练词向量，并且使用Annoy建立词向量索引，加速搜索速度。\n",
    "\n",
    "\n",
    "+ 认真阅读注释，对你做对该题至关重要\n",
    "\n",
    "+ 作业已经给出大部分程序实现代码，你只需要在`######## your code ~n line ########` 与 `######## your code end ########` 行之间根据提示补充完毕相应的代码即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 下载地址：[百度网盘](https://pan.baidu.com/s/1DoQbki3YwqkuwQUOj64R_g#list/path=%2F)\n",
    "+ 数据概览： 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条\n",
    "+ 推荐实验： 情感/观点/评论 倾向性分析\n",
    "+ 数据来源： [github](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/weibo_senti_100k/intro.ipynb)\n",
    "+ 原数据集： 新浪微博，情感分析标记语料共12万条，网上搜集，具体作者、来源不详"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据集下载并解压\n",
    "把下载后的文件放在与当前jupyter同级目录下的data文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.读取文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了使下面的代码演示更加舒适，这里开启全局禁用警告提示\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以看到数据集有两列，第一列是标签列（对于训练词向量任务，没有用），1代表积极的情绪，0代表消极的情绪；第二列是review（评论）列，我们只使用标签列训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = './data/'\n",
    "pd_all = pd.read_csv(path + 'weibo_senti_100k.csv')\n",
    "pd_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题一：从pd_all中提取review列的数据，并且把每一行的数据存储到data中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## your code ~n line ######## 提示：这里data是一个列表，列表中每一个元素就是一条评论，方法有多种，可自行发挥\n",
    "positive = list(pd_all[pd_all.label==1]['review'])\n",
    "negative = list(pd_all[pd_all.label==0]['review'])\n",
    "data = positive + negative\n",
    "######## your code end ########\n",
    "'一共%s条数据.' % (len(data), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.中文分词\n",
    "这里我们使用jieba进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "# 实现分词函数\n",
    "def tokenizer(text):\n",
    "    #精确模式\n",
    "    tokens = jieba.lcut(text, cut_all=False)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题二：把数据处理成训练词向量需要的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "sentence_tokens = []\n",
    "for paragraph in tqdm(data):\n",
    "    # 提示：对data中的每一个条评论进行分词，每个分词后的句子，词与词之间用空格隔开，并追加到sentence_tokens中\n",
    "    ######## your code ~n line ########\n",
    "    tokens = ' '.join(tokenizer(paragraph))\n",
    "    ######## your code end line ########\n",
    "    sentence_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_tokens[10004]\n",
    "'一共%s个句子.' % (len(sentence_tokens), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于数据量比较大这里将其保存在文件中，每行一个句子，句子中词与词用空格分开\n",
    "with open('sentence_tokens_zh.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim被称为“为人类进行主题建模”的自然语言处理包。但实际上远不止这些。它是一个领先和最先进的软件包，可以用于处理文本、文本向量（如Word2Vec、FastText等）和构建主题模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装gensim\n",
    "!pip install gensim==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 中有较多的参数需要指定，这里逐一进行说明\n",
    "param = {\n",
    "    'sentences':None, # 是一个可以迭代的对象，对于大语料集，建议使用BrownCorpus,Text8Corpus或LineSentence构建。\n",
    "    'corpus_file':None, # 语料的路径，一般与LineSentence配合使用来提升训练的性能\n",
    "    'vector_size':100, # 词向量大小，默认100\n",
    "    'alpha':0.025, # 初始学习速率\n",
    "    'window':7, # 窗口大小，默认5\n",
    "    'min_count':5, # 词频低于min_count以下的词舍去\n",
    "    'max_vocab_size':None, # 最大的词典大小\n",
    "    'sample':1e-3, # 配置哪些高频字随机降采样的阈值，取值范围(0, 1e-5)\n",
    "    'seed':1, # 随机种子，可保证每次训练的结果一致\n",
    "    'workers':6, # 训练使用的线程数量\n",
    "    'min_alpha':0.0001, # 随着训练的进行，学习率将从“alpha”线性下降到“min_alpha”\n",
    "    'sg':0, # 选择1使用skip-gram模型，0使用cbow模型\n",
    "    'hs':0, # 选择1使用hierarchical softmax进行优化，选择0而且“negative”参数是非0，则使用负采样\n",
    "    'negative':5, # 负采样词的个数\n",
    "    'ns_exponent':0.75, # 用于形成负采样分布的指数\n",
    "    'cbow_mean':1, # 如果为0，则使用上下文词向量之和。如果1，则使用平均值，仅在使用cbow时适用\n",
    "    'hashfxn':hash, # 用于随机初始化权重的哈希函数，以提高训练再现性\n",
    "    'epochs':5, # 训练的轮次\n",
    "    'sorted_vocab':1, # 如果为1，则在指定单词索引之前，按频率降序对词汇进行排序。\n",
    "    'batch_words':10000, # 每一批的传递给线程的单词的数量，默认为10000\n",
    "    'compute_loss':False, # 如果是True计算并存储损失值；可以使用gensim.models.word2vec.Word2Vec.get_latest_training_loss方法获取\n",
    "    'max_final_vocab':None, # 限制目标词表的大小\n",
    "    'shrink_windows':True,  # 有效窗口大小从[1，window]均匀采样\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从gensim中导出Word2Vec包\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题三：使用LineSentence实现大规模文本的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.训练模型\n",
    "filename = 'sentence_tokens_zh.txt'\n",
    "# 提示：初始化LineSentence类，赋值给text。[LineSentence类官方教程](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence)\n",
    "####### your code ~1 line ######## \n",
    "text = LineSentence(filename)\n",
    "####### your code end ######## \n",
    "param['sentences'] = text\n",
    "wv_model = Word2Vec(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.保存模型\n",
    "# binary设置为True，训练的词向量将会以二进制的方式存储\n",
    "wv_model.wv.save_word2vec_format('./wv_model/word2vec_zh.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题四：加载训练的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.加载模型（词向量）\n",
    "from gensim.models import KeyedVectors\n",
    "# 提示：gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "####### your code ~1 line ######## \n",
    "wv_model = KeyedVectors.load_word2vec_format('./wv_model/word2vec_zh.txt') \n",
    "####### your code end ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出词语对应的词向量。\n",
    "vec = wv_model[['糟糕','有趣','喜欢']]\n",
    "print('三个词的词向量矩阵的维度是：', vec.shape,'。')\n",
    "print('-------------------------------我是分隔符------------------------')\n",
    "# 计算两个词的相似程度。\n",
    "print('喜欢和喜爱的余弦相似度是：', wv_model.similarity('喜欢', '喜爱'),'。')\n",
    "print('-------------------------------我是分隔符------------------------')\n",
    "# 得到和某个词比较相关的词的列表\n",
    "sim1 = wv_model.most_similar('有趣',topn=10)\n",
    "for key in sim1:\n",
    "    print('和有趣比较相关的词有',key[0],'，余弦距离是：',key[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#%%time 显示整个cell的运行时间\n",
    "key_word = '完美'\n",
    "sim1 = wv_model.most_similar(key_word,topn=10)\n",
    "for key in sim1:\n",
    "    print('和'+ key_word +'比较相关的词有',key[0],'，余弦距离是：',key[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Annoy加速查询\n",
    "\n",
    "Annoy [github链接](https://github.com/spotify/annoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方demo:\n",
    "```\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "f = 40  # Length of item vector that will be indexed\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "for i in range(1000):\n",
    "    v = [random.gauss(0, 1) for z in range(f)]\n",
    "    t.add_item(i, v)\n",
    "t.build(10) # 10 trees\n",
    "t.save('test.ann')\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "print(u.get_nns_by_item(0, 1000)) # will find the 1000 nearest neighbors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装annoy\n",
    "!pip install annoy==1.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "vector_file_path = './wv_model/word2vec_zh.txt'\n",
    "save_file = './wv_model/word2vec_zh.annoy'\n",
    "def get_word_vec():\n",
    "    with open(vector_file_path,'r',encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 掠过第一行的表头\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # 去除两端换行符\n",
    "            line=line.strip()  \n",
    "            line=line.split(' ')\n",
    "            word = line[0]\n",
    "            vec = [float(i) for i in line[1:]]\n",
    "            assert len(vec)==param['vector_size']\n",
    "            yield word, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题五：实现annoy建立索引的过程\n",
    "可以根据官方的demo作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(emb_size=param['vector_size'],metric='angular',num_trees=100):\n",
    "    # 提示：新建AnnoyIndex对象t，并指定参数\n",
    "    ####### your code ~1 line ######## \n",
    "    t=AnnoyIndex(emb_size, metric)\n",
    "    ####### your code end ######## \n",
    "    num=-1\n",
    "    for word,vec in get_word_vec():\n",
    "        num+=1\n",
    "        # 提示：遍历往t中添加向量\n",
    "        ####### your code ~1 line ######## \n",
    "        t.add_item(num,vec)\n",
    "        ####### your code end ######## \n",
    "    t.build(num_trees)\n",
    "    t.save(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_annoy_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_model = AnnoyIndex(param['vector_size'],'angular')\n",
    "annoy_model.load(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过wv_model拿到词与id、id与词的对应关系\n",
    "word2id = wv_model.key_to_index\n",
    "id2word = wv_model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = '完美'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用`Annoy`比使用`most_similar`函数查询Top 10相似词的速度较快些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "topn = 10\n",
    "raw_res = annoy_model.get_nns_by_item(i=word2id[key_word],n=topn,include_distances=True)\n",
    "for key, sim in zip(*raw_res):\n",
    "    print('和'+ key_word +'比较相关的词有',id2word[key],'，距离是：',sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_core",
   "language": "python",
   "name": "nlp_core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
