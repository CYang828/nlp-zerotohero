{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185ca02b-d2a2-4ef0-ba96-a90588d57fad",
   "metadata": {},
   "source": [
    "# 一个包搞定中文数据集: datasetstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33507bf-fade-4345-adef-4d193563eac6",
   "metadata": {},
   "source": [
    "工作中，总是要使用各种中文数据集，每次使用数据集都要花费不少的时间进行寻找，写预处理代码，结合不同的模型和框架做出相应的处理。有的时候好不容易找到合适的数据集，但是却因为网络问题，无法下载，下载了很长一段时间，突然弹出 timeout。\n",
    "\n",
    "既浪费时间，也浪费精力。\n",
    "\n",
    "所以，就决定自己造个轮子，搞定这个问题。\n",
    "\n",
    "考虑到这个包要能有很好的多框架兼容性，并且还要有很好的性能和源码的架构。找来找去，最终找到了 Huggingface 的 Datasets 库，这个包有着非常好的框架兼容性，性能和源码架构，是一个非常好的解决方案。但是！它依然存在一个问题，由于它采用的存储后端是国外的 AWS S3 和 github 的 LFS，必然的，导致了它的网络非常的不稳定，经常的出各种网络问题。\n",
    "\n",
    "既然找到了问题，那么也就操刀解决掉它。于是，就有了 datasetstore，把原有的国外存储后端进行了替换，使用国内的存储，并且也针对一些中文数据集的特有情况，做了一些处理，方便我们使用各种各种各样的数据集。\n",
    "\n",
    "接下来，让我们看看如何快速的使用 [datasetstore](https://github.com/CYang828/datasetstore)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fcf8c-6f65-4241-8981-0232b0aeb8a8",
   "metadata": {},
   "source": [
    "## 快速开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710df61-a9ce-47a7-9f08-49094b25b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasetstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef837a2-7f3c-4915-a244-362abcdd2213",
   "metadata": {},
   "source": [
    "一条命令，安装好所有的依赖，就可以直接使用了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5cfe8f-4720-4c94-9c23-ad105a6b1bde",
   "metadata": {},
   "source": [
    "## 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d72f4e-b8ca-452e-9970-d817af3cc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetstore import load_dataset, list_datasets\n",
    "\n",
    "# 打印支持的数据集\n",
    "print(list_datasets())\n",
    "\n",
    "# 加载数据及并打印并第一个样本\n",
    "hotel_review = load_dataset('hotel-review')\n",
    "print(hotel_review['train'][0])\n",
    "\n",
    "# 处理数据集 - 给每个样本增加一个文本长度的特征\n",
    "hotel_review = hotel_review.map(lambda x: {\"length\": len(x[\"text\"])})\n",
    "\n",
    "# 结合 transformers 库，快速使用各种模型处理任务\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenized_dataset = hotel_review.map(lambda x: tokenizer(x['text']), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20cff0-db37-4b9e-8125-b45e575da5d9",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bbc15-7f97-435d-b4a9-31fab08961af",
   "metadata": {},
   "source": [
    "在使用 `load_dataset()` 接口的时候，datasetstore 会从云端下载所需要的数据集，目前支持的[数据集在这里](https://github.com/CYang828/datasetstore#%E7%9B%AE%E5%89%8D%E6%94%AF%E6%8C%81%E6%95%B0%E6%8D%AE%E9%9B%86)。下载截图如下：\n",
    "![](http://aimaksen.bslience.cn/screanshot-datasets.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a64a38-1e40-42cc-8214-7f333c2c6267",
   "metadata": {},
   "source": [
    "也可以调用 `list_datasets()` 来查看所有已经支持的数据集。\n",
    "\n",
    "**如果有你想用的数据集是不支持的，你可以联系作者（也就是我） zhangchunyang_pri@126.com，包含数据集附件和数据集的使用方法，我来上传上去。**后续也会开放接口，让大家自己上传数据集，目前这个功能先在测试中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e5d01-7e6f-4742-831d-6b17b546589f",
   "metadata": {},
   "source": [
    "## 数据集的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1922f-1828-4832-ac54-9e775d658b46",
   "metadata": {},
   "source": [
    "数据集的使用也非常的简单，下面我对一些常用的方法做些介绍，更多的可以参考 [HuggingFace Datasets 文档](https://huggingface.co/docs/datasets/tutorial)来查看更多细节的使用方法。那如果你觉得看起来费劲，也可以留言给我，我后面会写一些使用的方法。如果觉得原始包有些地方不符合我们的使用习惯，也可以留言给我，我会在后续的迭代中，更新到 datasetstore 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbe173-cafb-429c-be5e-5add9f96558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过索引获取样本\n",
    "dataset[0]\n",
    "\n",
    "# 通过特证明获取特征\n",
    "dataset[\"text\"]\n",
    "\n",
    "# 使用数据集切片功能\n",
    "dataset[:3]\n",
    "\n",
    "# 数据集排序\n",
    "dataset.sort(\"label\")\n",
    "\n",
    "# 数据集打乱\n",
    "shuffled_dataset = sorted_dataset.shuffle(seed=42)\n",
    "\n",
    "# 数据集的选择\n",
    "small_dataset = dataset.select([0, 10, 20, 30, 40, 50])\n",
    "\n",
    "# 数据集的筛选\n",
    "start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n",
    "len(start_with_ar)\n",
    "\n",
    "# 数据集切分\n",
    "dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# 数据集切片\n",
    "datasets = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset.shard(num_shards=4, index=0)\n",
    "\n",
    "# 数据集重命名特征\n",
    "dataset = dataset.rename_column(\"text\", \"sentenceA\")\n",
    "\n",
    "# 数据集特征移除\n",
    "dataset.remove_columns(\"label\")\n",
    "\n",
    "# 数据集 map\n",
    "def add_prefix(example):\n",
    "    example[\"sentence1\"] = 'My sentence: '' + example[\"sentence1\"]\n",
    "    return example\n",
    "\n",
    "updated_dataset = small_dataset.map(add_prefix)\n",
    "\n",
    "# 多进程处理\n",
    "updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63ac9c-f9ee-4239-81bf-e92f06fec144",
   "metadata": {},
   "source": [
    "还有更多的用途和功能以后再写，欢迎使用，提出你的建议，如果你也能加入进来就更好了。\n",
    "\n",
    "觉得有用，请给我一个 star，这是对我最大的支持。\n",
    "https://github.com/CYang828/datasetstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc0778-e65b-48a5-865f-f68602c3a976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
